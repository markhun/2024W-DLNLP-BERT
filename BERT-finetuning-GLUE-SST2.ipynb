{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT on GLUE - SST-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Stanford Sentiment Analysis](https://nlp.stanford.edu/sentiment/index.html):\n",
    "\n",
    "Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Where to store the huggingface data. On the provided Jupyterlab instance that should be within the shared group folder.\n",
    "os.environ['HF_HOME'] = '../groups/192.039-2024W/bert/huggingface/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from transformers import set_seed\n",
    "\n",
    "# RANDOMNESS SEED\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Which dataset to load\n",
    "DATASET_NAME = \"glue\"\n",
    "DATASET_TASK = \"sst2\"\n",
    "\n",
    "PRE_TRAINED_CHECKPOINT = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "TRAIN_OUTPUT_DIR = (\n",
    "    Path(\"../groups/192.039-2024W/bert\") / \"training\" / f\"{DATASET_NAME}-{DATASET_TASK}\"\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32  # Original Paper claims to use 32 for GLUE tasks\n",
    "NUM_EPOCHS = 5  # Original Paper claims to use 3 fine-tuning epochs for GLUE tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "GPU used: NVIDIA GeForce RTX 4060 Ti\n",
      "\n",
      "==============NVSMI LOG==============\n",
      "\n",
      "Timestamp                                 : Sun Jan  5 12:36:51 2025\n",
      "Driver Version                            : 550.135\n",
      "CUDA Version                              : 12.4\n",
      "\n",
      "Attached GPUs                             : 1\n",
      "GPU 00000000:07:00.0\n",
      "    FB Memory Usage\n",
      "        Total                             : 16380 MiB\n",
      "        Reserved                          : 307 MiB\n",
      "        Used                              : 11900 MiB\n",
      "        Free                              : 4175 MiB\n",
      "    BAR1 Memory Usage\n",
      "        Total                             : 256 MiB\n",
      "        Used                              : 29 MiB\n",
      "        Free                              : 227 MiB\n",
      "    Conf Compute Protected Memory Usage\n",
      "        Total                             : 0 MiB\n",
      "        Used                              : 0 MiB\n",
      "        Free                              : 0 MiB\n",
      "    Compute Mode                          : Default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  device_count = torch.cuda.device_count()\n",
    "  device_name = torch.cuda.get_device_name(0)\n",
    "\n",
    "  print(f\"There are {device_count} GPU(s) available.\")\n",
    "  print(f\"GPU used: {device_name}\")\n",
    "  ! nvidia-smi -q --display=MEMORY,COMPUTE\n",
    "\n",
    "else:\n",
    "  print(\"No GPU available, using CPU.\")\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the GLUE dataset different tasks have different accessor keys\n",
    "_task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME, DATASET_TASK)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66730</th>\n",
       "      <td>with outtakes in which most of the characters ...</td>\n",
       "      <td>0</td>\n",
       "      <td>66730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29890</th>\n",
       "      <td>enigma is well-made</td>\n",
       "      <td>1</td>\n",
       "      <td>29890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45801</th>\n",
       "      <td>is ) so stoked to make an important film about...</td>\n",
       "      <td>0</td>\n",
       "      <td>45801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29352</th>\n",
       "      <td>the closest thing to the experience of space t...</td>\n",
       "      <td>1</td>\n",
       "      <td>29352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19858</th>\n",
       "      <td>lose their luster</td>\n",
       "      <td>0</td>\n",
       "      <td>19858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15364</th>\n",
       "      <td>reopens an interesting controversy and</td>\n",
       "      <td>1</td>\n",
       "      <td>15364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30026</th>\n",
       "      <td>competent , unpretentious entertainment</td>\n",
       "      <td>1</td>\n",
       "      <td>30026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38039</th>\n",
       "      <td>an effective portrait of a life in stasis</td>\n",
       "      <td>1</td>\n",
       "      <td>38039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14261</th>\n",
       "      <td>transcendent performance</td>\n",
       "      <td>1</td>\n",
       "      <td>14261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11661</th>\n",
       "      <td>so vivid</td>\n",
       "      <td>1</td>\n",
       "      <td>11661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label    idx\n",
       "66730  with outtakes in which most of the characters ...      0  66730\n",
       "29890                               enigma is well-made       1  29890\n",
       "45801  is ) so stoked to make an important film about...      0  45801\n",
       "29352  the closest thing to the experience of space t...      1  29352\n",
       "19858                                 lose their luster       0  19858\n",
       "15364            reopens an interesting controversy and       1  15364\n",
       "30026           competent , unpretentious entertainment       1  30026\n",
       "38039         an effective portrait of a life in stasis       1  38039\n",
       "14261                          transcendent performance       1  14261\n",
       "11661                                          so vivid       1  11661"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset[\"train\"]).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_lables_in_dataset=array([0, 1])\n",
      "num_labels=2\n"
     ]
    }
   ],
   "source": [
    "unique_lables_in_dataset = pd.DataFrame(dataset[\"train\"])[\"label\"].unique()\n",
    "num_labels = len(unique_lables_in_dataset)\n",
    "\n",
    "print(f\"{unique_lables_in_dataset=}\")\n",
    "print(f\"{num_labels=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_CHECKPOINT, do_lower_case=\"uncased\" in PRE_TRAINED_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT has a maximum sequence length of 512. We can check the sequence lengths resulting from tokenizing our dataset to see if our dataset exceeds this restriction of BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length in split='train': 66\n",
      "Max length in split='validation': 55\n",
      "Max length in split='test': 64\n"
     ]
    }
   ],
   "source": [
    "first_sentence_key, second_sentence_key = _task_to_keys[DATASET_TASK]\n",
    "\n",
    "if second_sentence_key == None:  # Simply tokenize sentence\n",
    "\n",
    "    for split in dataset.keys():\n",
    "        max_len = 0\n",
    "        for sentence in dataset[split][first_sentence_key]:\n",
    "            # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "            input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "            \n",
    "            max_len = max(max_len, len(input_ids))\n",
    "        \n",
    "\n",
    "        print(f\"Max length in {split=}: {max_len}\")\n",
    "\n",
    "else:  # Append both sentences via [SEP] and tokenize\n",
    "\n",
    "    for split in dataset.keys():\n",
    "        max_len = 0\n",
    "        for sentence1, sentence2 in zip(dataset[split][first_sentence_key], dataset[split][second_sentence_key]):\n",
    "            # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "            input_ids = tokenizer.encode(sentence1, sentence2,  add_special_tokens=True)\n",
    "            \n",
    "            max_len = max(max_len, len(input_ids))\n",
    "        \n",
    "\n",
    "        print(f\"Max length in {split=}: {max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505772810392475795ee960ff42c9979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_func(item):\n",
    "    \"\"\"Tokenize passed item. \n",
    "    \n",
    "    Depending on dataset task the passed item will either contain one sentence or two sentences.\n",
    "    In the last case the two sentences will be appended via a [SEP] token.\n",
    "    \"\"\"\n",
    "    if second_sentence_key is None:\n",
    "        return tokenizer(item[first_sentence_key], add_special_tokens=True, truncation=True)\n",
    "    else:\n",
    "        return tokenizer(item[first_sentence_key], item[second_sentence_key], add_special_tokens=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a tokenized dataset item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': ['hide new secretions from the parental units '],\n",
       " 'label': [0],\n",
       " 'idx': [0]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': ['hide new secretions from the parental units '],\n",
       " 'label': [0],\n",
       " 'idx': [0],\n",
       " 'input_ids': [[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102]],\n",
       " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization added the `input_ids` field, which contains the tokenized sentence with a `[CLS]`(101) and two `[SEP]`(102) tokens added. A `token_type_ids` field which indicates first and second portion of the inputs, if necessary. And an `attention_mask` for the given input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface's `transformers` library provides a `DataCollatorWithPadding` class, which allows us to use dynamic padding.  \n",
    "Dynamic padding will add `[PAD]` tokens to the length of the longest sequence within a batch, instead of padding to the maximum sequence length within the entire dataset.  \n",
    "This will avoid unnecessary padding and therefore improve execution efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>5342</td>\n",
       "      <td>2047</td>\n",
       "      <td>3595</td>\n",
       "      <td>8496</td>\n",
       "      <td>2013</td>\n",
       "      <td>1996</td>\n",
       "      <td>18643</td>\n",
       "      <td>3197</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>3397</td>\n",
       "      <td>2053</td>\n",
       "      <td>15966</td>\n",
       "      <td>1010</td>\n",
       "      <td>2069</td>\n",
       "      <td>4450</td>\n",
       "      <td>2098</td>\n",
       "      <td>18201</td>\n",
       "      <td>2015</td>\n",
       "      <td>102.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>2008</td>\n",
       "      <td>7459</td>\n",
       "      <td>2049</td>\n",
       "      <td>3494</td>\n",
       "      <td>1998</td>\n",
       "      <td>10639</td>\n",
       "      <td>2015</td>\n",
       "      <td>2242</td>\n",
       "      <td>2738</td>\n",
       "      <td>3376.0</td>\n",
       "      <td>2055.0</td>\n",
       "      <td>2529.0</td>\n",
       "      <td>3267.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2      3     4     5      6      7      8     9       10  \\\n",
       "0  101  5342  2047   3595  8496  2013   1996  18643   3197   102     NaN   \n",
       "1  101  3397  2053  15966  1010  2069   4450   2098  18201  2015   102.0   \n",
       "2  101  2008  7459   2049  3494  1998  10639   2015   2242  2738  3376.0   \n",
       "\n",
       "       11      12      13     14  \n",
       "0     NaN     NaN     NaN    NaN  \n",
       "1     NaN     NaN     NaN    NaN  \n",
       "2  2055.0  2529.0  3267.0  102.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Example: Select a few samples from the training set\n",
    "samples = tokenized_dataset[\"train\"][:3]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", first_sentence_key, second_sentence_key]}  # Drop `idx` and `sentence` columns, as DataCollator can't process those.\n",
    "pd.DataFrame(samples[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>5342</td>\n",
       "      <td>2047</td>\n",
       "      <td>3595</td>\n",
       "      <td>8496</td>\n",
       "      <td>2013</td>\n",
       "      <td>1996</td>\n",
       "      <td>18643</td>\n",
       "      <td>3197</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>3397</td>\n",
       "      <td>2053</td>\n",
       "      <td>15966</td>\n",
       "      <td>1010</td>\n",
       "      <td>2069</td>\n",
       "      <td>4450</td>\n",
       "      <td>2098</td>\n",
       "      <td>18201</td>\n",
       "      <td>2015</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>2008</td>\n",
       "      <td>7459</td>\n",
       "      <td>2049</td>\n",
       "      <td>3494</td>\n",
       "      <td>1998</td>\n",
       "      <td>10639</td>\n",
       "      <td>2015</td>\n",
       "      <td>2242</td>\n",
       "      <td>2738</td>\n",
       "      <td>3376</td>\n",
       "      <td>2055</td>\n",
       "      <td>2529</td>\n",
       "      <td>3267</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2      3     4     5      6      7      8     9     10    11  \\\n",
       "0  101  5342  2047   3595  8496  2013   1996  18643   3197   102     0     0   \n",
       "1  101  3397  2053  15966  1010  2069   4450   2098  18201  2015   102     0   \n",
       "2  101  2008  7459   2049  3494  1998  10639   2015   2242  2738  3376  2055   \n",
       "\n",
       "     12    13   14  \n",
       "0     0     0    0  \n",
       "1     0     0    0  \n",
       "2  2529  3267  102  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply padding using data_collator\n",
    "batch = data_collator(samples)\n",
    "pd.DataFrame(batch[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `data_collator` will insert `[PAD]` (0) tokens to the maximum length of the passed batch of data items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GLUE dataset specifies one or more evaluation metrics depending on the selected task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"glue\", module_type: \"metric\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "Examples:\n",
       "\n",
       "    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\n",
       "    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0, 'f1': 1.0}\n",
       "\n",
       "    >>> glue_metric = evaluate.load('glue', 'stsb')\n",
       "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
       "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
       "\n",
       "    >>> glue_metric = evaluate.load('glue', 'cola')\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'matthews_correlation': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(DATASET_NAME, DATASET_TASK)\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the selected GLUE task we optimize for different evaluation metrics. See BERT paper p.6:\n",
    "\n",
    "> F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_task_to_metric = {\n",
    "    \"cola\": \"matthews_correlation\",\n",
    "    \"mnli\": \"accuracy\",\n",
    "    \"mnli-mm\": \"accuracy\",\n",
    "    \"mrpc\": \"f1\",\n",
    "    \"qnli\": \"accuracy\",\n",
    "    \"qqp\": \"f1\",\n",
    "    \"rte\": \"accuracy\",\n",
    "    \"sst2\": \"accuracy\",\n",
    "    \"stsb\": \"spearmanr\",\n",
    "}\n",
    "\n",
    "metric_for_best_model = _task_to_metric[DATASET_TASK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use \"['accuracy']\" as an evaluation metric for the task sst2\n"
     ]
    }
   ],
   "source": [
    "def get_metric_name_for_specific_task():\n",
    "    \"\"\"Helper function to derive the evaluation metric name for the specified GLUE task.\n",
    "\n",
    "    The tasks specified by the GLUE benchmark use different evaluation metrics.\n",
    "    Unfortunatly there is no easy way to derive there name after loading the corresponding metric function via HuggingFace's `evaluate` library.\n",
    "    However we can simply do a \"trial run\" and expect the name key of its output.\n",
    "    \"\"\"\n",
    "    output = metric.compute(\n",
    "        predictions=[1, 0], references=[1, 1]\n",
    "    )  # dummy input - we just want to inspect the returned dictionary.\n",
    "    metric_names = output.keys()\n",
    "    \n",
    "    return list(metric_names)\n",
    "\n",
    "\n",
    "metric_names = get_metric_name_for_specific_task()\n",
    "print(f'We will use \"{metric_names}\" as an evaluation metric for the task {DATASET_TASK}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert metric_for_best_model in metric_names, \"Metric to optimize for not found in evaluation metrics provided by GLUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    PRE_TRAINED_CHECKPOINT,\n",
    "    num_labels=num_labels,\n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=TRAIN_OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1000,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=2e-5,  # Original paper uses best out of  5e-5, 4e-5, 3e-5, and 2e-5\n",
    "    weight_decay=0.01,  # Original paper uses 0.01 on pre-training\n",
    "    save_total_limit = 3,  # Keep at most the three checkpoints (latest + best one)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_for_best_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if DATASET_TASK != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "validation_key = \"validation_mismatched\" if DATASET_TASK == \"mnli-mm\" else \"validation_matched\" if DATASET_TASK == \"mnli\" else \"validation\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[validation_key],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- training_arguments.output_dir='../groups/192.039-2024W/bert/training/glue-sst2'\n",
      "--- training_arguments.metric_for_best_model='accuracy'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9b45e19f2441c9b96abaf1a9ad7fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2524, 'grad_norm': 4.768163681030273, 'learning_rate': 1.809976247030879e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07705c3821184ae2b848a92f0ec7457b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28161919116973877, 'eval_accuracy': 0.8807339449541285, 'eval_runtime': 0.9268, 'eval_samples_per_second': 940.901, 'eval_steps_per_second': 30.212, 'epoch': 0.48}\n",
      "{'loss': 0.1764, 'grad_norm': 6.948215961456299, 'learning_rate': 1.6199524940617578e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b574f22f496e40b1ae24d30981c24e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23435935378074646, 'eval_accuracy': 0.9197247706422018, 'eval_runtime': 0.9324, 'eval_samples_per_second': 935.187, 'eval_steps_per_second': 30.029, 'epoch': 0.95}\n",
      "{'loss': 0.1147, 'grad_norm': 7.395206928253174, 'learning_rate': 1.4299287410926367e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9cea2e28f8453cbb5e071317a0e6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2737013101577759, 'eval_accuracy': 0.9151376146788991, 'eval_runtime': 0.9147, 'eval_samples_per_second': 953.346, 'eval_steps_per_second': 30.612, 'epoch': 1.43}\n",
      "{'loss': 0.1115, 'grad_norm': 0.34083613753318787, 'learning_rate': 1.2399049881235155e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d04ecd429174033aa624eb216dab910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2634883522987366, 'eval_accuracy': 0.9277522935779816, 'eval_runtime': 0.917, 'eval_samples_per_second': 950.93, 'eval_steps_per_second': 30.534, 'epoch': 1.9}\n",
      "{'loss': 0.0812, 'grad_norm': 7.222424030303955, 'learning_rate': 1.0498812351543943e-05, 'epoch': 2.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da490a70047445469ec6b3161ee887e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2984648644924164, 'eval_accuracy': 0.9277522935779816, 'eval_runtime': 0.9175, 'eval_samples_per_second': 950.407, 'eval_steps_per_second': 30.518, 'epoch': 2.38}\n",
      "{'loss': 0.0754, 'grad_norm': 7.154967308044434, 'learning_rate': 8.598574821852733e-06, 'epoch': 2.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d31d2882d994a0f8fdafc7a4b4d16fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28637173771858215, 'eval_accuracy': 0.9254587155963303, 'eval_runtime': 0.9171, 'eval_samples_per_second': 950.805, 'eval_steps_per_second': 30.53, 'epoch': 2.85}\n",
      "{'loss': 0.0565, 'grad_norm': 8.668444633483887, 'learning_rate': 6.698337292161521e-06, 'epoch': 3.33}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec912dd4793d4e0e9e20ff3e321e1291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3360050320625305, 'eval_accuracy': 0.926605504587156, 'eval_runtime': 0.9218, 'eval_samples_per_second': 945.936, 'eval_steps_per_second': 30.374, 'epoch': 3.33}\n",
      "{'loss': 0.0513, 'grad_norm': 6.326416492462158, 'learning_rate': 4.798099762470309e-06, 'epoch': 3.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74da3b820a104d7e84839e550fcb1e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3407740592956543, 'eval_accuracy': 0.9208715596330275, 'eval_runtime': 0.9147, 'eval_samples_per_second': 953.322, 'eval_steps_per_second': 30.611, 'epoch': 3.8}\n",
      "{'loss': 0.0411, 'grad_norm': 0.05190267786383629, 'learning_rate': 2.897862232779098e-06, 'epoch': 4.28}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238a15f9ccbe44d5b8ec3dc4b23356e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3797931373119354, 'eval_accuracy': 0.9243119266055045, 'eval_runtime': 0.9245, 'eval_samples_per_second': 943.188, 'eval_steps_per_second': 30.286, 'epoch': 4.28}\n",
      "{'loss': 0.0344, 'grad_norm': 15.143209457397461, 'learning_rate': 9.976247030878861e-07, 'epoch': 4.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b157ec6909404e6aba20c4e2e5c96ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3959137499332428, 'eval_accuracy': 0.9231651376146789, 'eval_runtime': 0.9254, 'eval_samples_per_second': 942.334, 'eval_steps_per_second': 30.258, 'epoch': 4.75}\n",
      "{'train_runtime': 1312.1593, 'train_samples_per_second': 256.634, 'train_steps_per_second': 8.021, 'train_loss': 0.09612227143131356, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"--- {training_arguments.output_dir=}\")\n",
    "print(f\"--- {training_arguments.metric_for_best_model=}\")\n",
    "training_summary = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10525, training_loss=0.09612227143131356, metrics={'train_runtime': 1312.1593, 'train_samples_per_second': 256.634, 'train_steps_per_second': 8.021, 'total_flos': 6961950385929960.0, 'train_loss': 0.09612227143131356, 'epoch': 5.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `trainer.evaluate()` to check that the `trainer` instance did indeed reload the model checkpoint with the highest evaluation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33400461de0480fa62777dbd393bd03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2634883522987366,\n",
       " 'eval_accuracy': 0.9277522935779816,\n",
       " 'eval_runtime': 0.9187,\n",
       " 'eval_samples_per_second': 949.189,\n",
       " 'eval_steps_per_second': 30.479,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_evaluation = trainer.evaluate()\n",
    "best_model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.2524</td>\n",
       "      <td>4.768164</td>\n",
       "      <td>1.809976e-05</td>\n",
       "      <td>0.475059</td>\n",
       "      <td>0.281619</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.9268</td>\n",
       "      <td>940.901</td>\n",
       "      <td>30.212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.1764</td>\n",
       "      <td>6.948216</td>\n",
       "      <td>1.619952e-05</td>\n",
       "      <td>0.950119</td>\n",
       "      <td>0.234359</td>\n",
       "      <td>0.919725</td>\n",
       "      <td>0.9324</td>\n",
       "      <td>935.187</td>\n",
       "      <td>30.029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.1147</td>\n",
       "      <td>7.395207</td>\n",
       "      <td>1.429929e-05</td>\n",
       "      <td>1.425178</td>\n",
       "      <td>0.273701</td>\n",
       "      <td>0.915138</td>\n",
       "      <td>0.9147</td>\n",
       "      <td>953.346</td>\n",
       "      <td>30.612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.1115</td>\n",
       "      <td>0.340836</td>\n",
       "      <td>1.239905e-05</td>\n",
       "      <td>1.900238</td>\n",
       "      <td>0.263488</td>\n",
       "      <td>0.927752</td>\n",
       "      <td>0.9170</td>\n",
       "      <td>950.930</td>\n",
       "      <td>30.534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.0812</td>\n",
       "      <td>7.222424</td>\n",
       "      <td>1.049881e-05</td>\n",
       "      <td>2.375297</td>\n",
       "      <td>0.298465</td>\n",
       "      <td>0.927752</td>\n",
       "      <td>0.9175</td>\n",
       "      <td>950.407</td>\n",
       "      <td>30.518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>0.0754</td>\n",
       "      <td>7.154967</td>\n",
       "      <td>8.598575e-06</td>\n",
       "      <td>2.850356</td>\n",
       "      <td>0.286372</td>\n",
       "      <td>0.925459</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>950.805</td>\n",
       "      <td>30.530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>0.0565</td>\n",
       "      <td>8.668445</td>\n",
       "      <td>6.698337e-06</td>\n",
       "      <td>3.325416</td>\n",
       "      <td>0.336005</td>\n",
       "      <td>0.926606</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>945.936</td>\n",
       "      <td>30.374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>0.0513</td>\n",
       "      <td>6.326416</td>\n",
       "      <td>4.798100e-06</td>\n",
       "      <td>3.800475</td>\n",
       "      <td>0.340774</td>\n",
       "      <td>0.920872</td>\n",
       "      <td>0.9147</td>\n",
       "      <td>953.322</td>\n",
       "      <td>30.611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.051903</td>\n",
       "      <td>2.897862e-06</td>\n",
       "      <td>4.275534</td>\n",
       "      <td>0.379793</td>\n",
       "      <td>0.924312</td>\n",
       "      <td>0.9245</td>\n",
       "      <td>943.188</td>\n",
       "      <td>30.286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.0344</td>\n",
       "      <td>15.143209</td>\n",
       "      <td>9.976247e-07</td>\n",
       "      <td>4.750594</td>\n",
       "      <td>0.395914</td>\n",
       "      <td>0.923165</td>\n",
       "      <td>0.9254</td>\n",
       "      <td>942.334</td>\n",
       "      <td>30.258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10525</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.263488</td>\n",
       "      <td>0.927752</td>\n",
       "      <td>0.9187</td>\n",
       "      <td>949.189</td>\n",
       "      <td>30.479</td>\n",
       "      <td>1312.1593</td>\n",
       "      <td>256.634</td>\n",
       "      <td>8.021</td>\n",
       "      <td>6.961950e+15</td>\n",
       "      <td>0.096122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  grad_norm  learning_rate     epoch  eval_loss  eval_accuracy  \\\n",
       "step                                                                          \n",
       "1000   0.2524   4.768164   1.809976e-05  0.475059   0.281619       0.880734   \n",
       "2000   0.1764   6.948216   1.619952e-05  0.950119   0.234359       0.919725   \n",
       "3000   0.1147   7.395207   1.429929e-05  1.425178   0.273701       0.915138   \n",
       "4000   0.1115   0.340836   1.239905e-05  1.900238   0.263488       0.927752   \n",
       "5000   0.0812   7.222424   1.049881e-05  2.375297   0.298465       0.927752   \n",
       "6000   0.0754   7.154967   8.598575e-06  2.850356   0.286372       0.925459   \n",
       "7000   0.0565   8.668445   6.698337e-06  3.325416   0.336005       0.926606   \n",
       "8000   0.0513   6.326416   4.798100e-06  3.800475   0.340774       0.920872   \n",
       "9000   0.0411   0.051903   2.897862e-06  4.275534   0.379793       0.924312   \n",
       "10000  0.0344  15.143209   9.976247e-07  4.750594   0.395914       0.923165   \n",
       "10525     NaN        NaN            NaN  5.000000   0.263488       0.927752   \n",
       "\n",
       "       eval_runtime  eval_samples_per_second  eval_steps_per_second  \\\n",
       "step                                                                  \n",
       "1000         0.9268                  940.901                 30.212   \n",
       "2000         0.9324                  935.187                 30.029   \n",
       "3000         0.9147                  953.346                 30.612   \n",
       "4000         0.9170                  950.930                 30.534   \n",
       "5000         0.9175                  950.407                 30.518   \n",
       "6000         0.9171                  950.805                 30.530   \n",
       "7000         0.9218                  945.936                 30.374   \n",
       "8000         0.9147                  953.322                 30.611   \n",
       "9000         0.9245                  943.188                 30.286   \n",
       "10000        0.9254                  942.334                 30.258   \n",
       "10525        0.9187                  949.189                 30.479   \n",
       "\n",
       "       train_runtime  train_samples_per_second  train_steps_per_second  \\\n",
       "step                                                                     \n",
       "1000             NaN                       NaN                     NaN   \n",
       "2000             NaN                       NaN                     NaN   \n",
       "3000             NaN                       NaN                     NaN   \n",
       "4000             NaN                       NaN                     NaN   \n",
       "5000             NaN                       NaN                     NaN   \n",
       "6000             NaN                       NaN                     NaN   \n",
       "7000             NaN                       NaN                     NaN   \n",
       "8000             NaN                       NaN                     NaN   \n",
       "9000             NaN                       NaN                     NaN   \n",
       "10000            NaN                       NaN                     NaN   \n",
       "10525      1312.1593                   256.634                   8.021   \n",
       "\n",
       "         total_flos  train_loss  \n",
       "step                             \n",
       "1000            NaN         NaN  \n",
       "2000            NaN         NaN  \n",
       "3000            NaN         NaN  \n",
       "4000            NaN         NaN  \n",
       "5000            NaN         NaN  \n",
       "6000            NaN         NaN  \n",
       "7000            NaN         NaN  \n",
       "8000            NaN         NaN  \n",
       "9000            NaN         NaN  \n",
       "10000           NaN         NaN  \n",
       "10525  6.961950e+15    0.096122  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history = pd.DataFrame(trainer.state.log_history)\n",
    "training_history.groupby(\"step\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Loss and Evaluation Metrics over Training Steps"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = training_history[[\"loss\", \"eval_loss\", \"step\", f\"eval_{metric_for_best_model}\"]]\n",
    "data.columns = [\"Train. Loss\", \"Eval. Loss\", \"Training Steps\", \"Acc.\"]\n",
    "data = pd.melt(data, ['Training Steps'])\n",
    "\n",
    "plot = sns.lineplot(data=data, x=\"Training Steps\", y=\"value\", hue=\"variable\", style=\"variable\", markers=True)\n",
    "plot.set_ylabel(\"\")\n",
    "plot.set_ylim((0, plot.get_ylim()[1]))\n",
    "plot.legend(title=\"\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"### Loss and Evaluation Metrics over Training Steps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Best Model performance:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Our Model\" based on google-bert/bert-base-uncased, best performance on validation data.\n",
      "\"BERT_BASE\" and \"BERT_LARGE\" performance on GLUE testing data as reported in original paper.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Our Model</th>\n",
       "      <th>original BERT_BASE</th>\n",
       "      <th>original BERT_LARGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_loss</th>\n",
       "      <td>0.263488</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_accuracy</th>\n",
       "      <td>0.927752</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Our Model original BERT_BASE original BERT_LARGE\n",
       "eval_loss       0.263488                  -                   -\n",
       "eval_accuracy   0.927752              0.935               0.949"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(f\"### Best Model performance:\"))\n",
    "results = pd.DataFrame(\n",
    "    best_model_evaluation.values(),\n",
    "    index=best_model_evaluation.keys(),\n",
    "    columns=[\"Our Model\"],\n",
    ").drop(\n",
    "    # Drop runtime measurements\n",
    "    index=[\"eval_runtime\", \"eval_samples_per_second\", \"eval_steps_per_second\", \"epoch\"]\n",
    ")\n",
    "# Achieved scores from original BERT paper:\n",
    "results[\"original BERT_BASE\"] = [\"-\", 0.935]\n",
    "results[\"original BERT_LARGE\"] = [\"-\", 0.949]\n",
    "print(f'\"Our Model\" based on {PRE_TRAINED_CHECKPOINT}, best performance on validation data.')\n",
    "print('\"BERT_BASE\" and \"BERT_LARGE\" performance on GLUE testing data as reported in original paper.')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
