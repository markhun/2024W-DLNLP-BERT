{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469f9176-0d48-4edc-980d-f21a620d0403",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT on SQuAD v1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90420218-2622-4d15-9371-60d1c51e589f",
   "metadata": {},
   "source": [
    "The Stanford Question Answering Dataset is a collection of crowd-sourced question/answer pairs, given a question and a passage from Wikipedia containing the answer. The task is to predict the answer text span in the passage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc98e26-fe74-40fd-846b-a15213dd304e",
   "metadata": {},
   "source": [
    "## 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7488d34-5623-4a2b-b704-36196e66acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Store the huggingface data in a shared group folder on the provided JupyterLab instance.\n",
    "os.environ['HF_HOME'] = '../../groups/192.039-2024W/bert/huggingface/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28bd8ea0-93f0-4499-b1de-f2478534305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from transformers import set_seed\n",
    "\n",
    "# RANDOMNESS SEED\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Which dataset to load\n",
    "DATASET_NAME = \"squad\"\n",
    "MODEL_VERSION = \"single\"\n",
    "\n",
    "TRAIN_OUTPUT_DIR = (\n",
    "    Path(\"../../groups/192.039-2024W/bert\") / \"training\" / f\"{DATASET_NAME}-{MODEL_VERSION}\"\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32  # Original Paper claims to use 32 for the SQuAD finetuning\n",
    "NUM_EPOCHS = 3  # Original Paper claims to use 3 fine-tuning epochs for the SQuAD finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02825b84-76b4-471c-9b9c-eaa261628ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "GPU used: NVIDIA A40\n",
      "\n",
      "==============NVSMI LOG==============\n",
      "\n",
      "Timestamp                                 : Tue Jan 28 07:47:36 2025\n",
      "Driver Version                            : 550.90.07\n",
      "CUDA Version                              : 12.4\n",
      "\n",
      "Attached GPUs                             : 1\n",
      "GPU 00000000:05:00.0\n",
      "    FB Memory Usage\n",
      "        Total                             : 46068 MiB\n",
      "        Reserved                          : 665 MiB\n",
      "        Used                              : 4 MiB\n",
      "        Free                              : 45401 MiB\n",
      "    BAR1 Memory Usage\n",
      "        Total                             : 65536 MiB\n",
      "        Used                              : 2 MiB\n",
      "        Free                              : 65534 MiB\n",
      "    Conf Compute Protected Memory Usage\n",
      "        Total                             : 0 MiB\n",
      "        Used                              : 0 MiB\n",
      "        Free                              : 0 MiB\n",
      "    Compute Mode                          : Default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  device_count = torch.cuda.device_count()\n",
    "  device_name = torch.cuda.get_device_name(0)\n",
    "\n",
    "  print(f\"There are {device_count} GPU(s) available.\")\n",
    "  print(f\"GPU used: {device_name}\")\n",
    "  ! nvidia-smi -q --display=MEMORY,COMPUTE\n",
    "\n",
    "else:\n",
    "  print(\"No GPU available, using CPU.\")\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb70d3d-3598-4b13-b5a5-a8f49783c0d2",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e7de3e-0ad2-4efd-82e2-ae52bed2ac56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f72cb-efe8-4118-b755-03a19306c13a",
   "metadata": {},
   "source": [
    "The training set only contains one possible answer. We can check this by filtering through the dataset and looking for an entry in the answers field that has more than one text field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0fc91a-3c16-402e-95f9-77c985924392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 0\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95907b83-83ea-4a90-998f-3c381d8bec16",
   "metadata": {},
   "source": [
    "However, the evaluation set has several possible answers for each sample, which may be the same or different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624753e6-a125-4a4c-a1f5-c6337fd24583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "902d4d64-459d-4bad-af10-2afc6465ebc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Santa Clara, California',\n",
       "  \"Levi's Stadium\",\n",
       "  \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"],\n",
       " 'answer_start': [403, 355, 355]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][2][\"answers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe1035-02a4-4679-87b0-bd2dc96dd821",
   "metadata": {},
   "source": [
    "## 2. BERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d476ca62-346a-446b-89aa-a1e67cc67e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_CHECKPOINT = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15284633-06b4-4bd1-82cc-6f0a78d46bb6",
   "metadata": {},
   "source": [
    "### 2.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5e133ea-0892-46c6-9b08-e7e54b6b4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefeeb83-863e-4955-87d1-f6a5bbe61357",
   "metadata": {},
   "source": [
    "#### 2.1.1 Preprocessing training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5145c73-decc-4453-a575-c84066671114",
   "metadata": {},
   "source": [
    "These preprocessing steps are essential when it comes to question answering tasks. In case the dataset has some examples that have a very long context which exceeds the maximum length of the model, we need to truncate their context. Next, we then need to set the start and end posistions of the answer to the original context. With that mapping we then can find the start and end tokens of the answer. Therefore, we need to find which part of the offset corresponds to the question and which one corresponds to the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "131c6f0f-6368-471f-b578-7ef1108379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edd1a94b-a459-42de-8b74-1574fbc8acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e9b3246-d8f2-4f41-9a7d-c56f079c6b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 88524)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85a8f1a3-4b67-47fa-b742-bc8bcf972f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_ids</th>\n",
       "      <td>[101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 1005, 1055, 2751, 8514, 2003, 1037, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 692...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_type_ids</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attention_mask</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_positions</th>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_positions</th>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                               0\n",
       "input_ids        [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 1005, 1055, 2751, 8514, 2003, 1037, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 692...\n",
       "token_type_ids                                                                                                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]\n",
       "attention_mask                                                                                                 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]\n",
       "start_positions                                                                                                                                                                                                                                                                                                                                                                                                              130\n",
       "end_positions                                                                                                                                                                                                                                                                                                                                                                                                                137"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with pd.option_context('display.max_colwidth', 400):\n",
    "    display(pd.DataFrame(train_dataset[:1]).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01027528-ff3b-4821-90c0-3cc12bc80d03",
   "metadata": {},
   "source": [
    "#### 2.1.2 Preprocessing validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57038a79-ef56-4ef9-9d73-8d4975120f06",
   "metadata": {},
   "source": [
    "After preprocessing the training dataset, we will preprocess the validation dataset. This differs a little bit from the preprocessing of the training dataset, because we do not need to generate labels. This would only be necessary when we want to compute a validation loss, but since that number does not really tell us if the model is good or not, we will not compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "816b744d-f021-4339-878c-c8450ef71ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61107a13-9111-4353-b229-c8d241b20882",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = dataset[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc03950e-ad6d-4f1d-a7a2-6d17db81c5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10570, 10784)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019412d4-4359-4b66-a1e2-514ff4089dae",
   "metadata": {},
   "source": [
    "#### 2.1.3 Dealing with Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e42a81-a486-446f-9264-5d7f7b33c01a",
   "metadata": {},
   "source": [
    "As we already handled the padding in the preprocessing step by padding all the samples to the maximum length that we set, we do not need to define a data collator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf76fbc-c366-4659-80e7-f5aee8e96833",
   "metadata": {},
   "source": [
    "### 2.2 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ac6f0ec-bf0f-4eda-8522-6531a949b69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"squad\", module_type: \"metric\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\n",
       "Computes SQuAD scores (F1 and EM).\n",
       "Args:\n",
       "    predictions: List of question-answers dictionaries with the following key-values:\n",
       "        - 'id': id of the question-answer pair as given in the references (see below)\n",
       "        - 'prediction_text': the text of the answer\n",
       "    references: List of question-answers dictionaries with the following key-values:\n",
       "        - 'id': id of the question-answer pair (see above),\n",
       "        - 'answers': a Dict in the SQuAD dataset format\n",
       "            {\n",
       "                'text': list of possible texts for the answer, as a list of strings\n",
       "                'answer_start': list of start positions for the answer, as a list of ints\n",
       "            }\n",
       "            Note that answer_start values are not taken into account to compute the metric.\n",
       "Returns:\n",
       "    'exact_match': Exact match (the normalized answer exactly match the gold answer)\n",
       "    'f1': The F-score of predicted tokens versus the gold answer\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n",
       "    >>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
       "    >>> squad_metric = evaluate.load(\"squad\")\n",
       "    >>> results = squad_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'exact_match': 100.0, 'f1': 100.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(DATASET_NAME)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90eaa629-4b19-4f1e-8d53-43055005c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa42319-28c8-480c-8498-89ca102fa7f7",
   "metadata": {},
   "source": [
    "### 2.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2223f3b-d664-4209-ac28-75cadb1a59f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(PRE_TRAINED_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6708bcfc-8f4b-4cab-a425-b93bca85d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=(TRAIN_OUTPUT_DIR / PRE_TRAINED_CHECKPOINT.replace(\"/\", \"_\")).resolve(),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=2e-5,  # Original paper uses 5e-5\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5640b0cd-62d1-4859-aa22-fc022a457548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d7d2411-01da-408f-a580-001687934c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- training_arguments.output_dir='/home/e12433721/groups/192.039-2024W/bert/training/squad-single/google-bert_bert-base-uncased'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8301' max='8301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8301/8301 1:09:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2767</td>\n",
       "      <td>1.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5534</td>\n",
       "      <td>0.873500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8301</td>\n",
       "      <td>0.689600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"--- {training_arguments.output_dir=}\")\n",
    "training_summary_bert_base = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "016e41ee-5c50-4ce1-bb94-8dfa250165db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8301, training_loss=0.9924698455005421, metrics={'train_runtime': 4149.3898, 'train_samples_per_second': 64.003, 'train_steps_per_second': 2.001, 'total_flos': 5.204482670991974e+16, 'train_loss': 0.9924698455005421, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_summary_bert_base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa61568-a31f-4628-921f-d71fb5dfe128",
   "metadata": {},
   "source": [
    "### 2.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "098612f6-c8b8-4e21-8769-5b5394909b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b8511ea04d45b298a50206d33b6755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "scores_bert_base = compute_metrics(start_logits, end_logits, validation_dataset, dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9be627cc-8d29-4463-8d91-5cb5c9bfbb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 79.91485335856197, 'f1': 87.69099229972197}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_bert_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a8a01a1-be24-4237-9d78-115fd7af1a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>step</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.4143</td>\n",
       "      <td>20.969122</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>2767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8735</td>\n",
       "      <td>17.271847</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>5534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.6896</td>\n",
       "      <td>19.913282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8301</td>\n",
       "      <td>4149.3898</td>\n",
       "      <td>64.003</td>\n",
       "      <td>2.001</td>\n",
       "      <td>5.204483e+16</td>\n",
       "      <td>0.99247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  grad_norm  learning_rate  step  train_runtime  \\\n",
       "epoch                                                          \n",
       "1      1.4143  20.969122       0.000013  2767            NaN   \n",
       "2      0.8735  17.271847       0.000007  5534            NaN   \n",
       "3      0.6896  19.913282       0.000000  8301      4149.3898   \n",
       "\n",
       "       train_samples_per_second  train_steps_per_second    total_flos  \\\n",
       "epoch                                                                   \n",
       "1                           NaN                     NaN           NaN   \n",
       "2                           NaN                     NaN           NaN   \n",
       "3                        64.003                   2.001  5.204483e+16   \n",
       "\n",
       "       train_loss  \n",
       "epoch              \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3         0.99247  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history_bert_base = pd.DataFrame(trainer.state.log_history)\n",
    "training_history_bert_base.epoch = training_history_bert_base.epoch.astype(int)\n",
    "training_history_bert_base.groupby(\"epoch\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f98e0d7-3e30-4e41-b3ec-a8e4bf34820a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Model performance:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"BERT_BASE\" performance on the SQuAD dataset as reported in the original paper.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>our BERT_BASE</th>\n",
       "      <th>original BERT_BASE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>79.914853</td>\n",
       "      <td>80.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>87.690992</td>\n",
       "      <td>88.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    our BERT_BASE  original BERT_BASE\n",
       "EM      79.914853                80.8\n",
       "F1      87.690992                88.5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f\"### Model performance:\"))\n",
    "results = pd.DataFrame(\n",
    "    [scores_bert_base[\"exact_match\"]] + [scores_bert_base[\"f1\"]],\n",
    "    index=[\"EM\"] + [\"F1\"],\n",
    "    columns=[\"our BERT_BASE\"],\n",
    ")\n",
    "\n",
    "# Achieved scores from original BERT paper:\n",
    "results[\"original BERT_BASE\"] = [80.8,88.5]\n",
    "\n",
    "print(\n",
    "    '\"BERT_BASE\" performance on the SQuAD dataset as reported in the original paper.'\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52afbebc-d697-4fc4-8a27-8fdc1f61d3e2",
   "metadata": {},
   "source": [
    "## 3. BERT-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "034cd7e4-9a06-496d-a02e-87c8d13bf71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_CHECKPOINT = \"google-bert/bert-large-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3f9ef-b482-4520-8009-831e444a531a",
   "metadata": {},
   "source": [
    "### 3.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36874f62-0e9c-4b40-b91c-a23a7b66c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6b9a8-91bd-4be8-b4ef-1df4f51a6c56",
   "metadata": {},
   "source": [
    "#### 3.1.1 Preprocessing training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "857ed85e-4a21-48ea-b3c0-48108bafd8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9173d61e-6e60-4e56-b8f8-f88a7a1e97c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4db6ae-c678-49df-89f4-caf61885989b",
   "metadata": {},
   "source": [
    "#### 3.1.2 Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79e9b1fb-fab3-4e95-965d-f8e1d5ac9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abe5227f-fffa-4a32-a733-4981ac5db985",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = dataset[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda99b6e-8597-4f22-90b5-9ceb59bdcedf",
   "metadata": {},
   "source": [
    "### 3.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9da843dc-1257-4e63-8565-1538882768d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(PRE_TRAINED_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2695fb0f-5385-435f-b41f-ec8ee5b7af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=(TRAIN_OUTPUT_DIR / PRE_TRAINED_CHECKPOINT.replace(\"/\", \"_\")).resolve(),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=2e-5,  # Original paper uses 5e-5\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1feb1160-539e-4644-9ebe-bb5e487d7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43ea9eae-d306-4ed8-8c4a-3278b9774f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- training_arguments.output_dir='/home/e12433721/groups/192.039-2024W/bert/training/squad-single/google-bert_bert-large-uncased'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8301' max='8301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8301/8301 3:32:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2767</td>\n",
       "      <td>1.158200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5534</td>\n",
       "      <td>0.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8301</td>\n",
       "      <td>0.468400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"--- {training_arguments.output_dir=}\")\n",
    "training_summary_bert_large = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c204534e-a500-4028-8da9-3eea0a01e232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8301, training_loss=0.7666577816066965, metrics={'train_runtime': 12760.915, 'train_samples_per_second': 20.811, 'train_steps_per_second': 0.651, 'total_flos': 1.849789299850629e+17, 'train_loss': 0.7666577816066965, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_summary_bert_large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b4b8a-9703-4033-803d-5310712222f9",
   "metadata": {},
   "source": [
    "### 3.3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19011eb0-6afa-4b69-a009-b83f4f767d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3366dd0a3084aaa9dcd048fe9e4a66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "scores_bert_large = compute_metrics(start_logits, end_logits, validation_dataset, dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fd16c67-cd84-497a-8edf-b167e70d3621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>step</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.1582</td>\n",
       "      <td>13.103781</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>2767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6734</td>\n",
       "      <td>15.960748</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>5534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4684</td>\n",
       "      <td>20.557493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8301</td>\n",
       "      <td>12760.915</td>\n",
       "      <td>20.811</td>\n",
       "      <td>0.651</td>\n",
       "      <td>1.849789e+17</td>\n",
       "      <td>0.766658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  grad_norm  learning_rate  step  train_runtime  \\\n",
       "epoch                                                          \n",
       "1      1.1582  13.103781       0.000013  2767            NaN   \n",
       "2      0.6734  15.960748       0.000007  5534            NaN   \n",
       "3      0.4684  20.557493       0.000000  8301      12760.915   \n",
       "\n",
       "       train_samples_per_second  train_steps_per_second    total_flos  \\\n",
       "epoch                                                                   \n",
       "1                           NaN                     NaN           NaN   \n",
       "2                           NaN                     NaN           NaN   \n",
       "3                        20.811                   0.651  1.849789e+17   \n",
       "\n",
       "       train_loss  \n",
       "epoch              \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3        0.766658  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history_bert_large = pd.DataFrame(trainer.state.log_history)\n",
    "training_history_bert_large.epoch = training_history_bert_large.epoch.astype(int)\n",
    "training_history_bert_large.groupby(\"epoch\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2d524d7-e1f7-4459-b630-ec789ffc104e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Model performance:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"BERT_BASE\" and \"BERT_LARGE\" performance on the SQuAD dataset as reported in the original paper.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>our BERT_BASE</th>\n",
       "      <th>original BERT_BASE</th>\n",
       "      <th>our BERT_LARGE</th>\n",
       "      <th>original BERT_LARGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>79.914853</td>\n",
       "      <td>80.8</td>\n",
       "      <td>83.263955</td>\n",
       "      <td>84.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>87.690992</td>\n",
       "      <td>88.5</td>\n",
       "      <td>90.457076</td>\n",
       "      <td>90.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    our BERT_BASE  original BERT_BASE  our BERT_LARGE  original BERT_LARGE\n",
       "EM      79.914853                80.8       83.263955                 84.1\n",
       "F1      87.690992                88.5       90.457076                 90.9"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f\"### Model performance:\"))\n",
    "results[\"our BERT_LARGE\"] = [\n",
    "    scores_bert_large[\"exact_match\"],\n",
    "    scores_bert_large[\"f1\"],\n",
    "]\n",
    "# Achieved scores from original BERT paper:\n",
    "results[\"original BERT_LARGE\"] = [84.1,90.9]\n",
    "\n",
    "results = results[\n",
    "    [\n",
    "        \"our BERT_BASE\",\n",
    "        \"original BERT_BASE\",\n",
    "        \"our BERT_LARGE\",\n",
    "        \"original BERT_LARGE\",\n",
    "    ]\n",
    "]\n",
    "print(\n",
    "    '\"BERT_BASE\" and \"BERT_LARGE\" performance on the SQuAD dataset as reported in the original paper.'\n",
    ")\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
