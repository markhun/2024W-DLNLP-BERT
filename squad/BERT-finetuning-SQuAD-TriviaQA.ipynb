{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469f9176-0d48-4edc-980d-f21a620d0403",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT on SQuAD v1.0 and TriviaQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90420218-2622-4d15-9371-60d1c51e589f",
   "metadata": {},
   "source": [
    "The TriviaQA Dataset is also a question/answer dataset similar to the SQuAD dataset. The paper states that the F1 and EM score improves when finetuning the BERT large model first on the larger TriviaQA dataset and then on the SQuAD dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc98e26-fe74-40fd-846b-a15213dd304e",
   "metadata": {},
   "source": [
    "## 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7488d34-5623-4a2b-b704-36196e66acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Store the huggingface data in a shared group folder on the provided JupyterLab instance.\n",
    "os.environ['HF_HOME'] = '../../groups/192.039-2024W/bert/huggingface/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28bd8ea0-93f0-4499-b1de-f2478534305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from transformers import set_seed\n",
    "\n",
    "# RANDOMNESS SEED\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Which datasets to load\n",
    "DATASET_NAME_SQUAD = \"squad\"\n",
    "DATASET_NAME_TRIVIAQA = \"trivia_qa\"\n",
    "\n",
    "TRAIN_OUTPUT_DIR = (\n",
    "    Path(\"../../groups/192.039-2024W/bert\") / \"training\" / f\"{DATASET_NAME_SQUAD}-{DATASET_NAME_TRIVIAQA}\"\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32  # Original Paper claims to use 32 for the SQuAD task\n",
    "NUM_EPOCHS = 3  # Original Paper claims to use 3 fine-tuning epochs for the SQuAD task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02825b84-76b4-471c-9b9c-eaa261628ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "GPU used: NVIDIA A40\n",
      "\n",
      "==============NVSMI LOG==============\n",
      "\n",
      "Timestamp                                 : Tue Jan 28 13:47:49 2025\n",
      "Driver Version                            : 550.90.07\n",
      "CUDA Version                              : 12.4\n",
      "\n",
      "Attached GPUs                             : 1\n",
      "GPU 00000000:05:00.0\n",
      "    FB Memory Usage\n",
      "        Total                             : 46068 MiB\n",
      "        Reserved                          : 665 MiB\n",
      "        Used                              : 23791 MiB\n",
      "        Free                              : 21613 MiB\n",
      "    BAR1 Memory Usage\n",
      "        Total                             : 65536 MiB\n",
      "        Used                              : 4 MiB\n",
      "        Free                              : 65532 MiB\n",
      "    Conf Compute Protected Memory Usage\n",
      "        Total                             : 0 MiB\n",
      "        Used                              : 0 MiB\n",
      "        Free                              : 0 MiB\n",
      "    Compute Mode                          : Default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  device_count = torch.cuda.device_count()\n",
    "  device_name = torch.cuda.get_device_name(0)\n",
    "\n",
    "  print(f\"There are {device_count} GPU(s) available.\")\n",
    "  print(f\"GPU used: {device_name}\")\n",
    "  ! nvidia-smi -q --display=MEMORY,COMPUTE\n",
    "\n",
    "else:\n",
    "  print(\"No GPU available, using CPU.\")\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb70d3d-3598-4b13-b5a5-a8f49783c0d2",
   "metadata": {},
   "source": [
    "## 1. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1801ce6e-fa91-4357-b6d1-a3bc80365773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bafc8cf96e84ae9a35d68128340ebee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b641d08b6cd740cb89b06d280de86127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6db15dc8594e8f885bd293e7f986e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer'],\n",
       "        num_rows: 138384\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer'],\n",
       "        num_rows: 17944\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'question_id', 'question_source', 'entity_pages', 'search_results', 'answer'],\n",
       "        num_rows: 17210\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset_triviaqa = load_dataset(\"mandarjoshi/trivia_qa\", \"rc\")\n",
    "dataset_triviaqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a4183-cc35-4128-8cd2-c5d846f7f3d3",
   "metadata": {},
   "source": [
    "Since the TriviaQA dataset is quite large and it will take a lot of hours for training, we will use the first 50000 samples of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a5178b8-f23e-4ac8-99f2-63c3154dcb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset_triviaqa = dataset_triviaqa[\"train\"].select(range(50000))\n",
    "len(small_train_dataset_triviaqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e7de3e-0ad2-4efd-82e2-ae52bed2ac56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_squad = load_dataset(DATASET_NAME_SQUAD)\n",
    "dataset_squad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe1035-02a4-4679-87b0-bd2dc96dd821",
   "metadata": {},
   "source": [
    "## 2. Finetuning on TriviaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d476ca62-346a-446b-89aa-a1e67cc67e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_CHECKPOINT = \"google-bert/bert-large-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15284633-06b4-4bd1-82cc-6f0a78d46bb6",
   "metadata": {},
   "source": [
    "### 2.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5e133ea-0892-46c6-9b08-e7e54b6b4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefeeb83-863e-4955-87d1-f6a5bbe61357",
   "metadata": {},
   "source": [
    "#### 2.1.1 Preprocessing training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "131c6f0f-6368-471f-b578-7ef1108379eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_triviaqa_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = []\n",
    "    for entity_page, search_result in zip(examples[\"entity_pages\"], examples[\"search_results\"]):\n",
    "        wiki_context = entity_page.get(\"wiki_context\", [])\n",
    "        search_context = search_result.get(\"search_context\", [])\n",
    "        if wiki_context:\n",
    "            context = wiki_context[0].strip()\n",
    "        elif search_context:\n",
    "            context = search_context[0].strip()\n",
    "        else:\n",
    "            context = \"\"\n",
    "        contexts.append(context)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=512,\n",
    "        truncation=\"only_second\",\n",
    "        stride=min(128, tokenizer.model_max_length // 2),\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    answers = examples[\"answer\"]\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx][\"value\"]\n",
    "        ct = contexts[sample_idx]\n",
    "        answer_start = context.find(answer)\n",
    "        answer_end = answer_start + len(answer)\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        if offset[context_start][0] > answer_start or offset[context_end][1] < answer_end:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= answer_start:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= answer_end:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edd1a94b-a459-42de-8b74-1574fbc8acb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b363f20a7f4e420a8013979816d81cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_triviaqa = small_train_dataset_triviaqa.map(\n",
    "    preprocess_triviaqa_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=small_train_dataset_triviaqa.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e9b3246-d8f2-4f41-9a7d-c56f079c6b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 840275)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_train_dataset_triviaqa), len(train_dataset_triviaqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a8f1a3-4b67-47fa-b742-bc8bcf972f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_ids</th>\n",
       "      <td>[101, 2029, 2137, 1011, 2141, 11881, 2180, 1996, 10501, 3396, 2005, 3906, 1999, 4479, 1029, 102, 1996, 10501, 3396, 1999, 3906, 4479, 1996, 10501, 3396, 1999, 3906, 4479, 11881, 4572, 1996, 10501, 3396, 1999, 3906, 4479, 11881, 4572, 3396, 3745, 1024, 1015, 1013, 1015, 1996, 10501, 3396, 1999, 3906, 4479, 2001, 3018, 2000, 11881, 4572, 1000, 2005, 2010, 21813, 1998, 8425, 2396, 1997, 6412, 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_type_ids</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attention_mask</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_positions</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end_positions</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                               0\n",
       "input_ids        [101, 2029, 2137, 1011, 2141, 11881, 2180, 1996, 10501, 3396, 2005, 3906, 1999, 4479, 1029, 102, 1996, 10501, 3396, 1999, 3906, 4479, 1996, 10501, 3396, 1999, 3906, 4479, 11881, 4572, 1996, 10501, 3396, 1999, 3906, 4479, 11881, 4572, 3396, 3745, 1024, 1015, 1013, 1015, 1996, 10501, 3396, 1999, 3906, 4479, 2001, 3018, 2000, 11881, 4572, 1000, 2005, 2010, 21813, 1998, 8425, 2396, 1997, 6412, 199...\n",
       "token_type_ids                                                                                                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]\n",
       "attention_mask                                                                                                 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]\n",
       "start_positions                                                                                                                                                                                                                                                                                                                                                                                                                0\n",
       "end_positions                                                                                                                                                                                                                                                                                                                                                                                                                  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with pd.option_context('display.max_colwidth', 400):\n",
    "    display(pd.DataFrame(train_dataset_triviaqa[:1]).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f95f5b77-9b90-426b-9409-5b2b33b7265b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>Which American-born Sinclair won the Nobel Prize for Literature in 1930?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_id</th>\n",
       "      <td>tc_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_source</th>\n",
       "      <td>http://www.triviacountry.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entity_pages</th>\n",
       "      <td>{'doc_source': [], 'filename': [], 'title': [], 'wiki_context': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_results</th>\n",
       "      <td>{'description': ['The Nobel Prize in Literature 1930 Sinclair ... The Nobel Prize in Literature 1930 was awarded to ... nobelprize.org/nobel_prizes/literature/laureates/1930/&gt;', 'Why Don’t More Americans Win the Nobel Prize? By . ... When the Nobel Prize in Literature was awarded to Sinclair ... In 1930, Lewis told his Nobel audience that ...', '... Sauk Centre native Sinclair Lewis became the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <td>{'aliases': ['(Harry) Sinclair Lewis', 'Harry Sinclair Lewis', 'Lewis, (Harry) Sinclair', 'Grace Hegger', 'Sinclair Lewis'], 'normalized_aliases': ['grace hegger', 'lewis harry sinclair', 'harry sinclair lewis', 'sinclair lewis'], 'matched_wiki_entity_name': '', 'normalized_matched_wiki_entity_name': '', 'normalized_value': 'sinclair lewis', 'type': 'WikipediaEntity', 'value': 'Sinclair Lewis'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                               0\n",
       "question                                                                                                                                                                                                                                                                                                                                                Which American-born Sinclair won the Nobel Prize for Literature in 1930?\n",
       "question_id                                                                                                                                                                                                                                                                                                                                                                                                                 tc_1\n",
       "question_source                                                                                                                                                                                                                                                                                                                                                                                    http://www.triviacountry.com/\n",
       "entity_pages                                                                                                                                                                                                                                                                                                                                                 {'doc_source': [], 'filename': [], 'title': [], 'wiki_context': []}\n",
       "search_results   {'description': ['The Nobel Prize in Literature 1930 Sinclair ... The Nobel Prize in Literature 1930 was awarded to ... nobelprize.org/nobel_prizes/literature/laureates/1930/>', 'Why Don’t More Americans Win the Nobel Prize? By . ... When the Nobel Prize in Literature was awarded to Sinclair ... In 1930, Lewis told his Nobel audience that ...', '... Sauk Centre native Sinclair Lewis became the...\n",
       "answer             {'aliases': ['(Harry) Sinclair Lewis', 'Harry Sinclair Lewis', 'Lewis, (Harry) Sinclair', 'Grace Hegger', 'Sinclair Lewis'], 'normalized_aliases': ['grace hegger', 'lewis harry sinclair', 'harry sinclair lewis', 'sinclair lewis'], 'matched_wiki_entity_name': '', 'normalized_matched_wiki_entity_name': '', 'normalized_value': 'sinclair lewis', 'type': 'WikipediaEntity', 'value': 'Sinclair Lewis'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with pd.option_context('display.max_colwidth', 400):\n",
    "    display(pd.DataFrame(small_train_dataset_triviaqa[:1]).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01027528-ff3b-4821-90c0-3cc12bc80d03",
   "metadata": {},
   "source": [
    "#### 2.1.2 Preprocessing validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57038a79-ef56-4ef9-9d73-8d4975120f06",
   "metadata": {},
   "source": [
    "After preprocessing the training dataset, we will preprocess the validation dataset. This differs a little bit from the preprocessing of the training dataset, because we do not need to generate labels. This would only be necessary when we want to compute a validation loss, but since that number does not really tell us if the model is good or not, we will not compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "816b744d-f021-4339-878c-c8450ef71ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_triviaqa_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = []\n",
    "    for entity_page, search_result in zip(examples[\"entity_pages\"], examples[\"search_results\"]):\n",
    "        wiki_context = entity_page.get(\"wiki_context\", [])\n",
    "        search_context = search_result.get(\"search_context\", [])\n",
    "        if wiki_context:\n",
    "            context = wiki_context[0].strip()\n",
    "        elif search_context:\n",
    "            context = search_context[0].strip()\n",
    "        else:\n",
    "            context = \"\"\n",
    "        contexts.append(context)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=512,\n",
    "        truncation=\"only_second\",\n",
    "        stride=min(128, tokenizer.model_max_length // 2),\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    # Map each overflowed tokenization back to its corresponding example\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"question_id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    # Add example IDs for evaluation\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61107a13-9111-4353-b229-c8d241b20882",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset_triviaqa = dataset_triviaqa[\"validation\"].map(\n",
    "    preprocess_triviaqa_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_triviaqa[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bda8588a-dc9d-4c95-bda3-7f702be9fb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17944, 296523)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_triviaqa[\"validation\"]), len(validation_dataset_triviaqa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73242dad-f062-461f-a316-b2aca428aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_triviaqa = dataset_triviaqa[\"test\"].map(\n",
    "    preprocess_triviaqa_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_triviaqa[\"test\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "093f21a0-eb63-479a-861b-886e5c41a357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17210, 284990)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_triviaqa[\"test\"]), len(test_dataset_triviaqa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb09854b-06db-43a7-b3cf-e5fe66c1922b",
   "metadata": {},
   "source": [
    "### 2.2 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8d12481-d56a-46c9-9d81-3efacd93028a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"squad\", module_type: \"metric\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\n",
       "Computes SQuAD scores (F1 and EM).\n",
       "Args:\n",
       "    predictions: List of question-answers dictionaries with the following key-values:\n",
       "        - 'id': id of the question-answer pair as given in the references (see below)\n",
       "        - 'prediction_text': the text of the answer\n",
       "    references: List of question-answers dictionaries with the following key-values:\n",
       "        - 'id': id of the question-answer pair (see above),\n",
       "        - 'answers': a Dict in the SQuAD dataset format\n",
       "            {\n",
       "                'text': list of possible texts for the answer, as a list of strings\n",
       "                'answer_start': list of start positions for the answer, as a list of ints\n",
       "            }\n",
       "            Note that answer_start values are not taken into account to compute the metric.\n",
       "Returns:\n",
       "    'exact_match': Exact match (the normalized answer exactly match the gold answer)\n",
       "    'f1': The F-score of predicted tokens versus the gold answer\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n",
       "    >>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
       "    >>> squad_metric = evaluate.load(\"squad\")\n",
       "    >>> results = squad_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'exact_match': 100.0, 'f1': 100.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Since there is no specific metric for the TriviaQA dataset, we will load the metric for the SQuAD dataset.\n",
    "# We can do that, because they are both question/answering datasets and we are preprocessing them the same way.\n",
    "metric = evaluate.load(DATASET_NAME_SQUAD)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d8133e3-e2fe-4c64-bc7d-0a065965ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "\n",
    "def compute_metrics_triviaqa(eval_pred):\n",
    "    predictions, _ = eval_pred\n",
    "    start_logits, end_logits = predictions\n",
    "    features = validation_dataset_triviaqa\n",
    "    examples = dataset_triviaqa[\"validation\"]\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"question_id\"]\n",
    "        contexts = []\n",
    "        for entity_page, search_result in zip(examples[\"entity_pages\"], examples[\"search_results\"]):\n",
    "            wiki_context = entity_page.get(\"wiki_context\", [])\n",
    "            search_context = search_result.get(\"search_context\", [])\n",
    "            if wiki_context:\n",
    "                context = wiki_context[0].strip()\n",
    "            elif search_context:\n",
    "                context = search_context[0].strip()\n",
    "            else:\n",
    "                context = \"\"\n",
    "            contexts.append(context)\n",
    "            answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa42319-28c8-480c-8498-89ca102fa7f7",
   "metadata": {},
   "source": [
    "### 2.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2223f3b-d664-4209-ac28-75cadb1a59f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(PRE_TRAINED_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6708bcfc-8f4b-4cab-a425-b93bca85d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=(TRAIN_OUTPUT_DIR / PRE_TRAINED_CHECKPOINT.replace(\"/\", \"_\")).resolve(),\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5640b0cd-62d1-4859-aa22-fc022a457548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset_triviaqa,\n",
    "    eval_dataset=validation_dataset_triviaqa,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics_triviaqa\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d7d2411-01da-408f-a580-001687934c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- training_arguments.output_dir='/home/e12433721/groups/192.039-2024W/bert/training/squad-trivia_qa/google-bert_bert-large-uncased'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='157554' max='157554' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157554/157554 18:59:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.020800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.239500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.239500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"--- {training_arguments.output_dir=}\")\n",
    "training_summary_bert_large_triviaqa = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "016e41ee-5c50-4ce1-bb94-8dfa250165db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=157554, training_loss=5.499922447224444, metrics={'train_runtime': 68380.4298, 'train_samples_per_second': 36.865, 'train_steps_per_second': 2.304, 'total_flos': 2.3411078034310656e+18, 'train_loss': 5.499922447224444, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_summary_bert_large_triviaqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a5fb6ba-bb06-4bf2-a06b-c2e9b9fb0024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to: /home/e12433721/groups/192.039-2024W/bert/training/squad-trivia_qa/triviaqa_model\n"
     ]
    }
   ],
   "source": [
    "# Define the save directory for the fine-tuned model\n",
    "triviaqa_model_dir = (TRAIN_OUTPUT_DIR / \"triviaqa_model\").resolve()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(triviaqa_model_dir)\n",
    "tokenizer.save_pretrained(triviaqa_model_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved to: {triviaqa_model_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa61568-a31f-4628-921f-d71fb5dfe128",
   "metadata": {},
   "source": [
    "### 2.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a8a01a1-be24-4237-9d78-115fd7af1a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0208</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.333613e-05</td>\n",
       "      <td>52518</td>\n",
       "      <td>2045.2202</td>\n",
       "      <td>144.983</td>\n",
       "      <td>9.062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.2395</td>\n",
       "      <td>2.873855</td>\n",
       "      <td>6.671998e-06</td>\n",
       "      <td>105036</td>\n",
       "      <td>2026.4040</td>\n",
       "      <td>146.330</td>\n",
       "      <td>9.146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.2395</td>\n",
       "      <td>2.443461</td>\n",
       "      <td>8.124199e-09</td>\n",
       "      <td>157554</td>\n",
       "      <td>2018.2761</td>\n",
       "      <td>146.919</td>\n",
       "      <td>9.183</td>\n",
       "      <td>68380.4298</td>\n",
       "      <td>36.865</td>\n",
       "      <td>2.304</td>\n",
       "      <td>2.341108e+18</td>\n",
       "      <td>5.499922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  grad_norm  learning_rate    step  eval_runtime  \\\n",
       "epoch                                                           \n",
       "1      4.0208        inf   1.333613e-05   52518     2045.2202   \n",
       "2      6.2395   2.873855   6.671998e-06  105036     2026.4040   \n",
       "3      6.2395   2.443461   8.124199e-09  157554     2018.2761   \n",
       "\n",
       "       eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "epoch                                                                  \n",
       "1                      144.983                  9.062            NaN   \n",
       "2                      146.330                  9.146            NaN   \n",
       "3                      146.919                  9.183     68380.4298   \n",
       "\n",
       "       train_samples_per_second  train_steps_per_second    total_flos  \\\n",
       "epoch                                                                   \n",
       "1                           NaN                     NaN           NaN   \n",
       "2                           NaN                     NaN           NaN   \n",
       "3                        36.865                   2.304  2.341108e+18   \n",
       "\n",
       "       train_loss  \n",
       "epoch              \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3        5.499922  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history_bert_large_triviaqa = pd.DataFrame(trainer.state.log_history)\n",
    "training_history_bert_large_triviaqa.epoch = training_history_bert_large_triviaqa.epoch.astype(int)\n",
    "training_history_bert_large_triviaqa.groupby(\"epoch\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52afbebc-d697-4fc4-8a27-8fdc1f61d3e2",
   "metadata": {},
   "source": [
    "## 3. Finetuning on SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "034cd7e4-9a06-496d-a02e-87c8d13bf71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_CHECKPOINT = triviaqa_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3f9ef-b482-4520-8009-831e444a531a",
   "metadata": {},
   "source": [
    "### 3.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36874f62-0e9c-4b40-b91c-a23a7b66c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6b9a8-91bd-4be8-b4ef-1df4f51a6c56",
   "metadata": {},
   "source": [
    "#### 3.1.1 Preprocessing training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "857ed85e-4a21-48ea-b3c0-48108bafd8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_squad_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9173d61e-6e60-4e56-b8f8-f88a7a1e97c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4240f807d1d947bc9c9e5c87d6521aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_squad = dataset_squad[\"train\"].map(\n",
    "    preprocess_squad_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_squad[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4db6ae-c678-49df-89f4-caf61885989b",
   "metadata": {},
   "source": [
    "#### 3.1.2 Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79e9b1fb-fab3-4e95-965d-f8e1d5ac9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_squad_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "abe5227f-fffa-4a32-a733-4981ac5db985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a31ebd2fd940a8b5edc84d2fdb199e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_dataset_squad = dataset_squad[\"validation\"].map(\n",
    "    preprocess_squad_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_squad[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2fee6-87b1-4bfe-80fb-5faf1f9fb418",
   "metadata": {},
   "source": [
    "### 3.2 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0afc16e3-cae9-4967-845b-3f5b5ff9e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "\n",
    "def compute_metrics_squad(eval_pred):\n",
    "    predictions, _ = eval_pred\n",
    "    start_logits, end_logits = predictions\n",
    "    features = validation_dataset_squad\n",
    "    examples = dataset_squad[\"validation\"]\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda99b6e-8597-4f22-90b5-9ceb59bdcedf",
   "metadata": {},
   "source": [
    "### 3.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9da843dc-1257-4e63-8565-1538882768d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(PRE_TRAINED_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2695fb0f-5385-435f-b41f-ec8ee5b7af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=(TRAIN_OUTPUT_DIR / PRE_TRAINED_CHECKPOINT).resolve(),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=2e-5,  # Original paper uses 5e-5\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1feb1160-539e-4644-9ebe-bb5e487d7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset_squad,\n",
    "    eval_dataset=validation_dataset_squad,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics_squad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43ea9eae-d306-4ed8-8c4a-3278b9774f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- training_arguments.output_dir='/home/e12433721/groups/192.039-2024W/bert/training/squad-trivia_qa/triviaqa_model'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8301' max='8301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8301/8301 1:16:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.950800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.950900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.951000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"--- {training_arguments.output_dir=}\")\n",
    "training_summary_bert_large_squad = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c204534e-a500-4028-8da9-3eea0a01e232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8301, training_loss=5.9509137641549215, metrics={'train_runtime': 4572.3403, 'train_samples_per_second': 58.082, 'train_steps_per_second': 1.815, 'total_flos': 1.849789299850629e+17, 'train_loss': 5.9509137641549215, 'epoch': 3.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_summary_bert_large_squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9410339-d9b3-4f70-8652-20a09b12515d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.9508</td>\n",
       "      <td>1.369943</td>\n",
       "      <td>1.333333e-05</td>\n",
       "      <td>2767</td>\n",
       "      <td>48.6471</td>\n",
       "      <td>221.678</td>\n",
       "      <td>6.927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.9509</td>\n",
       "      <td>1.302679</td>\n",
       "      <td>6.669076e-06</td>\n",
       "      <td>5534</td>\n",
       "      <td>48.7133</td>\n",
       "      <td>221.377</td>\n",
       "      <td>6.918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.9510</td>\n",
       "      <td>1.335975</td>\n",
       "      <td>7.228045e-09</td>\n",
       "      <td>8301</td>\n",
       "      <td>48.7052</td>\n",
       "      <td>221.414</td>\n",
       "      <td>6.919</td>\n",
       "      <td>4572.3403</td>\n",
       "      <td>58.082</td>\n",
       "      <td>1.815</td>\n",
       "      <td>1.849789e+17</td>\n",
       "      <td>5.950914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  grad_norm  learning_rate  step  eval_runtime  \\\n",
       "epoch                                                         \n",
       "1      5.9508   1.369943   1.333333e-05  2767       48.6471   \n",
       "2      5.9509   1.302679   6.669076e-06  5534       48.7133   \n",
       "3      5.9510   1.335975   7.228045e-09  8301       48.7052   \n",
       "\n",
       "       eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "epoch                                                                  \n",
       "1                      221.678                  6.927            NaN   \n",
       "2                      221.377                  6.918            NaN   \n",
       "3                      221.414                  6.919      4572.3403   \n",
       "\n",
       "       train_samples_per_second  train_steps_per_second    total_flos  \\\n",
       "epoch                                                                   \n",
       "1                           NaN                     NaN           NaN   \n",
       "2                           NaN                     NaN           NaN   \n",
       "3                        58.082                   1.815  1.849789e+17   \n",
       "\n",
       "       train_loss  \n",
       "epoch              \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3        5.950914  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history_bert_large_squad = pd.DataFrame(trainer.state.log_history)\n",
    "training_history_bert_large_squad.epoch = training_history_bert_large_squad.epoch.astype(int)\n",
    "training_history_bert_large_squad.groupby(\"epoch\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a32e8918-cc6d-4989-8f0c-ecb07ac3a883",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['eval_f1', 'eval_exact_match'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_history_bert_large_squad\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_f1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_exact_match\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m data[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/2024W-DLNLP-BERT/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/2024W-DLNLP-BERT/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/2024W-DLNLP-BERT/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['eval_f1', 'eval_exact_match'] not in index\""
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = training_history_bert_large_squad[[\"eval_f1\", \"eval_exact_match\", \"epoch\"]]\n",
    "data.columns = [\"F1\", \"EM\", \"Training Epoch\"]\n",
    "data = data[:-1]\n",
    "data = pd.melt(data, ['Training Epoch']).dropna()\n",
    "\n",
    "plot = sns.lineplot(data=data, x=\"Training Epoch\", y=\"value\", hue=\"variable\", style=\"variable\", markers=True)\n",
    "plot.set_ylabel(\"\")\n",
    "plot.set(xticks=list(set(training_history_bert_large_squad.epoch)))\n",
    "plot.set_ylim((0, plot.get_ylim()[1]))\n",
    "plot.legend(title=\"\")\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"### Loss and Evaluation Metrics over Training Epochs ({PRE_TRAINED_CHECKPOINT})\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b4b8a-9703-4033-803d-5310712222f9",
   "metadata": {},
   "source": [
    "### 3.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fbcc9d3f-ecf1-4a36-875f-0102c08ca5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"squad\", module_type: \"metric\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\n",
       "Computes SQuAD scores (F1 and EM).\n",
       "Args:\n",
       "    predictions: List of question-answers dictionaries with the following key-values:\n",
       "        - 'id': id of the question-answer pair as given in the references (see below)\n",
       "        - 'prediction_text': the text of the answer\n",
       "    references: List of question-answers dictionaries with the following key-values:\n",
       "        - 'id': id of the question-answer pair (see above),\n",
       "        - 'answers': a Dict in the SQuAD dataset format\n",
       "            {\n",
       "                'text': list of possible texts for the answer, as a list of strings\n",
       "                'answer_start': list of start positions for the answer, as a list of ints\n",
       "            }\n",
       "            Note that answer_start values are not taken into account to compute the metric.\n",
       "Returns:\n",
       "    'exact_match': Exact match (the normalized answer exactly match the gold answer)\n",
       "    'f1': The F-score of predicted tokens versus the gold answer\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n",
       "    >>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
       "    >>> squad_metric = evaluate.load(\"squad\")\n",
       "    >>> results = squad_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'exact_match': 100.0, 'f1': 100.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Since there is no specific metric for the TriviaQA dataset, we will load the metric for the SQuAD dataset.\n",
    "# We can do that, because they are both question/answering datasets and we are preprocessing them the same way.\n",
    "metric = evaluate.load(DATASET_NAME_SQUAD)\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b4f3f98-4ae5-4ca2-891f-569ca191f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "\n",
    "def compute_metrics_triviaqa_test(predictions):\n",
    "    start_logits, end_logits = predictions\n",
    "    features = test_dataset_triviaqa\n",
    "    examples = dataset_triviaqa[\"test\"]\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"question_id\"]\n",
    "        contexts = []\n",
    "        for entity_page, search_result in zip(examples[\"entity_pages\"], examples[\"search_results\"]):\n",
    "            wiki_context = entity_page.get(\"wiki_context\", [])\n",
    "            search_context = search_result.get(\"search_context\", [])\n",
    "            if wiki_context:\n",
    "                context = wiki_context[0].strip()\n",
    "            elif search_context:\n",
    "                context = search_context[0].strip()\n",
    "            else:\n",
    "                context = \"\"\n",
    "            contexts.append(context)\n",
    "            answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fafb1d7f-b20a-43c5-a2bf-352aadf39fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test_dataset_triviaqa = dataset_triviaqa[\"test\"].select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51385344-e883-4a27-90d8-694d4c22c8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada7e487f765477abfd1dbca15db0aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "small_tokenized_test_dataset_triviaqa = small_test_dataset_triviaqa.map(\n",
    "    preprocess_triviaqa_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_triviaqa[\"test\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "19011eb0-6afa-4b69-a009-b83f4f767d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4436c0c4c5ff4f9489574070a78d32e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 154492 is out of bounds for axis 0 with size 9011",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m predictions, _, _ \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(small_tokenized_test_dataset_triviaqa)\n\u001b[0;32m----> 2\u001b[0m scores_bert_large_squad \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_metrics_triviaqa_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 34\u001b[0m, in \u001b[0;36mcompute_metrics_triviaqa_test\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Loop through all features associated with that example\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature_index \u001b[38;5;129;01min\u001b[39;00m example_to_features[example_id]:\n\u001b[0;32m---> 34\u001b[0m     start_logit \u001b[38;5;241m=\u001b[39m \u001b[43mstart_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     35\u001b[0m     end_logit \u001b[38;5;241m=\u001b[39m end_logits[feature_index]\n\u001b[1;32m     36\u001b[0m     offsets \u001b[38;5;241m=\u001b[39m features[feature_index][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 154492 is out of bounds for axis 0 with size 9011"
     ]
    }
   ],
   "source": [
    "predictions, _, _ = trainer.predict(small_tokenized_test_dataset_triviaqa)\n",
    "scores_bert_large_squad = compute_metrics_triviaqa_test(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d524d7-e1f7-4459-b630-ec789ffc104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(f\"### Model performance:\"))\n",
    "final_results = pd.DataFrame(\n",
    "    [scores_bert_large_squad[\"exact_match\"]] + [scores_bert_large_squad[\"f1\"]],\n",
    "    index=[\"EM\"] + [\"F1\"],\n",
    "    columns=[\"our BERT_LARGE on TriviaQA Dev\"],\n",
    ")\n",
    "\n",
    "# Achieved scores from original BERT paper:\n",
    "final_results[\"original BERT_LARGE on TriviaQA Dev\"] = [84.2,91.1]\n",
    "final_results[\"our BERT_LARGE on TriviaQA Test\"] = [training_history_bert_large_squad[\"eval_exact_match\"] + training_history_bert_large_squad[\"eval_f1\"]]\n",
    "final_results[\"original BERT_LARGE on TriviaQA Test\"] = [85.1, 91.8]\n",
    "\n",
    "print(\n",
    "    '\"BERT_LARGE\" performance on the TriviaQA and SQuAD dataset as reported in the original paper.'\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc65377f-3ce3-40eb-8d72-68ae33020ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
