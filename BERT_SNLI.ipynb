{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30698,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('snli')\n",
    "\n",
    "labels = dataset.unique(\"label\").keys()\n",
    "num_labels = len(labels)\n",
    "# label -1 is used if the gold label is missing, see: https://github.com/huggingface/datasets/issues/296\n",
    "# therefore we remove all entries with label -1\n",
    "dataset = dataset.filter(lambda x: x[\"label\"] != -1)\n",
    "\n",
    "# shuffle the dataset to avoid any bias\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# rename label to labels, because the model expects a column named labels\n",
    "dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "print(f\"{labels=}\")\n",
    "print(f\"{num_labels=}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-20T06:04:40.529557Z",
     "iopub.execute_input": "2024-06-20T06:04:40.530245Z",
     "iopub.status.idle": "2024-06-20T06:04:50.113628Z",
     "shell.execute_reply.started": "2024-06-20T06:04:40.530215Z",
     "shell.execute_reply": "2024-06-20T06:04:50.112587Z"
    },
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class NLIClassifier(nn.Module):\n",
    "    def __init__(self, model, num_labels=3, freeze_bert=False):\n",
    "        super(NLIClassifier, self).__init__()\n",
    "        self.bert = model.to(device)\n",
    "        # freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # Extract [CLS] token representation\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "model = BertModel.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    torch_dtype=\"auto\",\n",
    ").to(device)\n",
    "nli_model = NLIClassifier(model, num_labels=3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_dataset = dataset.map(lambda x: tokenizer(x[\"premise\"], x[\"hypothesis\"], add_special_tokens=True, truncation=True), batched=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-20T06:04:51.664603Z",
     "iopub.execute_input": "2024-06-20T06:04:51.665249Z",
     "iopub.status.idle": "2024-06-20T06:04:51.845661Z",
     "shell.execute_reply.started": "2024-06-20T06:04:51.665216Z",
     "shell.execute_reply": "2024-06-20T06:04:51.844476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Dataloader with collate function moving data to device\n",
    "# batch.items() returns probably something like this:\n",
    "#[\n",
    "#  ('input_ids', tensor([[101, 2054, 2003, 2023, 102], [101, 2029, 3185, 2003, 102]])),\n",
    "#  ('attention_mask', tensor([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]])),\n",
    "#  ('labels', tensor([0, 1]))\n",
    "#]\n",
    "# so we do not need the keys, just the values and send them to the device\n",
    "def collate_fn_with_device(batch):\n",
    "    batch = data_collator(batch)\n",
    "    return {k: v.to(device) for k, v in batch.items()}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-20T06:04:53.670448Z",
     "iopub.execute_input": "2024-06-20T06:04:53.671600Z",
     "iopub.status.idle": "2024-06-20T06:04:53.690695Z",
     "shell.execute_reply.started": "2024-06-20T06:04:53.671554Z",
     "shell.execute_reply": "2024-06-20T06:04:53.689448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# remove all text columns, because they are already tokenized\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"premise\", \"hypothesis\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
    "traindataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=32, shuffle=True, collate_fn=collate_fn_with_device)\n",
    "evaldataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=32, shuffle=True, collate_fn=collate_fn_with_device)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Results:\n",
    "    def __init__(self):\n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "        self.epoch = []\n",
    "\n",
    "    def add(self, loss, acc, epoch):\n",
    "        self.loss.append(loss.item())\n",
    "        self.acc.append(acc)\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Loss: {self.loss}, Accuracy: {self.acc}, Epoch: {self.epoch}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "# optimizer = AdamW(nli_model.parameters(),lr=5e-5)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, nli_model.parameters()), lr=5e-5)\n",
    "\n",
    "from transformers import get_scheduler\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(traindataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "nli_model.to(device)\n",
    "nli_model.train()\n",
    "\n",
    "training_results = Results()\n",
    "validation_results = Results()\n",
    "i = 1\n",
    "j = 1\n",
    "# https://discuss.pytorch.org/t/how-to-calculate-the-validation-loss-during-each-epoch-of-training/145272/3\n",
    "for epoch in range(num_epochs):\n",
    "    nli_model.train()\n",
    "    for batch in traindataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = nli_model(**batch)\n",
    "        loss = criterion(outputs, batch[\"labels\"])\n",
    "        train_acc = (torch.argmax(outputs, dim=-1) == batch[\"labels\"]).float().mean()\n",
    "        current_epoch = i / len(traindataloader)\n",
    "        training_results.add(loss, train_acc.item(), current_epoch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "        i += 1\n",
    "\n",
    "    nli_model.eval()\n",
    "    for batch_eval in evaldataloader:\n",
    "        # for model inference we don't need gradients\n",
    "        with torch.no_grad():\n",
    "            outputs_eval = nli_model(**batch_eval)\n",
    "        current_epoch = j / len(evaldataloader)\n",
    "        acc = (torch.argmax(outputs_eval, dim=-1) == batch_eval[\"labels\"]).float().mean()\n",
    "        loss = criterion(outputs_eval, batch_eval[\"labels\"])\n",
    "        validation_results.add(loss, acc.item(), current_epoch)\n",
    "        j += 1\n",
    "\n",
    "torch.save(model, \"model.pth\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pickle\n",
    "\n",
    "# see https://stackoverflow.com/questions/4529815/saving-an-object-data-persistence\n",
    "\n",
    "with open('training.pkl', 'wb') as outp:\n",
    "    pickle.dump(training_results, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('validation.pkl', 'wb') as outp:\n",
    "    pickle.dump(validation_results, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot Training Loss vs Evaluation Loss\n",
    "plt.plot(training_results.epoch, training_results.loss, label='trainging Loss', color='red', marker='o')\n",
    "plt.plot(validation_results.epoch, validation_results.loss, label='Evaluation loss', color='green', marker='x')\n",
    "\n",
    "# Labels and Title\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training Loss vs Evaluation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-20T06:04:56.627164Z",
     "iopub.execute_input": "2024-06-20T06:04:56.627533Z",
     "iopub.status.idle": "2024-06-20T06:04:56.644063Z",
     "shell.execute_reply.started": "2024-06-20T06:04:56.627501Z",
     "shell.execute_reply": "2024-06-20T06:04:56.643015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "testdataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=32, shuffle=True, collate_fn=collate_fn_with_device)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "nli_model.eval()\n",
    "for batch in testdataloader:\n",
    "    # for model inference we don't need gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = nli_model(**batch)\n",
    "    predictions = torch.argmax(outputs, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(training_results.epoch, training_results.acc, label='Training Accuracy', color='green', marker='x')\n",
    "plt.plot(validation_results.epoch, validation_results.acc, label='Eval Accuracy', color='red', marker='o')\n",
    "\n",
    "# Labels and Title\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Training Accuarcy vs Evaluation Accuarcy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"confusion_matrix\")\n",
    "nli_model.eval()\n",
    "for batch in testdataloader:\n",
    "    # for model inference we don't need gradients\n",
    "    with torch.no_grad():\n",
    "        outputs = nli_model(**batch)\n",
    "\n",
    "    predictions = torch.argmax(outputs, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "nli_model.eval()\n",
    "\n",
    "# Iterate through the test dataloader\n",
    "for batch in testdataloader:\n",
    "    with torch.no_grad():\n",
    "        outputs = nli_model(**batch)\n",
    "        predictions = torch.argmax(outputs, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = metric.compute()['confusion_matrix']\n",
    "labels = [\"Neutral\", \"Entailment\", \"Contradiction\"]\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}
