{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT on GLUE - QNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding - Wang et al.](https://arxiv.org/pdf/1804.07461):\n",
    "\n",
    "The Stanford Question Answering Dataset (Rajpurkar et al. 2016) is a question-answering dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph (drawn\n",
    "from Wikipedia) contains the answer to the corresponding question (written by an annotator). We convert the task into sentence pair classification by forming a pair between each question and each\n",
    "sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains\n",
    "the answer to the question. This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer\n",
    "is always present in the input and that lexical overlap is a reliable cue. This process of recasting existing datasets into NLI is similar to methods introduced in White et al. (2017) and expanded\n",
    "upon in Demszky et al. (2018). We call the converted dataset QNLI (Question-answering NLI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Where to store the huggingface data. On the provided Jupyterlab instance that should be within the shared group folder.\n",
    "os.environ['HF_HOME'] = '../groups/192.039-2024W/bert/huggingface/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from transformers import set_seed\n",
    "\n",
    "# RANDOMNESS SEED\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Which dataset to load\n",
    "DATASET_NAME = \"glue\"\n",
    "DATASET_TASK = \"qnli\"\n",
    "\n",
    "PRE_TRAINED_CHECKPOINT = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "TRAIN_OUTPUT_DIR = (\n",
    "    Path(\"../groups/192.039-2024W/bert\") / \"training\" / f\"{DATASET_NAME}-{DATASET_TASK}\"\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32  # Original Paper claims to use 32 for GLUE tasks\n",
    "NUM_EPOCHS = 5  # Original Paper claims to use 3 fine-tuning epochs for GLUE tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "GPU used: NVIDIA GeForce RTX 4060 Ti\n",
      "\n",
      "==============NVSMI LOG==============\n",
      "\n",
      "Timestamp                                 : Sun Jan  5 13:14:26 2025\n",
      "Driver Version                            : 550.135\n",
      "CUDA Version                              : 12.4\n",
      "\n",
      "Attached GPUs                             : 1\n",
      "GPU 00000000:07:00.0\n",
      "    FB Memory Usage\n",
      "        Total                             : 16380 MiB\n",
      "        Reserved                          : 307 MiB\n",
      "        Used                              : 1155 MiB\n",
      "        Free                              : 14919 MiB\n",
      "    BAR1 Memory Usage\n",
      "        Total                             : 256 MiB\n",
      "        Used                              : 27 MiB\n",
      "        Free                              : 229 MiB\n",
      "    Conf Compute Protected Memory Usage\n",
      "        Total                             : 0 MiB\n",
      "        Used                              : 0 MiB\n",
      "        Free                              : 0 MiB\n",
      "    Compute Mode                          : Default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  device_count = torch.cuda.device_count()\n",
    "  device_name = torch.cuda.get_device_name(0)\n",
    "\n",
    "  print(f\"There are {device_count} GPU(s) available.\")\n",
    "  print(f\"GPU used: {device_name}\")\n",
    "  ! nvidia-smi -q --display=MEMORY,COMPUTE\n",
    "\n",
    "else:\n",
    "  print(\"No GPU available, using CPU.\")\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the GLUE dataset different tasks have different accessor keys\n",
    "_task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'sentence', 'label', 'idx'],\n",
       "        num_rows: 104743\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'sentence', 'label', 'idx'],\n",
       "        num_rows: 5463\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'sentence', 'label', 'idx'],\n",
       "        num_rows: 5463\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME, DATASET_TASK)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64724</th>\n",
       "      <td>In what year did Robert Louis Stevenson die?</td>\n",
       "      <td>Mission work in Samoa had begun in late 1830 b...</td>\n",
       "      <td>1</td>\n",
       "      <td>64724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38119</th>\n",
       "      <td>How was the Everton FC's crest redesign receiv...</td>\n",
       "      <td>The redesign was poorly received by supporters...</td>\n",
       "      <td>0</td>\n",
       "      <td>38119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41924</th>\n",
       "      <td>Beyonc√© was an honorary chair of the 2013 what?</td>\n",
       "      <td>She was also honorary chair of the 2013 Met Gala.</td>\n",
       "      <td>0</td>\n",
       "      <td>41924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42888</th>\n",
       "      <td>Who was the author of Conversations on the Plu...</td>\n",
       "      <td>Sarah Trimmer wrote a successful natural histo...</td>\n",
       "      <td>1</td>\n",
       "      <td>42888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47148</th>\n",
       "      <td>What is essential for the mating of the elemen...</td>\n",
       "      <td>Antennas are required by any radio receiver or...</td>\n",
       "      <td>0</td>\n",
       "      <td>47148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42142</th>\n",
       "      <td>What are some courses Eton offers in the summe...</td>\n",
       "      <td>These comparatively new developments will run ...</td>\n",
       "      <td>1</td>\n",
       "      <td>42142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19057</th>\n",
       "      <td>What does \"open mouth operations\" mean?</td>\n",
       "      <td>Fractional deposit lending (changes in the res...</td>\n",
       "      <td>0</td>\n",
       "      <td>19057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69777</th>\n",
       "      <td>Who was the head of state of Swaziland after K...</td>\n",
       "      <td>Following the elections of 1973, the constitut...</td>\n",
       "      <td>1</td>\n",
       "      <td>69777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32661</th>\n",
       "      <td>Why did Julius Caesar wish to invade Gaul?</td>\n",
       "      <td>When two local tribes began to migrate on a ro...</td>\n",
       "      <td>1</td>\n",
       "      <td>32661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61664</th>\n",
       "      <td>How is the city commonly abbreviated?</td>\n",
       "      <td>This motto was quickly adopted as a city nickn...</td>\n",
       "      <td>1</td>\n",
       "      <td>61664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "64724       In what year did Robert Louis Stevenson die?   \n",
       "38119  How was the Everton FC's crest redesign receiv...   \n",
       "41924    Beyonc√© was an honorary chair of the 2013 what?   \n",
       "42888  Who was the author of Conversations on the Plu...   \n",
       "47148  What is essential for the mating of the elemen...   \n",
       "42142  What are some courses Eton offers in the summe...   \n",
       "19057            What does \"open mouth operations\" mean?   \n",
       "69777  Who was the head of state of Swaziland after K...   \n",
       "32661         Why did Julius Caesar wish to invade Gaul?   \n",
       "61664              How is the city commonly abbreviated?   \n",
       "\n",
       "                                                sentence  label    idx  \n",
       "64724  Mission work in Samoa had begun in late 1830 b...      1  64724  \n",
       "38119  The redesign was poorly received by supporters...      0  38119  \n",
       "41924  She was also honorary chair of the 2013 Met Gala.      0  41924  \n",
       "42888  Sarah Trimmer wrote a successful natural histo...      1  42888  \n",
       "47148  Antennas are required by any radio receiver or...      0  47148  \n",
       "42142  These comparatively new developments will run ...      1  42142  \n",
       "19057  Fractional deposit lending (changes in the res...      0  19057  \n",
       "69777  Following the elections of 1973, the constitut...      1  69777  \n",
       "32661  When two local tribes began to migrate on a ro...      1  32661  \n",
       "61664  This motto was quickly adopted as a city nickn...      1  61664  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset[\"train\"]).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_lables_in_dataset=array([1, 0])\n",
      "num_labels=2\n"
     ]
    }
   ],
   "source": [
    "unique_lables_in_dataset = pd.DataFrame(dataset[\"train\"])[\"label\"].unique()\n",
    "num_labels = len(unique_lables_in_dataset)\n",
    "\n",
    "print(f\"{unique_lables_in_dataset=}\")\n",
    "print(f\"{num_labels=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_CHECKPOINT, do_lower_case=\"uncased\" in PRE_TRAINED_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT has a maximum sequence length of 512. We can check the sequence lengths resulting from tokenizing our dataset to see if our dataset exceeds this restriction of BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length in split='train': 550\n",
      "Max length in split='validation': 250\n",
      "Max length in split='test': 294\n"
     ]
    }
   ],
   "source": [
    "first_sentence_key, second_sentence_key = _task_to_keys[DATASET_TASK]\n",
    "\n",
    "if second_sentence_key == None:  # Simply tokenize sentence\n",
    "\n",
    "    for split in dataset.keys():\n",
    "        max_len = 0\n",
    "        for sentence in dataset[split][first_sentence_key]:\n",
    "            # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "            input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "            \n",
    "            max_len = max(max_len, len(input_ids))\n",
    "        \n",
    "\n",
    "        print(f\"Max length in {split=}: {max_len}\")\n",
    "\n",
    "else:  # Append both sentences via [SEP] and tokenize\n",
    "\n",
    "    for split in dataset.keys():\n",
    "        max_len = 0\n",
    "        for sentence1, sentence2 in zip(dataset[split][first_sentence_key], dataset[split][second_sentence_key]):\n",
    "            # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "            input_ids = tokenizer.encode(sentence1, sentence2,  add_special_tokens=True)\n",
    "            \n",
    "            max_len = max(max_len, len(input_ids))\n",
    "        \n",
    "\n",
    "        print(f\"Max length in {split=}: {max_len}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset contains sequences longer than the maximum sequence length for BERT. This will be handled via truncation by our tokenization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05571a4d74846f8894592ba33ea0053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5463 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_func(item):\n",
    "    \"\"\"Tokenize passed item. \n",
    "    \n",
    "    Depending on dataset task the passed item will either contain one sentence or two sentences.\n",
    "    In the last case the two sentences will be appended via a [SEP] token.\n",
    "    \"\"\"\n",
    "    if second_sentence_key is None:\n",
    "        return tokenizer(item[first_sentence_key], add_special_tokens=True, truncation=True)\n",
    "    else:\n",
    "        return tokenizer(item[first_sentence_key], item[second_sentence_key], add_special_tokens=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a tokenized dataset item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ['When did the third Digimon series begin?'],\n",
       " 'sentence': ['Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese.'],\n",
       " 'label': [1],\n",
       " 'idx': [0]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ['When did the third Digimon series begin?'],\n",
       " 'sentence': ['Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese.'],\n",
       " 'label': [1],\n",
       " 'idx': [0],\n",
       " 'input_ids': [[101,\n",
       "   2043,\n",
       "   2106,\n",
       "   1996,\n",
       "   2353,\n",
       "   10667,\n",
       "   16339,\n",
       "   2078,\n",
       "   2186,\n",
       "   4088,\n",
       "   1029,\n",
       "   102,\n",
       "   4406,\n",
       "   1996,\n",
       "   2048,\n",
       "   3692,\n",
       "   2077,\n",
       "   2009,\n",
       "   1998,\n",
       "   2087,\n",
       "   1997,\n",
       "   1996,\n",
       "   3692,\n",
       "   2008,\n",
       "   2628,\n",
       "   1010,\n",
       "   10667,\n",
       "   16339,\n",
       "   2078,\n",
       "   24763,\n",
       "   2869,\n",
       "   3138,\n",
       "   1037,\n",
       "   9904,\n",
       "   1998,\n",
       "   2062,\n",
       "   12689,\n",
       "   3921,\n",
       "   2000,\n",
       "   2049,\n",
       "   2466,\n",
       "   3794,\n",
       "   10667,\n",
       "   16339,\n",
       "   2078,\n",
       "   2040,\n",
       "   2079,\n",
       "   2025,\n",
       "   27788,\n",
       "   10010,\n",
       "   12556,\n",
       "   2044,\n",
       "   2037,\n",
       "   6677,\n",
       "   1998,\n",
       "   2062,\n",
       "   3375,\n",
       "   2839,\n",
       "   2458,\n",
       "   1999,\n",
       "   1996,\n",
       "   2434,\n",
       "   2887,\n",
       "   1012,\n",
       "   102]],\n",
       " 'token_type_ids': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization added the `input_ids` field, which contains the tokenized sentence with a `[CLS]`(101) and two `[SEP]`(102) tokens added. A `token_type_ids` field which indicates first and second portion of the inputs, if necessary. And an `attention_mask` for the given input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface's `transformers` library provides a `DataCollatorWithPadding` class, which allows us to use dynamic padding.  \n",
    "Dynamic padding will add `[PAD]` tokens to the length of the longest sequence within a batch, instead of padding to the maximum sequence length within the entire dataset.  \n",
    "This will avoid unnecessary padding and therefore improve execution efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>2043</td>\n",
       "      <td>2106</td>\n",
       "      <td>1996</td>\n",
       "      <td>2353</td>\n",
       "      <td>10667</td>\n",
       "      <td>16339</td>\n",
       "      <td>2078</td>\n",
       "      <td>2186</td>\n",
       "      <td>4088</td>\n",
       "      <td>...</td>\n",
       "      <td>2062.0</td>\n",
       "      <td>3375.0</td>\n",
       "      <td>2839.0</td>\n",
       "      <td>2458.0</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>2434.0</td>\n",
       "      <td>2887.0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>2029</td>\n",
       "      <td>7421</td>\n",
       "      <td>10274</td>\n",
       "      <td>2411</td>\n",
       "      <td>2031</td>\n",
       "      <td>3265</td>\n",
       "      <td>22742</td>\n",
       "      <td>2015</td>\n",
       "      <td>2195</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>2054</td>\n",
       "      <td>2048</td>\n",
       "      <td>2477</td>\n",
       "      <td>2515</td>\n",
       "      <td>3769</td>\n",
       "      <td>4842</td>\n",
       "      <td>7475</td>\n",
       "      <td>16985</td>\n",
       "      <td>5488</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2      3     4      5      6      7      8     9   ...  \\\n",
       "0  101  2043  2106   1996  2353  10667  16339   2078   2186  4088  ...   \n",
       "1  101  2029  7421  10274  2411   2031   3265  22742   2015  2195  ...   \n",
       "2  101  2054  2048   2477  2515   3769   4842   7475  16985  5488  ...   \n",
       "\n",
       "       55      56      57      58      59      60      61      62      63  \\\n",
       "0  2062.0  3375.0  2839.0  2458.0  1999.0  1996.0  2434.0  2887.0  1012.0   \n",
       "1     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "      64  \n",
       "0  102.0  \n",
       "1    NaN  \n",
       "2    NaN  \n",
       "\n",
       "[3 rows x 65 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Example: Select a few samples from the training set\n",
    "samples = tokenized_dataset[\"train\"][:3]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", first_sentence_key, second_sentence_key]}  # Drop `idx` and `sentence` columns, as DataCollator can't process those.\n",
    "pd.DataFrame(samples[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>2043</td>\n",
       "      <td>2106</td>\n",
       "      <td>1996</td>\n",
       "      <td>2353</td>\n",
       "      <td>10667</td>\n",
       "      <td>16339</td>\n",
       "      <td>2078</td>\n",
       "      <td>2186</td>\n",
       "      <td>4088</td>\n",
       "      <td>...</td>\n",
       "      <td>2062</td>\n",
       "      <td>3375</td>\n",
       "      <td>2839</td>\n",
       "      <td>2458</td>\n",
       "      <td>1999</td>\n",
       "      <td>1996</td>\n",
       "      <td>2434</td>\n",
       "      <td>2887</td>\n",
       "      <td>1012</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>2029</td>\n",
       "      <td>7421</td>\n",
       "      <td>10274</td>\n",
       "      <td>2411</td>\n",
       "      <td>2031</td>\n",
       "      <td>3265</td>\n",
       "      <td>22742</td>\n",
       "      <td>2015</td>\n",
       "      <td>2195</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>2054</td>\n",
       "      <td>2048</td>\n",
       "      <td>2477</td>\n",
       "      <td>2515</td>\n",
       "      <td>3769</td>\n",
       "      <td>4842</td>\n",
       "      <td>7475</td>\n",
       "      <td>16985</td>\n",
       "      <td>5488</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2      3     4      5      6      7      8     9   ...    55  \\\n",
       "0  101  2043  2106   1996  2353  10667  16339   2078   2186  4088  ...  2062   \n",
       "1  101  2029  7421  10274  2411   2031   3265  22742   2015  2195  ...     0   \n",
       "2  101  2054  2048   2477  2515   3769   4842   7475  16985  5488  ...     0   \n",
       "\n",
       "     56    57    58    59    60    61    62    63   64  \n",
       "0  3375  2839  2458  1999  1996  2434  2887  1012  102  \n",
       "1     0     0     0     0     0     0     0     0    0  \n",
       "2     0     0     0     0     0     0     0     0    0  \n",
       "\n",
       "[3 rows x 65 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply padding using data_collator\n",
    "batch = data_collator(samples)\n",
    "pd.DataFrame(batch[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `data_collator` will insert `[PAD]` (0) tokens to the maximum length of the passed batch of data items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GLUE dataset specifies one or more evaluation metrics depending on the selected task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"glue\", module_type: \"metric\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "Examples:\n",
       "\n",
       "    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\n",
       "    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0, 'f1': 1.0}\n",
       "\n",
       "    >>> glue_metric = evaluate.load('glue', 'stsb')\n",
       "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
       "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
       "\n",
       "    >>> glue_metric = evaluate.load('glue', 'cola')\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'matthews_correlation': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(DATASET_NAME, DATASET_TASK)\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the selected GLUE task we optimize for different evaluation metrics. See BERT paper p.6:\n",
    "\n",
    "> F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_task_to_metric = {\n",
    "    \"cola\": \"matthews_correlation\",\n",
    "    \"mnli\": \"accuracy\",\n",
    "    \"mnli-mm\": \"accuracy\",\n",
    "    \"mrpc\": \"f1\",\n",
    "    \"qnli\": \"accuracy\",\n",
    "    \"qqp\": \"f1\",\n",
    "    \"rte\": \"accuracy\",\n",
    "    \"sst2\": \"accuracy\",\n",
    "    \"stsb\": \"spearmanr\",\n",
    "}\n",
    "\n",
    "metric_for_best_model = _task_to_metric[DATASET_TASK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use \"['accuracy']\" as an evaluation metric for the task qnli\n"
     ]
    }
   ],
   "source": [
    "def get_metric_name_for_specific_task():\n",
    "    \"\"\"Helper function to derive the evaluation metric name for the specified GLUE task.\n",
    "\n",
    "    The tasks specified by the GLUE benchmark use different evaluation metrics.\n",
    "    Unfortunatly there is no easy way to derive there name after loading the corresponding metric function via HuggingFace's `evaluate` library.\n",
    "    However we can simply do a \"trial run\" and expect the name key of its output.\n",
    "    \"\"\"\n",
    "    output = metric.compute(\n",
    "        predictions=[1, 0], references=[1, 1]\n",
    "    )  # dummy input - we just want to inspect the returned dictionary.\n",
    "    metric_names = output.keys()\n",
    "    \n",
    "    return list(metric_names)\n",
    "\n",
    "\n",
    "metric_names = get_metric_name_for_specific_task()\n",
    "print(f'We will use \"{metric_names}\" as an evaluation metric for the task {DATASET_TASK}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert metric_for_best_model in metric_names, \"Metric to optimize for not found in evaluation metrics provided by GLUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    PRE_TRAINED_CHECKPOINT,\n",
    "    num_labels=num_labels,\n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=TRAIN_OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1000,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=2e-5,  # Original paper uses best out of  5e-5, 4e-5, 3e-5, and 2e-5\n",
    "    weight_decay=0.01,  # Original paper uses 0.01 on pre-training\n",
    "    save_total_limit = 3,  # Keep at most the three checkpoints (latest + best one)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_for_best_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if DATASET_TASK != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "validation_key = \"validation_mismatched\" if DATASET_TASK == \"mnli-mm\" else \"validation_matched\" if DATASET_TASK == \"mnli\" else \"validation\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[validation_key],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- training_arguments.output_dir='../groups/192.039-2024W/bert/training/glue-qnli'\n",
      "--- training_arguments.metric_for_best_model='accuracy'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f2804268424008bf45e1206140815f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.408, 'grad_norm': 5.035590648651123, 'learning_rate': 1.8778252901649362e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f4932e0ce54484b75ee070caad8615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2824442386627197, 'eval_accuracy': 0.8848617975471352, 'eval_runtime': 15.8096, 'eval_samples_per_second': 345.55, 'eval_steps_per_second': 10.816, 'epoch': 0.31}\n",
      "{'loss': 0.3248, 'grad_norm': 7.9803786277771, 'learning_rate': 1.755650580329872e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573b373a87b147c69b39b4a70423968b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2805670499801636, 'eval_accuracy': 0.8817499542375984, 'eval_runtime': 11.6114, 'eval_samples_per_second': 470.485, 'eval_steps_per_second': 14.727, 'epoch': 0.61}\n",
      "{'loss': 0.2967, 'grad_norm': 5.119344234466553, 'learning_rate': 1.6334758704948076e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73af530f9d1d429eba71225c3751c58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2327309548854828, 'eval_accuracy': 0.9101226432363171, 'eval_runtime': 11.6546, 'eval_samples_per_second': 468.742, 'eval_steps_per_second': 14.672, 'epoch': 0.92}\n",
      "{'loss': 0.2224, 'grad_norm': 3.0939462184906006, 'learning_rate': 1.5113011606597437e-05, 'epoch': 1.22}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec554ab48904d82a07c462f302004f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2557765543460846, 'eval_accuracy': 0.9062786015010068, 'eval_runtime': 16.0961, 'eval_samples_per_second': 339.399, 'eval_steps_per_second': 10.624, 'epoch': 1.22}\n",
      "{'loss': 0.195, 'grad_norm': 8.610628128051758, 'learning_rate': 1.3891264508246794e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1c9addfb75437c924c235a6de79035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23751363158226013, 'eval_accuracy': 0.9141497345780707, 'eval_runtime': 15.4616, 'eval_samples_per_second': 353.327, 'eval_steps_per_second': 11.06, 'epoch': 1.53}\n",
      "{'loss': 0.201, 'grad_norm': 3.541438102722168, 'learning_rate': 1.2669517409896153e-05, 'epoch': 1.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c87ed01f96140259535d99ddce2a272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23961345851421356, 'eval_accuracy': 0.9128683873329673, 'eval_runtime': 16.3388, 'eval_samples_per_second': 334.358, 'eval_steps_per_second': 10.466, 'epoch': 1.83}\n",
      "{'loss': 0.1594, 'grad_norm': 5.338031768798828, 'learning_rate': 1.1447770311545512e-05, 'epoch': 2.14}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6e8ca61f904a1d9ec6bc293a50bbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31283828616142273, 'eval_accuracy': 0.9112209408749771, 'eval_runtime': 16.2547, 'eval_samples_per_second': 336.088, 'eval_steps_per_second': 10.52, 'epoch': 2.14}\n",
      "{'loss': 0.1168, 'grad_norm': 14.386604309082031, 'learning_rate': 1.0226023213194869e-05, 'epoch': 2.44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f5a144c4d84ee4b1a652d415c3cdff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30391186475753784, 'eval_accuracy': 0.914881933003844, 'eval_runtime': 16.1739, 'eval_samples_per_second': 337.766, 'eval_steps_per_second': 10.573, 'epoch': 2.44}\n",
      "{'loss': 0.1205, 'grad_norm': 8.410170555114746, 'learning_rate': 9.004276114844227e-06, 'epoch': 2.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf435ed3574413bb30235413a080af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3115503191947937, 'eval_accuracy': 0.9112209408749771, 'eval_runtime': 16.0631, 'eval_samples_per_second': 340.095, 'eval_steps_per_second': 10.645, 'epoch': 2.75}\n",
      "{'loss': 0.1132, 'grad_norm': 2.536112070083618, 'learning_rate': 7.782529016493586e-06, 'epoch': 3.05}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6006cc68e2fc4339b4ca082967a37802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3802693486213684, 'eval_accuracy': 0.9119531393007505, 'eval_runtime': 16.2314, 'eval_samples_per_second': 336.57, 'eval_steps_per_second': 10.535, 'epoch': 3.05}\n",
      "{'loss': 0.0747, 'grad_norm': 2.296729803085327, 'learning_rate': 6.560781918142944e-06, 'epoch': 3.36}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9e5e5c99914020aa997635a04878cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3942102789878845, 'eval_accuracy': 0.9123192385136372, 'eval_runtime': 16.1683, 'eval_samples_per_second': 337.883, 'eval_steps_per_second': 10.576, 'epoch': 3.36}\n",
      "{'loss': 0.0759, 'grad_norm': 2.495047092437744, 'learning_rate': 5.339034819792304e-06, 'epoch': 3.67}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444954037b9a4967918ee71433ed3d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3836275637149811, 'eval_accuracy': 0.9088412959912137, 'eval_runtime': 16.151, 'eval_samples_per_second': 338.246, 'eval_steps_per_second': 10.588, 'epoch': 3.67}\n",
      "{'loss': 0.0749, 'grad_norm': 33.62152862548828, 'learning_rate': 4.117287721441662e-06, 'epoch': 3.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550c2f8a3f94461c8a1d03265f429f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3924527168273926, 'eval_accuracy': 0.9136005857587406, 'eval_runtime': 16.2699, 'eval_samples_per_second': 335.773, 'eval_steps_per_second': 10.51, 'epoch': 3.97}\n",
      "{'loss': 0.0491, 'grad_norm': 31.06774139404297, 'learning_rate': 2.8955406230910206e-06, 'epoch': 4.28}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63b5ecb5b054f1dbd9e7a663f7ea41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4592088460922241, 'eval_accuracy': 0.9123192385136372, 'eval_runtime': 16.2839, 'eval_samples_per_second': 335.484, 'eval_steps_per_second': 10.501, 'epoch': 4.28}\n",
      "{'loss': 0.0484, 'grad_norm': 1.7823187112808228, 'learning_rate': 1.6737935247403788e-06, 'epoch': 4.58}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705766240b4f4cfbb89ecbd81acfe6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46036460995674133, 'eval_accuracy': 0.9134175361522973, 'eval_runtime': 16.0863, 'eval_samples_per_second': 339.605, 'eval_steps_per_second': 10.63, 'epoch': 4.58}\n",
      "{'loss': 0.0483, 'grad_norm': 34.86168670654297, 'learning_rate': 4.5204642638973736e-07, 'epoch': 4.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00dca48f84d54333978f4714a8453e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44440314173698425, 'eval_accuracy': 0.9146988833974007, 'eval_runtime': 16.1817, 'eval_samples_per_second': 337.604, 'eval_steps_per_second': 10.568, 'epoch': 4.89}\n",
      "{'train_runtime': 5297.4735, 'train_samples_per_second': 98.861, 'train_steps_per_second': 3.09, 'train_loss': 0.1554638878921947, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"--- {training_arguments.output_dir=}\")\n",
    "print(f\"--- {training_arguments.metric_for_best_model=}\")\n",
    "training_summary = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16370, training_loss=0.1554638878921947, metrics={'train_runtime': 5297.4735, 'train_samples_per_second': 98.861, 'train_steps_per_second': 3.09, 'total_flos': 2.7216140760897e+16, 'train_loss': 0.1554638878921947, 'epoch': 5.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `trainer.evaluate()` to check that the `trainer` instance did indeed reload the model checkpoint with the highest evaluation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1777ade80f741c88f8a597fc2154f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.30391186475753784,\n",
       " 'eval_accuracy': 0.914881933003844,\n",
       " 'eval_runtime': 15.8736,\n",
       " 'eval_samples_per_second': 344.156,\n",
       " 'eval_steps_per_second': 10.773,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_evaluation = trainer.evaluate()\n",
    "best_model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.4080</td>\n",
       "      <td>5.035591</td>\n",
       "      <td>1.877825e-05</td>\n",
       "      <td>0.305437</td>\n",
       "      <td>0.282444</td>\n",
       "      <td>0.884862</td>\n",
       "      <td>15.8096</td>\n",
       "      <td>345.550</td>\n",
       "      <td>10.816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.3248</td>\n",
       "      <td>7.980379</td>\n",
       "      <td>1.755651e-05</td>\n",
       "      <td>0.610874</td>\n",
       "      <td>0.280567</td>\n",
       "      <td>0.881750</td>\n",
       "      <td>11.6114</td>\n",
       "      <td>470.485</td>\n",
       "      <td>14.727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>0.2967</td>\n",
       "      <td>5.119344</td>\n",
       "      <td>1.633476e-05</td>\n",
       "      <td>0.916310</td>\n",
       "      <td>0.232731</td>\n",
       "      <td>0.910123</td>\n",
       "      <td>11.6546</td>\n",
       "      <td>468.742</td>\n",
       "      <td>14.672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.2224</td>\n",
       "      <td>3.093946</td>\n",
       "      <td>1.511301e-05</td>\n",
       "      <td>1.221747</td>\n",
       "      <td>0.255777</td>\n",
       "      <td>0.906279</td>\n",
       "      <td>16.0961</td>\n",
       "      <td>339.399</td>\n",
       "      <td>10.624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.1950</td>\n",
       "      <td>8.610628</td>\n",
       "      <td>1.389126e-05</td>\n",
       "      <td>1.527184</td>\n",
       "      <td>0.237514</td>\n",
       "      <td>0.914150</td>\n",
       "      <td>15.4616</td>\n",
       "      <td>353.327</td>\n",
       "      <td>11.060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>0.2010</td>\n",
       "      <td>3.541438</td>\n",
       "      <td>1.266952e-05</td>\n",
       "      <td>1.832621</td>\n",
       "      <td>0.239613</td>\n",
       "      <td>0.912868</td>\n",
       "      <td>16.3388</td>\n",
       "      <td>334.358</td>\n",
       "      <td>10.466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>0.1594</td>\n",
       "      <td>5.338032</td>\n",
       "      <td>1.144777e-05</td>\n",
       "      <td>2.138057</td>\n",
       "      <td>0.312838</td>\n",
       "      <td>0.911221</td>\n",
       "      <td>16.2547</td>\n",
       "      <td>336.088</td>\n",
       "      <td>10.520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>0.1168</td>\n",
       "      <td>14.386604</td>\n",
       "      <td>1.022602e-05</td>\n",
       "      <td>2.443494</td>\n",
       "      <td>0.303912</td>\n",
       "      <td>0.914882</td>\n",
       "      <td>16.1739</td>\n",
       "      <td>337.766</td>\n",
       "      <td>10.573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>0.1205</td>\n",
       "      <td>8.410171</td>\n",
       "      <td>9.004276e-06</td>\n",
       "      <td>2.748931</td>\n",
       "      <td>0.311550</td>\n",
       "      <td>0.911221</td>\n",
       "      <td>16.0631</td>\n",
       "      <td>340.095</td>\n",
       "      <td>10.645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.1132</td>\n",
       "      <td>2.536112</td>\n",
       "      <td>7.782529e-06</td>\n",
       "      <td>3.054368</td>\n",
       "      <td>0.380269</td>\n",
       "      <td>0.911953</td>\n",
       "      <td>16.2314</td>\n",
       "      <td>336.570</td>\n",
       "      <td>10.535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11000</th>\n",
       "      <td>0.0747</td>\n",
       "      <td>2.296730</td>\n",
       "      <td>6.560782e-06</td>\n",
       "      <td>3.359805</td>\n",
       "      <td>0.394210</td>\n",
       "      <td>0.912319</td>\n",
       "      <td>16.1683</td>\n",
       "      <td>337.883</td>\n",
       "      <td>10.576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12000</th>\n",
       "      <td>0.0759</td>\n",
       "      <td>2.495047</td>\n",
       "      <td>5.339035e-06</td>\n",
       "      <td>3.665241</td>\n",
       "      <td>0.383628</td>\n",
       "      <td>0.908841</td>\n",
       "      <td>16.1510</td>\n",
       "      <td>338.246</td>\n",
       "      <td>10.588</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13000</th>\n",
       "      <td>0.0749</td>\n",
       "      <td>33.621529</td>\n",
       "      <td>4.117288e-06</td>\n",
       "      <td>3.970678</td>\n",
       "      <td>0.392453</td>\n",
       "      <td>0.913601</td>\n",
       "      <td>16.2699</td>\n",
       "      <td>335.773</td>\n",
       "      <td>10.510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000</th>\n",
       "      <td>0.0491</td>\n",
       "      <td>31.067741</td>\n",
       "      <td>2.895541e-06</td>\n",
       "      <td>4.276115</td>\n",
       "      <td>0.459209</td>\n",
       "      <td>0.912319</td>\n",
       "      <td>16.2839</td>\n",
       "      <td>335.484</td>\n",
       "      <td>10.501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>0.0484</td>\n",
       "      <td>1.782319</td>\n",
       "      <td>1.673794e-06</td>\n",
       "      <td>4.581552</td>\n",
       "      <td>0.460365</td>\n",
       "      <td>0.913418</td>\n",
       "      <td>16.0863</td>\n",
       "      <td>339.605</td>\n",
       "      <td>10.630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16000</th>\n",
       "      <td>0.0483</td>\n",
       "      <td>34.861687</td>\n",
       "      <td>4.520464e-07</td>\n",
       "      <td>4.886988</td>\n",
       "      <td>0.444403</td>\n",
       "      <td>0.914699</td>\n",
       "      <td>16.1817</td>\n",
       "      <td>337.604</td>\n",
       "      <td>10.568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16370</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.303912</td>\n",
       "      <td>0.914882</td>\n",
       "      <td>15.8736</td>\n",
       "      <td>344.156</td>\n",
       "      <td>10.773</td>\n",
       "      <td>5297.4735</td>\n",
       "      <td>98.861</td>\n",
       "      <td>3.09</td>\n",
       "      <td>2.721614e+16</td>\n",
       "      <td>0.155464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  grad_norm  learning_rate     epoch  eval_loss  eval_accuracy  \\\n",
       "step                                                                          \n",
       "1000   0.4080   5.035591   1.877825e-05  0.305437   0.282444       0.884862   \n",
       "2000   0.3248   7.980379   1.755651e-05  0.610874   0.280567       0.881750   \n",
       "3000   0.2967   5.119344   1.633476e-05  0.916310   0.232731       0.910123   \n",
       "4000   0.2224   3.093946   1.511301e-05  1.221747   0.255777       0.906279   \n",
       "5000   0.1950   8.610628   1.389126e-05  1.527184   0.237514       0.914150   \n",
       "6000   0.2010   3.541438   1.266952e-05  1.832621   0.239613       0.912868   \n",
       "7000   0.1594   5.338032   1.144777e-05  2.138057   0.312838       0.911221   \n",
       "8000   0.1168  14.386604   1.022602e-05  2.443494   0.303912       0.914882   \n",
       "9000   0.1205   8.410171   9.004276e-06  2.748931   0.311550       0.911221   \n",
       "10000  0.1132   2.536112   7.782529e-06  3.054368   0.380269       0.911953   \n",
       "11000  0.0747   2.296730   6.560782e-06  3.359805   0.394210       0.912319   \n",
       "12000  0.0759   2.495047   5.339035e-06  3.665241   0.383628       0.908841   \n",
       "13000  0.0749  33.621529   4.117288e-06  3.970678   0.392453       0.913601   \n",
       "14000  0.0491  31.067741   2.895541e-06  4.276115   0.459209       0.912319   \n",
       "15000  0.0484   1.782319   1.673794e-06  4.581552   0.460365       0.913418   \n",
       "16000  0.0483  34.861687   4.520464e-07  4.886988   0.444403       0.914699   \n",
       "16370     NaN        NaN            NaN  5.000000   0.303912       0.914882   \n",
       "\n",
       "       eval_runtime  eval_samples_per_second  eval_steps_per_second  \\\n",
       "step                                                                  \n",
       "1000        15.8096                  345.550                 10.816   \n",
       "2000        11.6114                  470.485                 14.727   \n",
       "3000        11.6546                  468.742                 14.672   \n",
       "4000        16.0961                  339.399                 10.624   \n",
       "5000        15.4616                  353.327                 11.060   \n",
       "6000        16.3388                  334.358                 10.466   \n",
       "7000        16.2547                  336.088                 10.520   \n",
       "8000        16.1739                  337.766                 10.573   \n",
       "9000        16.0631                  340.095                 10.645   \n",
       "10000       16.2314                  336.570                 10.535   \n",
       "11000       16.1683                  337.883                 10.576   \n",
       "12000       16.1510                  338.246                 10.588   \n",
       "13000       16.2699                  335.773                 10.510   \n",
       "14000       16.2839                  335.484                 10.501   \n",
       "15000       16.0863                  339.605                 10.630   \n",
       "16000       16.1817                  337.604                 10.568   \n",
       "16370       15.8736                  344.156                 10.773   \n",
       "\n",
       "       train_runtime  train_samples_per_second  train_steps_per_second  \\\n",
       "step                                                                     \n",
       "1000             NaN                       NaN                     NaN   \n",
       "2000             NaN                       NaN                     NaN   \n",
       "3000             NaN                       NaN                     NaN   \n",
       "4000             NaN                       NaN                     NaN   \n",
       "5000             NaN                       NaN                     NaN   \n",
       "6000             NaN                       NaN                     NaN   \n",
       "7000             NaN                       NaN                     NaN   \n",
       "8000             NaN                       NaN                     NaN   \n",
       "9000             NaN                       NaN                     NaN   \n",
       "10000            NaN                       NaN                     NaN   \n",
       "11000            NaN                       NaN                     NaN   \n",
       "12000            NaN                       NaN                     NaN   \n",
       "13000            NaN                       NaN                     NaN   \n",
       "14000            NaN                       NaN                     NaN   \n",
       "15000            NaN                       NaN                     NaN   \n",
       "16000            NaN                       NaN                     NaN   \n",
       "16370      5297.4735                    98.861                    3.09   \n",
       "\n",
       "         total_flos  train_loss  \n",
       "step                             \n",
       "1000            NaN         NaN  \n",
       "2000            NaN         NaN  \n",
       "3000            NaN         NaN  \n",
       "4000            NaN         NaN  \n",
       "5000            NaN         NaN  \n",
       "6000            NaN         NaN  \n",
       "7000            NaN         NaN  \n",
       "8000            NaN         NaN  \n",
       "9000            NaN         NaN  \n",
       "10000           NaN         NaN  \n",
       "11000           NaN         NaN  \n",
       "12000           NaN         NaN  \n",
       "13000           NaN         NaN  \n",
       "14000           NaN         NaN  \n",
       "15000           NaN         NaN  \n",
       "16000           NaN         NaN  \n",
       "16370  2.721614e+16    0.155464  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history = pd.DataFrame(trainer.state.log_history)\n",
    "training_history.groupby(\"step\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Loss and Evaluation Metrics over Training Steps"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = training_history[[\"loss\", \"eval_loss\", \"step\", f\"eval_{metric_for_best_model}\"]]\n",
    "data.columns = [\"Train. Loss\", \"Eval. Loss\", \"Training Steps\", \"Acc.\"]\n",
    "data = pd.melt(data, ['Training Steps'])\n",
    "\n",
    "plot = sns.lineplot(data=data, x=\"Training Steps\", y=\"value\", hue=\"variable\", style=\"variable\", markers=True)\n",
    "plot.set_ylabel(\"\")\n",
    "plot.set_ylim((0, plot.get_ylim()[1]))\n",
    "plot.legend(title=\"\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"### Loss and Evaluation Metrics over Training Steps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Best Model performance:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Our Model\" based on google-bert/bert-base-uncased, best performance on validation data.\n",
      "\"BERT_BASE\" and \"BERT_LARGE\" performance on GLUE testing data as reported in original paper.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Our Model</th>\n",
       "      <th>original BERT_BASE</th>\n",
       "      <th>original BERT_LARGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_loss</th>\n",
       "      <td>0.303912</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_accuracy</th>\n",
       "      <td>0.914882</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Our Model original BERT_BASE original BERT_LARGE\n",
       "eval_loss       0.303912                  -                   -\n",
       "eval_accuracy   0.914882              0.905               0.927"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(f\"### Best Model performance:\"))\n",
    "results = pd.DataFrame(\n",
    "    best_model_evaluation.values(),\n",
    "    index=best_model_evaluation.keys(),\n",
    "    columns=[\"Our Model\"],\n",
    ").drop(\n",
    "    # Drop runtime measurements\n",
    "    index=[\"eval_runtime\", \"eval_samples_per_second\", \"eval_steps_per_second\", \"epoch\"]\n",
    ")\n",
    "# Achieved scores from original BERT paper:\n",
    "results[\"original BERT_BASE\"] = [\"-\", 0.905]\n",
    "results[\"original BERT_LARGE\"] = [\"-\", 0.927]\n",
    "print(f'\"Our Model\" based on {PRE_TRAINED_CHECKPOINT}, best performance on validation data.')\n",
    "print('\"BERT_BASE\" and \"BERT_LARGE\" performance on GLUE testing data as reported in original paper.')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
