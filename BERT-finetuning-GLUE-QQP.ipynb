{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT on GLUE - QQP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding - Wang et al.](https://arxiv.org/pdf/1804.07461):\n",
    "\n",
    "The Quora Question Pairs dataset is a collection of question pairs from the community\n",
    "question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent. As in MRPC, the class distribution in QQP is unbalanced (63% negative), so we\n",
    "report both accuracy and F1 score. We use the standard test set, for which we obtained private labels\n",
    "from the authors. We observe that the test set has a different label distribution than the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Where to store the huggingface data. On the provided Jupyterlab instance that should be within the shared group folder.\n",
    "os.environ['HF_HOME'] = '../groups/192.039-2024W/bert/huggingface/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from transformers import set_seed\n",
    "\n",
    "# RANDOMNESS SEED\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Which dataset to load\n",
    "DATASET_NAME = \"glue\"\n",
    "DATASET_TASK = \"qqp\"\n",
    "\n",
    "PRE_TRAINED_CHECKPOINT = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "TRAIN_OUTPUT_DIR = (\n",
    "    Path(\"../groups/192.039-2024W/bert\") / \"training\" / f\"{DATASET_NAME}-{DATASET_TASK}\"\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32  # Original Paper claims to use 32 for GLUE tasks\n",
    "NUM_EPOCHS = 5  # Original Paper claims to use 3 fine-tuning epochs for GLUE tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "GPU used: NVIDIA GeForce RTX 4060 Ti\n",
      "\n",
      "==============NVSMI LOG==============\n",
      "\n",
      "Timestamp                                 : Sun Jan  5 16:12:32 2025\n",
      "Driver Version                            : 550.135\n",
      "CUDA Version                              : 12.4\n",
      "\n",
      "Attached GPUs                             : 1\n",
      "GPU 00000000:07:00.0\n",
      "    FB Memory Usage\n",
      "        Total                             : 16380 MiB\n",
      "        Reserved                          : 307 MiB\n",
      "        Used                              : 1633 MiB\n",
      "        Free                              : 14442 MiB\n",
      "    BAR1 Memory Usage\n",
      "        Total                             : 256 MiB\n",
      "        Used                              : 22 MiB\n",
      "        Free                              : 234 MiB\n",
      "    Conf Compute Protected Memory Usage\n",
      "        Total                             : 0 MiB\n",
      "        Used                              : 0 MiB\n",
      "        Free                              : 0 MiB\n",
      "    Compute Mode                          : Default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  device_count = torch.cuda.device_count()\n",
    "  device_name = torch.cuda.get_device_name(0)\n",
    "\n",
    "  print(f\"There are {device_count} GPU(s) available.\")\n",
    "  print(f\"GPU used: {device_name}\")\n",
    "  ! nvidia-smi -q --display=MEMORY,COMPUTE\n",
    "\n",
    "else:\n",
    "  print(\"No GPU available, using CPU.\")\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the GLUE dataset different tasks have different accessor keys\n",
    "_task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 363846\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 40430\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 390965\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME, DATASET_TASK)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>215130</th>\n",
       "      <td>Why do some people think that having a baby is...</td>\n",
       "      <td>Why is having a baby a blessing?</td>\n",
       "      <td>1</td>\n",
       "      <td>215130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234600</th>\n",
       "      <td>How vulnerable are paratroopers on their desce...</td>\n",
       "      <td>Are conventional paratroopers obsolete?</td>\n",
       "      <td>0</td>\n",
       "      <td>234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118406</th>\n",
       "      <td>Examples of sole proprietorship? The</td>\n",
       "      <td>How are sole proprietorships started?</td>\n",
       "      <td>0</td>\n",
       "      <td>118406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284746</th>\n",
       "      <td>Why don't I get answers for some of my questio...</td>\n",
       "      <td>Why do some questions get more answers here in...</td>\n",
       "      <td>1</td>\n",
       "      <td>284746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300683</th>\n",
       "      <td>Do Kashmiris wants to join Pakistan?</td>\n",
       "      <td>Do the people of Kashmir want to join Pakistan...</td>\n",
       "      <td>0</td>\n",
       "      <td>300683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311541</th>\n",
       "      <td>I am bald on the right side above my ear becau...</td>\n",
       "      <td>I'm losing my hair from the front and sides an...</td>\n",
       "      <td>0</td>\n",
       "      <td>311541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335211</th>\n",
       "      <td>Which is the best and free small business acco...</td>\n",
       "      <td>Which is the best free accounting software for...</td>\n",
       "      <td>1</td>\n",
       "      <td>335211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124167</th>\n",
       "      <td>How do I calculate e^-0.4 without a calculator...</td>\n",
       "      <td>How do you find 75 percent of a number without...</td>\n",
       "      <td>0</td>\n",
       "      <td>124167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83077</th>\n",
       "      <td>Why do colors look more red in one eye and mor...</td>\n",
       "      <td>Why would one eye see different colors than th...</td>\n",
       "      <td>1</td>\n",
       "      <td>83077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79161</th>\n",
       "      <td>What are sound waves?</td>\n",
       "      <td>What are transverse and longitudinal sound waves?</td>\n",
       "      <td>0</td>\n",
       "      <td>79161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question1  \\\n",
       "215130  Why do some people think that having a baby is...   \n",
       "234600  How vulnerable are paratroopers on their desce...   \n",
       "118406               Examples of sole proprietorship? The   \n",
       "284746  Why don't I get answers for some of my questio...   \n",
       "300683               Do Kashmiris wants to join Pakistan?   \n",
       "311541  I am bald on the right side above my ear becau...   \n",
       "335211  Which is the best and free small business acco...   \n",
       "124167  How do I calculate e^-0.4 without a calculator...   \n",
       "83077   Why do colors look more red in one eye and mor...   \n",
       "79161                               What are sound waves?   \n",
       "\n",
       "                                                question2  label     idx  \n",
       "215130                   Why is having a baby a blessing?      1  215130  \n",
       "234600            Are conventional paratroopers obsolete?      0  234600  \n",
       "118406              How are sole proprietorships started?      0  118406  \n",
       "284746  Why do some questions get more answers here in...      1  284746  \n",
       "300683  Do the people of Kashmir want to join Pakistan...      0  300683  \n",
       "311541  I'm losing my hair from the front and sides an...      0  311541  \n",
       "335211  Which is the best free accounting software for...      1  335211  \n",
       "124167  How do you find 75 percent of a number without...      0  124167  \n",
       "83077   Why would one eye see different colors than th...      1   83077  \n",
       "79161   What are transverse and longitudinal sound waves?      0   79161  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset[\"train\"]).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_lables_in_dataset=array([0, 1])\n",
      "num_labels=2\n"
     ]
    }
   ],
   "source": [
    "unique_lables_in_dataset = pd.DataFrame(dataset[\"train\"])[\"label\"].unique()\n",
    "num_labels = len(unique_lables_in_dataset)\n",
    "\n",
    "print(f\"{unique_lables_in_dataset=}\")\n",
    "print(f\"{num_labels=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_CHECKPOINT, do_lower_case=\"uncased\" in PRE_TRAINED_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT has a maximum sequence length of 512. We can check the sequence lengths resulting from tokenizing our dataset to see if our dataset exceeds this restriction of BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length in split='train': 330\n",
      "Max length in split='validation': 199\n",
      "Max length in split='test': 319\n"
     ]
    }
   ],
   "source": [
    "first_sentence_key, second_sentence_key = _task_to_keys[DATASET_TASK]\n",
    "\n",
    "if second_sentence_key == None:  # Simply tokenize sentence\n",
    "\n",
    "    for split in dataset.keys():\n",
    "        max_len = 0\n",
    "        for sentence in dataset[split][first_sentence_key]:\n",
    "            # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "            input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "            \n",
    "            max_len = max(max_len, len(input_ids))\n",
    "        \n",
    "\n",
    "        print(f\"Max length in {split=}: {max_len}\")\n",
    "\n",
    "else:  # Append both sentences via [SEP] and tokenize\n",
    "\n",
    "    for split in dataset.keys():\n",
    "        max_len = 0\n",
    "        for sentence1, sentence2 in zip(dataset[split][first_sentence_key], dataset[split][second_sentence_key]):\n",
    "            # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "            input_ids = tokenizer.encode(sentence1, sentence2,  add_special_tokens=True)\n",
    "            \n",
    "            max_len = max(max_len, len(input_ids))\n",
    "        \n",
    "\n",
    "        print(f\"Max length in {split=}: {max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05689b825874ab280cc10b2a545393e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40430 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_func(item):\n",
    "    \"\"\"Tokenize passed item. \n",
    "    \n",
    "    Depending on dataset task the passed item will either contain one sentence or two sentences.\n",
    "    In the last case the two sentences will be appended via a [SEP] token.\n",
    "    \"\"\"\n",
    "    if second_sentence_key is None:\n",
    "        return tokenizer(item[first_sentence_key], add_special_tokens=True, truncation=True)\n",
    "    else:\n",
    "        return tokenizer(item[first_sentence_key], item[second_sentence_key], add_special_tokens=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a tokenized dataset item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question1': ['How is the life of a math student? Could you describe your own experiences?'],\n",
       " 'question2': ['Which level of prepration is enough for the exam jlpt5?'],\n",
       " 'label': [0],\n",
       " 'idx': [0]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question1': ['How is the life of a math student? Could you describe your own experiences?'],\n",
       " 'question2': ['Which level of prepration is enough for the exam jlpt5?'],\n",
       " 'label': [0],\n",
       " 'idx': [0],\n",
       " 'input_ids': [[101,\n",
       "   2129,\n",
       "   2003,\n",
       "   1996,\n",
       "   2166,\n",
       "   1997,\n",
       "   1037,\n",
       "   8785,\n",
       "   3076,\n",
       "   1029,\n",
       "   2071,\n",
       "   2017,\n",
       "   6235,\n",
       "   2115,\n",
       "   2219,\n",
       "   6322,\n",
       "   1029,\n",
       "   102,\n",
       "   2029,\n",
       "   2504,\n",
       "   1997,\n",
       "   17463,\n",
       "   8156,\n",
       "   2003,\n",
       "   2438,\n",
       "   2005,\n",
       "   1996,\n",
       "   11360,\n",
       "   1046,\n",
       "   14277,\n",
       "   2102,\n",
       "   2629,\n",
       "   1029,\n",
       "   102]],\n",
       " 'token_type_ids': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization added the `input_ids` field, which contains the tokenized sentence with a `[CLS]`(101) and two `[SEP]`(102) tokens added. A `token_type_ids` field which indicates first and second portion of the inputs, if necessary. And an `attention_mask` for the given input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface's `transformers` library provides a `DataCollatorWithPadding` class, which allows us to use dynamic padding.  \n",
    "Dynamic padding will add `[PAD]` tokens to the length of the longest sequence within a batch, instead of padding to the maximum sequence length within the entire dataset.  \n",
    "This will avoid unnecessary padding and therefore improve execution efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>2129</td>\n",
       "      <td>2003</td>\n",
       "      <td>1996</td>\n",
       "      <td>2166</td>\n",
       "      <td>1997</td>\n",
       "      <td>1037</td>\n",
       "      <td>8785</td>\n",
       "      <td>3076</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>2438.0</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>11360.0</td>\n",
       "      <td>1046.0</td>\n",
       "      <td>14277.0</td>\n",
       "      <td>2102.0</td>\n",
       "      <td>2629.0</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>2129</td>\n",
       "      <td>2079</td>\n",
       "      <td>1045</td>\n",
       "      <td>2491</td>\n",
       "      <td>2026</td>\n",
       "      <td>7109</td>\n",
       "      <td>2100</td>\n",
       "      <td>6699</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>2054</td>\n",
       "      <td>5320</td>\n",
       "      <td>14708</td>\n",
       "      <td>3609</td>\n",
       "      <td>2000</td>\n",
       "      <td>2689</td>\n",
       "      <td>2000</td>\n",
       "      <td>3756</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2      3     4     5     6     7     8     9   ...      24  \\\n",
       "0  101  2129  2003   1996  2166  1997  1037  8785  3076  1029  ...  2438.0   \n",
       "1  101  2129  2079   1045  2491  2026  7109  2100  6699  1029  ...     NaN   \n",
       "2  101  2054  5320  14708  3609  2000  2689  2000  3756  1029  ...     NaN   \n",
       "\n",
       "       25      26       27      28       29      30      31      32     33  \n",
       "0  2005.0  1996.0  11360.0  1046.0  14277.0  2102.0  2629.0  1029.0  102.0  \n",
       "1     NaN     NaN      NaN     NaN      NaN     NaN     NaN     NaN    NaN  \n",
       "2     NaN     NaN      NaN     NaN      NaN     NaN     NaN     NaN    NaN  \n",
       "\n",
       "[3 rows x 34 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Example: Select a few samples from the training set\n",
    "samples = tokenized_dataset[\"train\"][:3]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", first_sentence_key, second_sentence_key]}  # Drop `idx` and `sentence` columns, as DataCollator can't process those.\n",
    "pd.DataFrame(samples[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>2129</td>\n",
       "      <td>2003</td>\n",
       "      <td>1996</td>\n",
       "      <td>2166</td>\n",
       "      <td>1997</td>\n",
       "      <td>1037</td>\n",
       "      <td>8785</td>\n",
       "      <td>3076</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>2438</td>\n",
       "      <td>2005</td>\n",
       "      <td>1996</td>\n",
       "      <td>11360</td>\n",
       "      <td>1046</td>\n",
       "      <td>14277</td>\n",
       "      <td>2102</td>\n",
       "      <td>2629</td>\n",
       "      <td>1029</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>2129</td>\n",
       "      <td>2079</td>\n",
       "      <td>1045</td>\n",
       "      <td>2491</td>\n",
       "      <td>2026</td>\n",
       "      <td>7109</td>\n",
       "      <td>2100</td>\n",
       "      <td>6699</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>2054</td>\n",
       "      <td>5320</td>\n",
       "      <td>14708</td>\n",
       "      <td>3609</td>\n",
       "      <td>2000</td>\n",
       "      <td>2689</td>\n",
       "      <td>2000</td>\n",
       "      <td>3756</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2      3     4     5     6     7     8     9   ...    24  \\\n",
       "0  101  2129  2003   1996  2166  1997  1037  8785  3076  1029  ...  2438   \n",
       "1  101  2129  2079   1045  2491  2026  7109  2100  6699  1029  ...     0   \n",
       "2  101  2054  5320  14708  3609  2000  2689  2000  3756  1029  ...     0   \n",
       "\n",
       "     25    26     27    28     29    30    31    32   33  \n",
       "0  2005  1996  11360  1046  14277  2102  2629  1029  102  \n",
       "1     0     0      0     0      0     0     0     0    0  \n",
       "2     0     0      0     0      0     0     0     0    0  \n",
       "\n",
       "[3 rows x 34 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply padding using data_collator\n",
    "batch = data_collator(samples)\n",
    "pd.DataFrame(batch[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `data_collator` will insert `[PAD]` (0) tokens to the maximum length of the passed batch of data items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GLUE dataset specifies one or more evaluation metrics depending on the selected task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"glue\", module_type: \"metric\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "Examples:\n",
       "\n",
       "    >>> glue_metric = evaluate.load('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\n",
       "    >>> glue_metric = evaluate.load('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0, 'f1': 1.0}\n",
       "\n",
       "    >>> glue_metric = evaluate.load('glue', 'stsb')\n",
       "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
       "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
       "\n",
       "    >>> glue_metric = evaluate.load('glue', 'cola')\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'matthews_correlation': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(DATASET_NAME, DATASET_TASK)\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the selected GLUE task we optimize for different evaluation metrics. See BERT paper p.6:\n",
    "\n",
    "> F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_task_to_metric = {\n",
    "    \"cola\": \"matthews_correlation\",\n",
    "    \"mnli\": \"accuracy\",\n",
    "    \"mnli-mm\": \"accuracy\",\n",
    "    \"mrpc\": \"f1\",\n",
    "    \"qnli\": \"accuracy\",\n",
    "    \"qqp\": \"f1\",\n",
    "    \"rte\": \"accuracy\",\n",
    "    \"sst2\": \"accuracy\",\n",
    "    \"stsb\": \"spearmanr\",\n",
    "}\n",
    "\n",
    "metric_for_best_model = _task_to_metric[DATASET_TASK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use \"['accuracy', 'f1']\" as an evaluation metric for the task qqp\n"
     ]
    }
   ],
   "source": [
    "def get_metric_name_for_specific_task():\n",
    "    \"\"\"Helper function to derive the evaluation metric name for the specified GLUE task.\n",
    "\n",
    "    The tasks specified by the GLUE benchmark use different evaluation metrics.\n",
    "    Unfortunatly there is no easy way to derive there name after loading the corresponding metric function via HuggingFace's `evaluate` library.\n",
    "    However we can simply do a \"trial run\" and expect the name key of its output.\n",
    "    \"\"\"\n",
    "    output = metric.compute(\n",
    "        predictions=[1, 0], references=[1, 1]\n",
    "    )  # dummy input - we just want to inspect the returned dictionary.\n",
    "    metric_names = output.keys()\n",
    "    \n",
    "    return list(metric_names)\n",
    "\n",
    "\n",
    "metric_names = get_metric_name_for_specific_task()\n",
    "print(f'We will use \"{metric_names}\" as an evaluation metric for the task {DATASET_TASK}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert metric_for_best_model in metric_names, \"Metric to optimize for not found in evaluation metrics provided by GLUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    PRE_TRAINED_CHECKPOINT,\n",
    "    num_labels=num_labels,\n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=TRAIN_OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5000,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5000,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=2e-5,  # Original paper uses best out of  5e-5, 4e-5, 3e-5, and 2e-5\n",
    "    weight_decay=0.01,  # Original paper uses 0.01 on pre-training\n",
    "    save_total_limit = 3,  # Keep at most the three checkpoints (latest + best one)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_for_best_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if DATASET_TASK != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "validation_key = \"validation_mismatched\" if DATASET_TASK == \"mnli-mm\" else \"validation_matched\" if DATASET_TASK == \"mnli\" else \"validation\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[validation_key],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- training_arguments.output_dir='../groups/192.039-2024W/bert/training/glue-qqp'\n",
      "--- training_arguments.metric_for_best_model='f1'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f1146d37144fb4bb0553873229ad7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56855 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3253, 'grad_norm': 5.69278621673584, 'learning_rate': 1.8241139741447543e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6d3087dbaa4b88a8fe613b644171c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.275972455739975, 'eval_accuracy': 0.8792728172149394, 'eval_f1': 0.8458842474187743, 'eval_runtime': 76.0597, 'eval_samples_per_second': 531.556, 'eval_steps_per_second': 16.619, 'epoch': 0.44}\n",
      "{'loss': 0.2659, 'grad_norm': 5.863916397094727, 'learning_rate': 1.6482279482895085e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c318715bc64334b690e8dc92597b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.251036137342453, 'eval_accuracy': 0.8937175364828098, 'eval_f1': 0.8585629176129818, 'eval_runtime': 75.7259, 'eval_samples_per_second': 533.9, 'eval_steps_per_second': 16.692, 'epoch': 0.88}\n",
      "{'loss': 0.2031, 'grad_norm': 3.80635142326355, 'learning_rate': 1.4723419224342628e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9664fb5098e4484486b528eb8af9192b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.255336195230484, 'eval_accuracy': 0.9005194162750433, 'eval_f1': 0.8663165591969687, 'eval_runtime': 76.2181, 'eval_samples_per_second': 530.451, 'eval_steps_per_second': 16.584, 'epoch': 1.32}\n",
      "{'loss': 0.184, 'grad_norm': 4.285217761993408, 'learning_rate': 1.2964558965790168e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e579e4a31cb244b79f80c44411be1e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24007625877857208, 'eval_accuracy': 0.9058867177838239, 'eval_f1': 0.8739356591458768, 'eval_runtime': 76.1094, 'eval_samples_per_second': 531.209, 'eval_steps_per_second': 16.608, 'epoch': 1.76}\n",
      "{'loss': 0.1537, 'grad_norm': 2.1157779693603516, 'learning_rate': 1.1205698707237711e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a360fca5b77a442188f7e79cb6e6428b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29386231303215027, 'eval_accuracy': 0.9056146425921345, 'eval_f1': 0.8704508419337317, 'eval_runtime': 76.1618, 'eval_samples_per_second': 530.844, 'eval_steps_per_second': 16.596, 'epoch': 2.2}\n",
      "{'loss': 0.1235, 'grad_norm': 7.388816833496094, 'learning_rate': 9.446838448685253e-06, 'epoch': 2.64}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae9a02c010b43d992883c009815f2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2870771288871765, 'eval_accuracy': 0.9069255503339104, 'eval_f1': 0.8754592090021512, 'eval_runtime': 76.5351, 'eval_samples_per_second': 528.254, 'eval_steps_per_second': 16.515, 'epoch': 2.64}\n",
      "{'loss': 0.1141, 'grad_norm': 7.300997734069824, 'learning_rate': 7.687978190132794e-06, 'epoch': 3.08}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff86ef57abf441b867d5a8e9b7fa155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3668602406978607, 'eval_accuracy': 0.9082364580756864, 'eval_f1': 0.8767114183171607, 'eval_runtime': 76.1538, 'eval_samples_per_second': 530.9, 'eval_steps_per_second': 16.598, 'epoch': 3.08}\n",
      "{'loss': 0.0858, 'grad_norm': 25.589889526367188, 'learning_rate': 5.929117931580336e-06, 'epoch': 3.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1991d0ea70ea48a1bc02553299fdd0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37339580059051514, 'eval_accuracy': 0.9095473658174623, 'eval_f1': 0.8775407695141144, 'eval_runtime': 76.1961, 'eval_samples_per_second': 530.605, 'eval_steps_per_second': 16.589, 'epoch': 3.52}\n",
      "{'loss': 0.0855, 'grad_norm': 0.23230697214603424, 'learning_rate': 4.170257673027878e-06, 'epoch': 3.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac14392c1f8422fa4b91729f79b2f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3605738580226898, 'eval_accuracy': 0.911723967350977, 'eval_f1': 0.8814088719056322, 'eval_runtime': 76.0276, 'eval_samples_per_second': 531.781, 'eval_steps_per_second': 16.626, 'epoch': 3.96}\n",
      "{'loss': 0.0642, 'grad_norm': 37.92523956298828, 'learning_rate': 2.41139741447542e-06, 'epoch': 4.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338cb2d305d84024ad700e8fb2c2ea02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4091295897960663, 'eval_accuracy': 0.9121444471926787, 'eval_f1': 0.8814656610825602, 'eval_runtime': 76.1577, 'eval_samples_per_second': 530.872, 'eval_steps_per_second': 16.597, 'epoch': 4.4}\n",
      "{'loss': 0.0607, 'grad_norm': 0.039010580629110336, 'learning_rate': 6.525371559229619e-07, 'epoch': 4.84}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d86e8139a5445482cffbcd7457a587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4158765375614166, 'eval_accuracy': 0.9109324758842444, 'eval_f1': 0.8803376200445286, 'eval_runtime': 76.046, 'eval_samples_per_second': 531.652, 'eval_steps_per_second': 16.622, 'epoch': 4.84}\n",
      "{'train_runtime': 14183.9915, 'train_samples_per_second': 128.259, 'train_steps_per_second': 4.008, 'train_loss': 0.148481072155868, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"--- {training_arguments.output_dir=}\")\n",
    "print(f\"--- {training_arguments.metric_for_best_model=}\")\n",
    "training_summary = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=56855, training_loss=0.148481072155868, metrics={'train_runtime': 14183.9915, 'train_samples_per_second': 128.259, 'train_steps_per_second': 4.008, 'total_flos': 6.421502703224244e+16, 'train_loss': 0.148481072155868, 'epoch': 5.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `trainer.evaluate()` to check that the `trainer` instance did indeed reload the model checkpoint with the highest evaluation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac8f8ac2e3e4cc58d19251ed58bbf2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1264 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4091295897960663,\n",
       " 'eval_accuracy': 0.9121444471926787,\n",
       " 'eval_f1': 0.8814656610825602,\n",
       " 'eval_runtime': 76.8719,\n",
       " 'eval_samples_per_second': 525.94,\n",
       " 'eval_steps_per_second': 16.443,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_evaluation = trainer.evaluate()\n",
    "best_model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.3253</td>\n",
       "      <td>5.692786</td>\n",
       "      <td>1.824114e-05</td>\n",
       "      <td>0.439715</td>\n",
       "      <td>0.275972</td>\n",
       "      <td>0.879273</td>\n",
       "      <td>0.845884</td>\n",
       "      <td>76.0597</td>\n",
       "      <td>531.556</td>\n",
       "      <td>16.619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.2659</td>\n",
       "      <td>5.863916</td>\n",
       "      <td>1.648228e-05</td>\n",
       "      <td>0.879430</td>\n",
       "      <td>0.251036</td>\n",
       "      <td>0.893718</td>\n",
       "      <td>0.858563</td>\n",
       "      <td>75.7259</td>\n",
       "      <td>533.900</td>\n",
       "      <td>16.692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>0.2031</td>\n",
       "      <td>3.806351</td>\n",
       "      <td>1.472342e-05</td>\n",
       "      <td>1.319145</td>\n",
       "      <td>0.255336</td>\n",
       "      <td>0.900519</td>\n",
       "      <td>0.866317</td>\n",
       "      <td>76.2181</td>\n",
       "      <td>530.451</td>\n",
       "      <td>16.584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20000</th>\n",
       "      <td>0.1840</td>\n",
       "      <td>4.285218</td>\n",
       "      <td>1.296456e-05</td>\n",
       "      <td>1.758860</td>\n",
       "      <td>0.240076</td>\n",
       "      <td>0.905887</td>\n",
       "      <td>0.873936</td>\n",
       "      <td>76.1094</td>\n",
       "      <td>531.209</td>\n",
       "      <td>16.608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>0.1537</td>\n",
       "      <td>2.115778</td>\n",
       "      <td>1.120570e-05</td>\n",
       "      <td>2.198575</td>\n",
       "      <td>0.293862</td>\n",
       "      <td>0.905615</td>\n",
       "      <td>0.870451</td>\n",
       "      <td>76.1618</td>\n",
       "      <td>530.844</td>\n",
       "      <td>16.596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>0.1235</td>\n",
       "      <td>7.388817</td>\n",
       "      <td>9.446838e-06</td>\n",
       "      <td>2.638290</td>\n",
       "      <td>0.287077</td>\n",
       "      <td>0.906926</td>\n",
       "      <td>0.875459</td>\n",
       "      <td>76.5351</td>\n",
       "      <td>528.254</td>\n",
       "      <td>16.515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35000</th>\n",
       "      <td>0.1141</td>\n",
       "      <td>7.300998</td>\n",
       "      <td>7.687978e-06</td>\n",
       "      <td>3.078005</td>\n",
       "      <td>0.366860</td>\n",
       "      <td>0.908236</td>\n",
       "      <td>0.876711</td>\n",
       "      <td>76.1538</td>\n",
       "      <td>530.900</td>\n",
       "      <td>16.598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40000</th>\n",
       "      <td>0.0858</td>\n",
       "      <td>25.589890</td>\n",
       "      <td>5.929118e-06</td>\n",
       "      <td>3.517721</td>\n",
       "      <td>0.373396</td>\n",
       "      <td>0.909547</td>\n",
       "      <td>0.877541</td>\n",
       "      <td>76.1961</td>\n",
       "      <td>530.605</td>\n",
       "      <td>16.589</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45000</th>\n",
       "      <td>0.0855</td>\n",
       "      <td>0.232307</td>\n",
       "      <td>4.170258e-06</td>\n",
       "      <td>3.957436</td>\n",
       "      <td>0.360574</td>\n",
       "      <td>0.911724</td>\n",
       "      <td>0.881409</td>\n",
       "      <td>76.0276</td>\n",
       "      <td>531.781</td>\n",
       "      <td>16.626</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>0.0642</td>\n",
       "      <td>37.925240</td>\n",
       "      <td>2.411397e-06</td>\n",
       "      <td>4.397151</td>\n",
       "      <td>0.409130</td>\n",
       "      <td>0.912144</td>\n",
       "      <td>0.881466</td>\n",
       "      <td>76.1577</td>\n",
       "      <td>530.872</td>\n",
       "      <td>16.597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55000</th>\n",
       "      <td>0.0607</td>\n",
       "      <td>0.039011</td>\n",
       "      <td>6.525372e-07</td>\n",
       "      <td>4.836866</td>\n",
       "      <td>0.415877</td>\n",
       "      <td>0.910932</td>\n",
       "      <td>0.880338</td>\n",
       "      <td>76.0460</td>\n",
       "      <td>531.652</td>\n",
       "      <td>16.622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56855</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.409130</td>\n",
       "      <td>0.912144</td>\n",
       "      <td>0.881466</td>\n",
       "      <td>76.8719</td>\n",
       "      <td>525.940</td>\n",
       "      <td>16.443</td>\n",
       "      <td>14183.9915</td>\n",
       "      <td>128.259</td>\n",
       "      <td>4.008</td>\n",
       "      <td>6.421503e+16</td>\n",
       "      <td>0.148481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  grad_norm  learning_rate     epoch  eval_loss  eval_accuracy  \\\n",
       "step                                                                          \n",
       "5000   0.3253   5.692786   1.824114e-05  0.439715   0.275972       0.879273   \n",
       "10000  0.2659   5.863916   1.648228e-05  0.879430   0.251036       0.893718   \n",
       "15000  0.2031   3.806351   1.472342e-05  1.319145   0.255336       0.900519   \n",
       "20000  0.1840   4.285218   1.296456e-05  1.758860   0.240076       0.905887   \n",
       "25000  0.1537   2.115778   1.120570e-05  2.198575   0.293862       0.905615   \n",
       "30000  0.1235   7.388817   9.446838e-06  2.638290   0.287077       0.906926   \n",
       "35000  0.1141   7.300998   7.687978e-06  3.078005   0.366860       0.908236   \n",
       "40000  0.0858  25.589890   5.929118e-06  3.517721   0.373396       0.909547   \n",
       "45000  0.0855   0.232307   4.170258e-06  3.957436   0.360574       0.911724   \n",
       "50000  0.0642  37.925240   2.411397e-06  4.397151   0.409130       0.912144   \n",
       "55000  0.0607   0.039011   6.525372e-07  4.836866   0.415877       0.910932   \n",
       "56855     NaN        NaN            NaN  5.000000   0.409130       0.912144   \n",
       "\n",
       "        eval_f1  eval_runtime  eval_samples_per_second  eval_steps_per_second  \\\n",
       "step                                                                            \n",
       "5000   0.845884       76.0597                  531.556                 16.619   \n",
       "10000  0.858563       75.7259                  533.900                 16.692   \n",
       "15000  0.866317       76.2181                  530.451                 16.584   \n",
       "20000  0.873936       76.1094                  531.209                 16.608   \n",
       "25000  0.870451       76.1618                  530.844                 16.596   \n",
       "30000  0.875459       76.5351                  528.254                 16.515   \n",
       "35000  0.876711       76.1538                  530.900                 16.598   \n",
       "40000  0.877541       76.1961                  530.605                 16.589   \n",
       "45000  0.881409       76.0276                  531.781                 16.626   \n",
       "50000  0.881466       76.1577                  530.872                 16.597   \n",
       "55000  0.880338       76.0460                  531.652                 16.622   \n",
       "56855  0.881466       76.8719                  525.940                 16.443   \n",
       "\n",
       "       train_runtime  train_samples_per_second  train_steps_per_second  \\\n",
       "step                                                                     \n",
       "5000             NaN                       NaN                     NaN   \n",
       "10000            NaN                       NaN                     NaN   \n",
       "15000            NaN                       NaN                     NaN   \n",
       "20000            NaN                       NaN                     NaN   \n",
       "25000            NaN                       NaN                     NaN   \n",
       "30000            NaN                       NaN                     NaN   \n",
       "35000            NaN                       NaN                     NaN   \n",
       "40000            NaN                       NaN                     NaN   \n",
       "45000            NaN                       NaN                     NaN   \n",
       "50000            NaN                       NaN                     NaN   \n",
       "55000            NaN                       NaN                     NaN   \n",
       "56855     14183.9915                   128.259                   4.008   \n",
       "\n",
       "         total_flos  train_loss  \n",
       "step                             \n",
       "5000            NaN         NaN  \n",
       "10000           NaN         NaN  \n",
       "15000           NaN         NaN  \n",
       "20000           NaN         NaN  \n",
       "25000           NaN         NaN  \n",
       "30000           NaN         NaN  \n",
       "35000           NaN         NaN  \n",
       "40000           NaN         NaN  \n",
       "45000           NaN         NaN  \n",
       "50000           NaN         NaN  \n",
       "55000           NaN         NaN  \n",
       "56855  6.421503e+16    0.148481  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_history = pd.DataFrame(trainer.state.log_history)\n",
    "training_history.groupby(\"step\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Loss and Evaluation Metrics over Training Steps"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = training_history[[\"loss\", \"eval_loss\", \"step\", f\"eval_{metric_for_best_model}\"]]\n",
    "data.columns = [\"Train. Loss\", \"Eval. Loss\", \"Training Steps\", \"Acc.\"]\n",
    "data = data[:-1]  # drop last row, as this row just contains the values for the best checkpoint again\n",
    "data = pd.melt(data, ['Training Steps'])\n",
    "\n",
    "\n",
    "plot = sns.lineplot(data=data, x=\"Training Steps\", y=\"value\", hue=\"variable\", style=\"variable\", markers=True)\n",
    "plot.set_ylabel(\"\")\n",
    "plot.set_ylim((0, plot.get_ylim()[1]))\n",
    "plot.legend(title=\"\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"### Loss and Evaluation Metrics over Training Steps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Best Model performance:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Our Model\" based on google-bert/bert-base-uncased, best performance on validation data.\n",
      "\"BERT_BASE\" and \"BERT_LARGE\" performance on GLUE testing data as reported in original paper.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Our Model</th>\n",
       "      <th>original BERT_BASE</th>\n",
       "      <th>original BERT_LARGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_loss</th>\n",
       "      <td>0.409130</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_accuracy</th>\n",
       "      <td>0.912144</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eval_f1</th>\n",
       "      <td>0.881466</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Our Model original BERT_BASE original BERT_LARGE\n",
       "eval_loss       0.409130                  -                   -\n",
       "eval_accuracy   0.912144                  -                   -\n",
       "eval_f1         0.881466              0.712               0.721"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(f\"### Best Model performance:\"))\n",
    "results = pd.DataFrame(\n",
    "    best_model_evaluation.values(),\n",
    "    index=best_model_evaluation.keys(),\n",
    "    columns=[\"Our Model\"],\n",
    ").drop(\n",
    "    # Drop runtime measurements\n",
    "    index=[\"eval_runtime\", \"eval_samples_per_second\", \"eval_steps_per_second\", \"epoch\"]\n",
    ")\n",
    "# Achieved scores from original BERT paper:\n",
    "results[\"original BERT_BASE\"] = [\"-\", \"-\", 0.712]\n",
    "results[\"original BERT_LARGE\"] = [\"-\", \"-\", 0.721]\n",
    "print(f'\"Our Model\" based on {PRE_TRAINED_CHECKPOINT}, best performance on validation data.')\n",
    "print('\"BERT_BASE\" and \"BERT_LARGE\" performance on GLUE testing data as reported in original paper.')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
