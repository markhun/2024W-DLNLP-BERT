{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT on GLUE - MNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding - Wang et al.](https://arxiv.org/pdf/1804.07461):\n",
    "\n",
    "The Multi-Genre Natural Language Inference Corpus (Williams et al., 2018) is a crowdsourced collection of sentence pairs with textual entailment annotations. Given a premise sentence\n",
    "and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are\n",
    "gathered from ten different sources, including transcribed speech, fiction, and government reports.\n",
    "We use the standard test set, for which we obtained private labels from the authors, and evaluate\n",
    "on both the matched (in-domain) and mismatched (cross-domain) sections. We also use and recommend the SNLI corpus (Bowman et al., 2015) as 550k examples of auxiliary training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Where to store the huggingface data. On the provided Jupyterlab instance that should be within the shared group folder.\n",
    "os.environ['HF_HOME'] = '../groups/192.039-2024W/bert/huggingface/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from transformers import set_seed\n",
    "\n",
    "# RANDOMNESS SEED\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Which dataset to load\n",
    "DATASET_NAME = \"glue\"\n",
    "DATASET_TASK = \"mnli\"\n",
    "\n",
    "PRE_TRAINED_CHECKPOINT = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "TRAIN_OUTPUT_DIR = (\n",
    "    Path(\"../groups/192.039-2024W/bert\") / \"training\" / f\"{DATASET_NAME}-{DATASET_TASK}\"\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32  # Original Paper claims to use 32 for GLUE tasks\n",
    "NUM_EPOCHS = 5  # Original Paper claims to use 3 fine-tuning epochs for GLUE tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "  device_count = torch.cuda.device_count()\n",
    "  device_name = torch.cuda.get_device_name(0)\n",
    "\n",
    "  print(f\"There are {device_count} GPU(s) available.\")\n",
    "  print(f\"GPU used: {device_name}\")\n",
    "  ! nvidia-smi -q --display=MEMORY,COMPUTE\n",
    "\n",
    "else:\n",
    "  print(\"No GPU available, using CPU.\")\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the GLUE dataset different tasks have different accessor keys\n",
    "_task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME, DATASET_TASK)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNLI dataset is special, as it provides two validation and two test datasets. One matched (in-domain) and one mismatched (cross-domain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset[\"train\"]).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lables_in_dataset = pd.DataFrame(dataset[\"train\"])[\"label\"].unique()\n",
    "num_labels = len(unique_lables_in_dataset)\n",
    "\n",
    "print(f\"{unique_lables_in_dataset=}\")\n",
    "print(f\"{num_labels=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GLUE benchmark suite keeps the labels for its test dataset secret. This is a common practice in many machine learning benchmarks. By withholding the labels for the test set, it is ensured that the test set is used solely for evaluating the performance of models and models may not be trained on it. This encourages researchers to focus on developing models that generalize well, rather than optimizing for achieving a high score on the specific test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset[\"test_matched\"]).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only way to get an evaluation on the testing dataset is to train a model and sent it to the University of New York - which maintains the GLUE benchmark leaderboard - for evaluation. However this option only exists for researches about to publish a paper, therefore we can't do that.\n",
    "\n",
    "Instead, we will split the validation datasets to create two custom test datasets for our experiment. We will keep the train split as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the first split (80%) as new matched validation dataset and use the second split as new matched test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matched_validation_test_split = dataset[\"validation_matched\"].train_test_split(test_size=0.2)\n",
    "new_matched_validation_test_split[\"validation_matched\"] = new_matched_validation_test_split.pop(\"train\")\n",
    "new_matched_validation_test_split[\"test_matched\"] = new_matched_validation_test_split.pop(\"test\")\n",
    "new_matched_validation_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we do the same for the mismatched case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mismatched_validation_test_split = dataset[\"validation_mismatched\"].train_test_split(test_size=0.2)\n",
    "new_mismatched_validation_test_split[\"validation_mismatched\"] = new_mismatched_validation_test_split.pop(\"train\")\n",
    "new_mismatched_validation_test_split[\"test_mismatched\"] = new_mismatched_validation_test_split.pop(\"test\")\n",
    "new_mismatched_validation_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"validation_matched\"] = new_matched_validation_test_split[\"validation_matched\"]\n",
    "dataset[\"test_matched\"] = new_matched_validation_test_split[\"test_matched\"]\n",
    "dataset[\"validation_mismatched\"] = new_mismatched_validation_test_split[\"validation_mismatched\"]\n",
    "dataset[\"test_mismatched\"] = new_mismatched_validation_test_split[\"test_mismatched\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a test dataset with labels, which is __not__ part of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset[\"test_matched\"]).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset[\"test_mismatched\"]).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_CHECKPOINT = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_CHECKPOINT, do_lower_case=\"uncased\" in PRE_TRAINED_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT has a maximum sequence length of 512. We can check the sequence lengths resulting from tokenizing our dataset to see if our dataset exceeds this restriction of BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence_key, second_sentence_key = _task_to_keys[DATASET_TASK]\n",
    "\n",
    "if second_sentence_key == None:  # Simply tokenize sentence\n",
    "\n",
    "    for split in dataset.keys():\n",
    "        max_len = 0\n",
    "        for sentence in dataset[split][first_sentence_key]:\n",
    "            # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "            input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "            \n",
    "            max_len = max(max_len, len(input_ids))\n",
    "        \n",
    "\n",
    "        print(f\"Max length in {split=}: {max_len}\")\n",
    "\n",
    "else:  # Append both sentences via [SEP] and tokenize\n",
    "\n",
    "    for split in dataset.keys():\n",
    "        max_len = 0\n",
    "        for sentence1, sentence2 in zip(dataset[split][first_sentence_key], dataset[split][second_sentence_key]):\n",
    "            # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "            input_ids = tokenizer.encode(sentence1, sentence2,  add_special_tokens=True)\n",
    "            \n",
    "            max_len = max(max_len, len(input_ids))\n",
    "        \n",
    "\n",
    "        print(f\"Max length in {split=}: {max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(item):\n",
    "    \"\"\"Tokenize passed item. \n",
    "    \n",
    "    Depending on dataset task the passed item will either contain one sentence or two sentences.\n",
    "    In the last case the two sentences will be appended via a [SEP] token.\n",
    "    \"\"\"\n",
    "    if second_sentence_key is None:\n",
    "        return tokenizer(item[first_sentence_key], add_special_tokens=True, truncation=True)\n",
    "    else:\n",
    "        return tokenizer(item[first_sentence_key], item[second_sentence_key], add_special_tokens=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a tokenized dataset item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', 400):\n",
    "    display(pd.DataFrame(tokenized_dataset[\"train\"][:1]).transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization added the `input_ids` field, which contains the tokenized sentence with a `[CLS]`(101) and two `[SEP]`(102) tokens added. A `token_type_ids` field which indicates first and second portion of the inputs, if necessary. And an `attention_mask` for the given input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface's `transformers` library provides a `DataCollatorWithPadding` class, which allows us to use dynamic padding.  \n",
    "Dynamic padding will add `[PAD]` tokens to the length of the longest sequence within a batch, instead of padding to the maximum sequence length within the entire dataset.  \n",
    "This will avoid unnecessary padding and therefore improve execution efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Example: Select a few samples from the training set\n",
    "samples = tokenized_dataset[\"train\"][:3]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", first_sentence_key, second_sentence_key]}  # Drop `idx` and `sentence` columns, as DataCollator can't process those.\n",
    "pd.DataFrame(samples[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply padding using data_collator\n",
    "batch = data_collator(samples)\n",
    "pd.DataFrame(batch[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `data_collator` will insert `[PAD]` (0) tokens to the maximum length of the passed batch of data items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GLUE dataset specifies one or more evaluation metrics depending on the selected task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(DATASET_NAME, DATASET_TASK)\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the selected GLUE task we optimize for different evaluation metrics. See BERT paper p.6:\n",
    "\n",
    "> F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_task_to_metric = {\n",
    "    \"cola\": \"matthews_correlation\",\n",
    "    \"mnli\": \"accuracy\",\n",
    "    \"mrpc\": \"f1\",\n",
    "    \"qnli\": \"accuracy\",\n",
    "    \"qqp\": \"f1\",\n",
    "    \"rte\": \"accuracy\",\n",
    "    \"sst2\": \"accuracy\",\n",
    "    \"stsb\": \"spearmanr\",\n",
    "}\n",
    "\n",
    "metric_for_best_model = _task_to_metric[DATASET_TASK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_name_for_specific_task():\n",
    "    \"\"\"Helper function to derive the evaluation metric name for the specified GLUE task.\n",
    "\n",
    "    The tasks specified by the GLUE benchmark use different evaluation metrics.\n",
    "    Unfortunatly there is no easy way to derive there name after loading the corresponding metric function via HuggingFace's `evaluate` library.\n",
    "    However we can simply do a \"trial run\" and expect the name key of its output.\n",
    "    \"\"\"\n",
    "    output = metric.compute(\n",
    "        predictions=[1, 0], references=[1, 1]\n",
    "    )  # dummy input - we just want to inspect the returned dictionary.\n",
    "    metric_names = output.keys()\n",
    "    \n",
    "    return list(metric_names)\n",
    "\n",
    "\n",
    "metric_names = get_metric_name_for_specific_task()\n",
    "print(f'We will use \"{metric_names}\" as an evaluation metric for the task {DATASET_TASK}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert metric_for_best_model in metric_names, \"Metric to optimize for not found in evaluation metrics provided by GLUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    PRE_TRAINED_CHECKPOINT,\n",
    "    num_labels=num_labels,\n",
    "    torch_dtype=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=(TRAIN_OUTPUT_DIR / PRE_TRAINED_CHECKPOINT.replace(\"/\", \"_\")).resolve(),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=2e-5,  # Original paper uses best out of  5e-5, 4e-5, 3e-5, and 2e-5\n",
    "    weight_decay=0.01,  # Original paper uses 0.01 on pre-training\n",
    "    save_total_limit = 3,  # Keep at most the three checkpoints (latest + best one)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy_avg\",  # `eval_accuracy_avg` will be computed via a custom callback to be the avg of the accuracy for both validation datasets (matched and mismatched)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if DATASET_TASK != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this dataset provides two different evaluation datasets (matched and mismatched) we will perform two evaluations on each `eval_step`. We will then compute an average of both evaluation results and store the average within `transformer`s `metrics` dictionary as `eval_accuracy_avg`. We will also use this computed average as `metric_for_best_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "\n",
    "class AverageMatchedAndMismatchedAccuracies(TrainerCallback):\n",
    "    \"\"\"Callback to save the `eval_mnli_matched_accuracy` after the first evaluation step (MLNI matched)\n",
    "    and then compute the average of (MLNI matched acc. and mismatched acc.) on the second evaluation step\n",
    "    \"\"\"\n",
    "\n",
    "    matched_acc = None\n",
    "\n",
    "    def on_evaluate(\n",
    "        self,\n",
    "        args: TrainingArguments,\n",
    "        state: TrainerState,\n",
    "        control: TrainerControl,\n",
    "        metrics: dict[str, float],\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Event called after an evaluation phase.\"\"\"\n",
    "        if self.matched_acc is None:\n",
    "            # We are in the first evaluation step (matched) - save result metric for later use\n",
    "            self.matched_acc = metrics[\"eval_mnli_matched_accuracy\"]\n",
    "            return\n",
    "\n",
    "        # We are in the second evaluation step (mismatched)\n",
    "        # Use the `matched_acc` saved before and the `mismatched_acc` to compute average:\n",
    "        mismatched_acc = metrics[\"eval_mnli_mismatched_accuracy\"]\n",
    "        metrics[\"eval_accuracy_avg\"] = (self.matched_acc + mismatched_acc) / 2\n",
    "\n",
    "        self.matched_acc = None\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset={\n",
    "        \"mnli_matched\": tokenized_dataset[\"validation_matched\"],\n",
    "        \"mnli_mismatched\": tokenized_dataset[\"validation_mismatched\"],\n",
    "    },\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[AverageMatchedAndMismatchedAccuracies]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"--- {training_arguments.output_dir=}\")\n",
    "print(f\"--- {training_arguments.metric_for_best_model=}\")\n",
    "training_summary_bert_base = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary_bert_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `trainer.evaluate()` to check that the `trainer` instance did indeed reload the model checkpoint with the highest evaluation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_evaluation = trainer.evaluate()\n",
    "best_model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history_bert_base = pd.DataFrame(trainer.state.log_history)\n",
    "training_history_bert_base.epoch = training_history_bert_base.epoch.astype(int)\n",
    "training_history_bert_base.groupby(\"epoch\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = training_history_bert_base[[\"loss\", \"eval_mnli_matched_loss\", \"epoch\", \"eval_mnli_matched_accuracy\", ]]\n",
    "data.columns = [\"Train. Loss\", \"Eval. Loss\", \"Training Epoch\", \"Acc.\"]\n",
    "data = data[:-1]  # drop last row, as this row just contains the values for the best checkpoint again\n",
    "data = pd.melt(data, ['Training Epoch']).dropna()\n",
    "\n",
    "\n",
    "plot = sns.lineplot(data=data, x=\"Training Epoch\", y=\"value\", hue=\"variable\", style=\"variable\", markers=True)\n",
    "plot.set_ylabel(\"\")\n",
    "plot.set(xticks=list(set(training_history_bert_base.epoch)))\n",
    "plot.set_ylim((0, plot.get_ylim()[1]))\n",
    "plot.legend(title=\"\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"### Loss and Evaluation Metrics over Training  Epochs (matched {PRE_TRAINED_CHECKPOINT})\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = training_history_bert_base[[\"loss\", \"eval_mnli_mismatched_loss\", \"epoch\", \"eval_mnli_mismatched_accuracy\", ]]\n",
    "data.columns = [\"Train. Loss\", \"Eval. Loss\", \"Training Epoch\", \"Acc.\"]\n",
    "data = data[:-1]  # drop last row, as this row just contains the values for the best checkpoint again\n",
    "data = pd.melt(data, ['Training Epoch']).dropna()\n",
    "\n",
    "\n",
    "plot = sns.lineplot(data=data, x=\"Training Epoch\", y=\"value\", hue=\"variable\", style=\"variable\", markers=True)\n",
    "plot.set_ylabel(\"\")\n",
    "plot.set(xticks=list(set(training_history_bert_base.epoch)))\n",
    "plot.set_ylim((0, plot.get_ylim()[1]))\n",
    "plot.legend(title=\"\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"### Loss and Evaluation Metrics over Training Steps (mismatched {PRE_TRAINED_CHECKPOINT})\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x='label', data=pd.DataFrame(tokenized_dataset[\"test_matched\"]))\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"### Label frequency in matched test dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x='label', data=pd.DataFrame(tokenized_dataset[\"test_mismatched\"]))\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"### Label frequency in mismatched test dataset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset classes seem to be somewhat balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_matched = trainer.predict(tokenized_dataset[\"test_matched\"])\n",
    "predictions_mismatched = trainer.predict(tokenized_dataset[\"test_mismatched\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "bert_base_matched_cm = sklearn.metrics.confusion_matrix(\n",
    "    tokenized_dataset[\"test_matched\"][\"label\"],\n",
    "    predictions_matched.predictions.argmax(-1),\n",
    ")\n",
    "plot = sns.heatmap(bert_base_matched_cm, annot=True, fmt=\"d\")\n",
    "plot.set_xlabel(\"True label\")\n",
    "plot.set_ylabel(\"Predicted label\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(\n",
    "    Markdown(f\"### Prediction Confusion Matrix (matchded - {PRE_TRAINED_CHECKPOINT})\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "bert_base_mismatched_cm = sklearn.metrics.confusion_matrix(\n",
    "    tokenized_dataset[\"test_mismatched\"][\"label\"],\n",
    "    predictions_mismatched.predictions.argmax(-1),\n",
    ")\n",
    "plot = sns.heatmap(bert_base_mismatched_cm, annot=True, fmt=\"d\")\n",
    "plot.set_xlabel(\"True label\")\n",
    "plot.set_ylabel(\"Predicted label\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"### Prediction Confusion Matrix (mismatchded - {PRE_TRAINED_CHECKPOINT})\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_matched.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mismatched.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"### Best Model performance:\"))\n",
    "results = pd.DataFrame(\n",
    "    data=[training_summary_bert_base.metrics[\"train_runtime\"]]\n",
    "    + list(best_model_evaluation.values())\n",
    "    + [\n",
    "        predictions_matched.metrics[\"test_accuracy\"],\n",
    "        predictions_mismatched.metrics[\"test_accuracy\"],\n",
    "    ],\n",
    "    index=[\"train_runtime_s\"]\n",
    "    + list(best_model_evaluation.keys())\n",
    "    + [\n",
    "        \"matched_test_accuracy\",\n",
    "        \"mismatched_test_accuracy\",\n",
    "    ],\n",
    "    columns=[\"our BERT_BASE\"],\n",
    ").drop(\n",
    "    # Drop runtime measurements\n",
    "    index=[\n",
    "        \"eval_mnli_matched_runtime\",\n",
    "        \"eval_mnli_mismatched_runtime\",\n",
    "        \"eval_mnli_matched_samples_per_second\",\n",
    "        \"eval_mnli_mismatched_samples_per_second\",\n",
    "        \"eval_mnli_matched_steps_per_second\",\n",
    "        \"eval_mnli_mismatched_steps_per_second\",\n",
    "        \"epoch\",\n",
    "    ]\n",
    ")\n",
    "# Achieved scores from original BERT paper:\n",
    "results[\"original BERT_BASE\"] = [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", 0.846, 0.834]\n",
    "results[\"original BERT_LARGE\"] = [\"-\", \"-\", \"-\", \"-\", \"-\", \"-\", 0.867, 0.859]\n",
    "print(\n",
    "    f'\"Our Model\" based on {PRE_TRAINED_CHECKPOINT}, best performance on validation data.'\n",
    ")\n",
    "print(\n",
    "    '\"BERT_BASE\" and \"BERT_LARGE\" performance on GLUE testing data as reported in original paper.'\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BERT-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_CHECKPOINT = \"google-bert/bert-large-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_CHECKPOINT, do_lower_case=\"uncased\" in PRE_TRAINED_CHECKPOINT)\n",
    "\n",
    "def tokenize_func(item):\n",
    "    \"\"\"Tokenize passed item. \n",
    "    \n",
    "    Depending on dataset task the passed item will either contain one sentence or two sentences.\n",
    "    In the last case the two sentences will be appended via a [SEP] token.\n",
    "    \"\"\"\n",
    "    if second_sentence_key is None:\n",
    "        return tokenizer(item[first_sentence_key], add_special_tokens=True, truncation=True)\n",
    "    else:\n",
    "        return tokenizer(item[first_sentence_key], item[second_sentence_key], add_special_tokens=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(DATASET_NAME, DATASET_TASK)\n",
    "\n",
    "metric_for_best_model = _task_to_metric[DATASET_TASK]\n",
    "metric_names = get_metric_name_for_specific_task()\n",
    "print(f'We will use \"{metric_names}\" as an evaluation metric for the task {DATASET_TASK}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert metric_for_best_model in metric_names, \"Metric to optimize for not found in evaluation metrics provided by GLUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32  # BERT-large might need a smaller batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "try:\n",
    "    del model\n",
    "    del trainer\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    PRE_TRAINED_CHECKPOINT,\n",
    "    num_labels=num_labels,\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=(TRAIN_OUTPUT_DIR / PRE_TRAINED_CHECKPOINT.replace(\"/\", \"_\")).resolve(),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=2e-5,  # Original paper uses best out of  5e-5, 4e-5, 3e-5, and 2e-5\n",
    "    weight_decay=0.01,  # Original paper uses 0.01 on pre-training\n",
    "    save_total_limit = 3,  # Keep at most the three checkpoints (latest + best one)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy_avg\",  # `eval_accuracy_avg` will be computed via a custom callback to be the avg of the accuracy for both validation datasets (matched and mismatched)\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "validation_key = \"validation_mismatched\" if DATASET_TASK == \"mnli-mm\" else \"validation_matched\" if DATASET_TASK == \"mnli\" else \"validation\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset={\n",
    "        \"mnli_matched\": tokenized_dataset[\"validation_matched\"],\n",
    "        \"mnli_mismatched\": tokenized_dataset[\"validation_mismatched\"],\n",
    "    },\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[AverageMatchedAndMismatchedAccuracies]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"--- {training_arguments.output_dir=}\")\n",
    "print(f\"--- {training_arguments.metric_for_best_model=}\")\n",
    "training_summary_bert_large = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary_bert_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_evaluation = trainer.evaluate()\n",
    "best_model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history_bert_large = pd.DataFrame(trainer.state.log_history)\n",
    "training_history_bert_large.epoch = training_history_bert_large.epoch.astype(int)\n",
    "training_history_bert_large.groupby(\"epoch\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = training_history_bert_large[[\"loss\", \"eval_mnli_matched_loss\", \"epoch\", \"eval_mnli_matched_accuracy\", ]]\n",
    "data.columns = [\"Train. Loss\", \"Eval. Loss\", \"Training Epoch\", \"Acc.\"]\n",
    "data = data[:-1]  # drop last row, as this row just contains the values for the best checkpoint again\n",
    "data = pd.melt(data, ['Training Epoch']).dropna()\n",
    "\n",
    "\n",
    "plot = sns.lineplot(data=data, x=\"Training Epoch\", y=\"value\", hue=\"variable\", style=\"variable\", markers=True)\n",
    "plot.set_ylabel(\"\")\n",
    "plot.set(xticks=list(set(training_history_bert_large.epoch)))\n",
    "plot.set_ylim((0, plot.get_ylim()[1]))\n",
    "plot.legend(title=\"\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"### Loss and Evaluation Metrics over Training  Epochs (matched {PRE_TRAINED_CHECKPOINT})\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = training_history_bert_large[[\"loss\", \"eval_mnli_mismatched_loss\", \"epoch\", \"eval_mnli_mismatched_accuracy\", ]]\n",
    "data.columns = [\"Train. Loss\", \"Eval. Loss\", \"Training Epoch\", \"Acc.\"]\n",
    "data = data[:-1]  # drop last row, as this row just contains the values for the best checkpoint again\n",
    "data = pd.melt(data, ['Training Epoch']).dropna()\n",
    "\n",
    "\n",
    "plot = sns.lineplot(data=data, x=\"Training Epoch\", y=\"value\", hue=\"variable\", style=\"variable\", markers=True)\n",
    "plot.set_ylabel(\"\")\n",
    "plot.set(xticks=list(set(training_history_bert_large.epoch)))\n",
    "plot.set_ylim((0, plot.get_ylim()[1]))\n",
    "plot.legend(title=\"\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"### Loss and Evaluation Metrics over Training Steps (mismatched {PRE_TRAINED_CHECKPOINT})\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_matched = trainer.predict(tokenized_dataset[\"test_matched\"])\n",
    "predictions_mismatched = trainer.predict(tokenized_dataset[\"test_mismatched\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "bert_large_matched_cm = sklearn.metrics.confusion_matrix(\n",
    "    tokenized_dataset[\"test_matched\"][\"label\"],\n",
    "    predictions_matched.predictions.argmax(-1),\n",
    ")\n",
    "plot = sns.heatmap(bert_large_matched_cm, annot=True, fmt=\"d\")\n",
    "plot.set_xlabel(\"True label\")\n",
    "plot.set_ylabel(\"Predicted label\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(\n",
    "    Markdown(f\"### Prediction Confusion Matrix (matchded - {PRE_TRAINED_CHECKPOINT})\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "bert_large_mismatched_cm = sklearn.metrics.confusion_matrix(\n",
    "    tokenized_dataset[\"test_mismatched\"][\"label\"],\n",
    "    predictions_mismatched.predictions.argmax(-1),\n",
    ")\n",
    "plot = sns.heatmap(bert_large_mismatched_cm, annot=True, fmt=\"d\")\n",
    "plot.set_xlabel(\"True label\")\n",
    "plot.set_ylabel(\"Predicted label\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"### Prediction Confusion Matrix (mismatchded - {PRE_TRAINED_CHECKPOINT})\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"### Best Model performance:\"))\n",
    "results[\"our BERT_LARGE\"] = [\n",
    "    training_summary_bert_large.metrics[\"train_runtime\"],\n",
    "    best_model_evaluation[\"eval_mnli_matched_loss\"],\n",
    "    best_model_evaluation[\"eval_mnli_matched_accuracy\"],\n",
    "    best_model_evaluation[\"eval_mnli_mismatched_loss\"],\n",
    "    best_model_evaluation[\"eval_mnli_mismatched_accuracy\"],\n",
    "    best_model_evaluation[\"eval_accuracy_avg\"],\n",
    "    predictions_matched.metrics[\"test_accuracy\"],\n",
    "    predictions_mismatched.metrics[\"test_accuracy\"],\n",
    "]\n",
    "results = results[\n",
    "    [\n",
    "        \"our BERT_BASE\",\n",
    "        \"original BERT_BASE\",\n",
    "        \"our BERT_LARGE\",\n",
    "        \"original BERT_LARGE\",\n",
    "    ]\n",
    "]\n",
    "print('\"BERT_BASE\" and \"BERT_LARGE\" performance on GLUE testing data as reported in original paper.')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ModernBERT-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_CHECKPOINT = \"answerdotai/ModernBERT-base\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_CHECKPOINT, do_lower_case=\"uncased\" in PRE_TRAINED_CHECKPOINT)\n",
    "\n",
    "def tokenize_func(item):\n",
    "    \"\"\"Tokenize passed item. \n",
    "    \n",
    "    Depending on dataset task the passed item will either contain one sentence or two sentences.\n",
    "    In the last case the two sentences will be appended via a [SEP] token.\n",
    "    \"\"\"\n",
    "    if second_sentence_key is None:\n",
    "        return tokenizer(item[first_sentence_key], add_special_tokens=True, truncation=True)\n",
    "    else:\n",
    "        return tokenizer(item[first_sentence_key], item[second_sentence_key], add_special_tokens=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(DATASET_NAME, DATASET_TASK)\n",
    "\n",
    "metric_for_best_model = _task_to_metric[DATASET_TASK]\n",
    "metric_names = get_metric_name_for_specific_task()\n",
    "print(f'We will use \"{metric_names}\" as an evaluation metric for the task {DATASET_TASK}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert metric_for_best_model in metric_names, \"Metric to optimize for not found in evaluation metrics provided by GLUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "try:\n",
    "    del model\n",
    "    del trainer\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    PRE_TRAINED_CHECKPOINT,\n",
    "    num_labels=num_labels,\n",
    "    reference_compile=False\n",
    ")\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=(TRAIN_OUTPUT_DIR / PRE_TRAINED_CHECKPOINT.replace(\"/\", \"_\")).resolve(),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "    learning_rate=8e-5,  # Original paper recommends 8e-5\n",
    "    weight_decay=0.01,  # Original paper uses 0.01 on pre-training\n",
    "    save_total_limit = 3,  # Keep at most the three checkpoints (latest + best one)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy_avg\",  # `eval_accuracy_avg` will be computed via a custom callback to be the avg of the accuracy for both validation datasets (matched and mismatched)\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "validation_key = \"validation_mismatched\" if DATASET_TASK == \"mnli-mm\" else \"validation_matched\" if DATASET_TASK == \"mnli\" else \"validation\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset={\n",
    "        \"mnli_matched\": tokenized_dataset[\"validation_matched\"],\n",
    "        \"mnli_mismatched\": tokenized_dataset[\"validation_mismatched\"],\n",
    "    },\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[AverageMatchedAndMismatchedAccuracies]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"--- {training_arguments.output_dir=}\")\n",
    "print(f\"--- {training_arguments.metric_for_best_model=}\")\n",
    "training_summary_modernbert_base = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary_modernbert_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_evaluation = trainer.evaluate()\n",
    "best_model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history_modernbert_base = pd.DataFrame(trainer.state.log_history)\n",
    "training_history_modernbert_base.epoch = training_history_modernbert_base.epoch.astype(int)\n",
    "training_history_modernbert_base.groupby(\"epoch\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = training_history_modernbert_base[[\"loss\", \"eval_mnli_matched_loss\", \"epoch\", \"eval_mnli_matched_accuracy\", ]]\n",
    "data.columns = [\"Train. Loss\", \"Eval. Loss\", \"Training Epoch\", \"Acc.\"]\n",
    "data = data[:-1]  # drop last row, as this row just contains the values for the best checkpoint again\n",
    "data = pd.melt(data, ['Training Epoch']).dropna()\n",
    "\n",
    "\n",
    "plot = sns.lineplot(data=data, x=\"Training Epoch\", y=\"value\", hue=\"variable\", style=\"variable\", markers=True)\n",
    "plot.set_ylabel(\"\")\n",
    "plot.set(xticks=list(set(training_history_modernbert_base.epoch)))\n",
    "plot.set_ylim((0, plot.get_ylim()[1]))\n",
    "plot.legend(title=\"\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"### Loss and Evaluation Metrics over Training  Epochs (matched {PRE_TRAINED_CHECKPOINT})\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = training_history_modernbert_base[[\"loss\", \"eval_mnli_mismatched_loss\", \"epoch\", \"eval_mnli_mismatched_accuracy\", ]]\n",
    "data.columns = [\"Train. Loss\", \"Eval. Loss\", \"Training Epoch\", \"Acc.\"]\n",
    "data = data[:-1]  # drop last row, as this row just contains the values for the best checkpoint again\n",
    "data = pd.melt(data, ['Training Epoch']).dropna()\n",
    "\n",
    "\n",
    "plot = sns.lineplot(data=data, x=\"Training Epoch\", y=\"value\", hue=\"variable\", style=\"variable\", markers=True)\n",
    "plot.set_ylabel(\"\")\n",
    "plot.set(xticks=list(set(training_history_modernbert_base.epoch)))\n",
    "plot.set_ylim((0, plot.get_ylim()[1]))\n",
    "plot.legend(title=\"\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"### Loss and Evaluation Metrics over Training Steps (mismatched {PRE_TRAINED_CHECKPOINT})\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_matched = trainer.predict(tokenized_dataset[\"test_matched\"])\n",
    "predictions_mismatched = trainer.predict(tokenized_dataset[\"test_mismatched\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "modernbert_base_matched_cm = sklearn.metrics.confusion_matrix(\n",
    "    tokenized_dataset[\"test_matched\"][\"label\"],\n",
    "    predictions_matched.predictions.argmax(-1),\n",
    ")\n",
    "plot = sns.heatmap(modernbert_base_matched_cm, annot=True, fmt=\"d\")\n",
    "plot.set_xlabel(\"True label\")\n",
    "plot.set_ylabel(\"Predicted label\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(\n",
    "    Markdown(f\"### Prediction Confusion Matrix (matchded - {PRE_TRAINED_CHECKPOINT})\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "modernbert_base_mismatched_cm = sklearn.metrics.confusion_matrix(\n",
    "    tokenized_dataset[\"test_mismatched\"][\"label\"],\n",
    "    predictions_mismatched.predictions.argmax(-1),\n",
    ")\n",
    "plot = sns.heatmap(modernbert_base_mismatched_cm, annot=True, fmt=\"d\")\n",
    "plot.set_xlabel(\"True label\")\n",
    "plot.set_ylabel(\"Predicted label\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"### Prediction Confusion Matrix (mismatchded - {PRE_TRAINED_CHECKPOINT})\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_matched.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mismatched.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"### Best Model performance:\"))\n",
    "results[\"our ModernBERT_BASE\"] = [\n",
    "    training_summary_modernbert_base.metrics[\"train_runtime\"],\n",
    "    best_model_evaluation[\"eval_mnli_matched_loss\"],\n",
    "    best_model_evaluation[\"eval_mnli_matched_accuracy\"],\n",
    "    best_model_evaluation[\"eval_mnli_mismatched_loss\"],\n",
    "    best_model_evaluation[\"eval_mnli_mismatched_accuracy\"],\n",
    "    best_model_evaluation[\"eval_accuracy_avg\"],\n",
    "    predictions_matched.metrics[\"test_accuracy\"],\n",
    "    predictions_mismatched.metrics[\"test_accuracy\"],\n",
    "]\n",
    "results = results[\n",
    "    [\n",
    "        \"our BERT_BASE\",\n",
    "        \"original BERT_BASE\",\n",
    "        \"our ModernBERT_BASE\",\n",
    "        \"our BERT_LARGE\",\n",
    "        \"original BERT_LARGE\",\n",
    "    ]\n",
    "]\n",
    "print('\"BERT_BASE\" and \"BERT_LARGE\" performance on GLUE testing data as reported in original paper.')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ModernBERT-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_CHECKPOINT = \"answerdotai/ModernBERT-large\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_CHECKPOINT, do_lower_case=\"uncased\" in PRE_TRAINED_CHECKPOINT)\n",
    "\n",
    "def tokenize_func(item):\n",
    "    \"\"\"Tokenize passed item. \n",
    "    \n",
    "    Depending on dataset task the passed item will either contain one sentence or two sentences.\n",
    "    In the last case the two sentences will be appended via a [SEP] token.\n",
    "    \"\"\"\n",
    "    if second_sentence_key is None:\n",
    "        return tokenizer(item[first_sentence_key], add_special_tokens=True, truncation=True)\n",
    "    else:\n",
    "        return tokenizer(item[first_sentence_key], item[second_sentence_key], add_special_tokens=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(DATASET_NAME, DATASET_TASK)\n",
    "\n",
    "metric_for_best_model = _task_to_metric[DATASET_TASK]\n",
    "metric_names = get_metric_name_for_specific_task()\n",
    "print(f'We will use \"{metric_names}\" as an evaluation metric for the task {DATASET_TASK}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert metric_for_best_model in metric_names, \"Metric to optimize for not found in evaluation metrics provided by GLUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "try:\n",
    "    del model\n",
    "    del trainer\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    PRE_TRAINED_CHECKPOINT,\n",
    "    num_labels=num_labels,\n",
    "    reference_compile=False\n",
    ")\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=(TRAIN_OUTPUT_DIR / PRE_TRAINED_CHECKPOINT.replace(\"/\", \"_\")).resolve(),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_torch\",\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "    learning_rate=8e-5,  # Original paper recommends 8e-5\n",
    "    weight_decay=0.01,  # Original paper uses 0.01 on pre-training\n",
    "    save_total_limit = 3,  # Keep at most the three checkpoints (latest + best one)\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy_avg\",  # `eval_accuracy_avg` will be computed via a custom callback to be the avg of the accuracy for both validation datasets (matched and mismatched)\n",
    "    bf16=True,\n",
    "    bf16_full_eval=True,\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "validation_key = \"validation_mismatched\" if DATASET_TASK == \"mnli-mm\" else \"validation_matched\" if DATASET_TASK == \"mnli\" else \"validation\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset={\n",
    "        \"mnli_matched\": tokenized_dataset[\"validation_matched\"],\n",
    "        \"mnli_mismatched\": tokenized_dataset[\"validation_mismatched\"],\n",
    "    },\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[AverageMatchedAndMismatchedAccuracies]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"--- {training_arguments.output_dir=}\")\n",
    "print(f\"--- {training_arguments.metric_for_best_model=}\")\n",
    "training_summary_modernbert_large = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_summary_modernbert_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_evaluation = trainer.evaluate()\n",
    "best_model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history_modernbert_large = pd.DataFrame(trainer.state.log_history)\n",
    "training_history_modernbert_large.epoch = training_history_modernbert_large.epoch.astype(int)\n",
    "training_history_modernbert_large.groupby(\"epoch\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = training_history_modernbert_large[[\"loss\", \"eval_mnli_matched_loss\", \"epoch\", \"eval_mnli_matched_accuracy\", ]]\n",
    "data.columns = [\"Train. Loss\", \"Eval. Loss\", \"Training Epoch\", \"Acc.\"]\n",
    "data = data[:-1]  # drop last row, as this row just contains the values for the best checkpoint again\n",
    "data = pd.melt(data, ['Training Epoch']).dropna()\n",
    "\n",
    "\n",
    "plot = sns.lineplot(data=data, x=\"Training Epoch\", y=\"value\", hue=\"variable\", style=\"variable\", markers=True)\n",
    "plot.set_ylabel(\"\")\n",
    "plot.set(xticks=list(set(training_history_modernbert_large.epoch)))\n",
    "plot.set_ylim((0, plot.get_ylim()[1]))\n",
    "plot.legend(title=\"\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"### Loss and Evaluation Metrics over Training  Epochs (matched {PRE_TRAINED_CHECKPOINT})\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = training_history_modernbert_large[[\"loss\", \"eval_mnli_mismatched_loss\", \"epoch\", \"eval_mnli_mismatched_accuracy\", ]]\n",
    "data.columns = [\"Train. Loss\", \"Eval. Loss\", \"Training Epoch\", \"Acc.\"]\n",
    "data = data[:-1]  # drop last row, as this row just contains the values for the best checkpoint again\n",
    "data = pd.melt(data, ['Training Epoch']).dropna()\n",
    "\n",
    "\n",
    "plot = sns.lineplot(data=data, x=\"Training Epoch\", y=\"value\", hue=\"variable\", style=\"variable\", markers=True)\n",
    "plot.set_ylabel(\"\")\n",
    "plot.set(xticks=list(set(training_history_modernbert_large.epoch)))\n",
    "plot.set_ylim((0, plot.get_ylim()[1]))\n",
    "plot.legend(title=\"\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"### Loss and Evaluation Metrics over Training Steps (mismatched {PRE_TRAINED_CHECKPOINT})\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_matched = trainer.predict(tokenized_dataset[\"test_matched\"])\n",
    "predictions_mismatched = trainer.predict(tokenized_dataset[\"test_mismatched\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "modernbert_large_matched_cm = sklearn.metrics.confusion_matrix(\n",
    "    tokenized_dataset[\"test_matched\"][\"label\"],\n",
    "    predictions_matched.predictions.argmax(-1),\n",
    ")\n",
    "plot = sns.heatmap(modernbert_large_matched_cm, annot=True, fmt=\"d\")\n",
    "plot.set_xlabel(\"True label\")\n",
    "plot.set_ylabel(\"Predicted label\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(\n",
    "    Markdown(f\"### Prediction Confusion Matrix (matchded - {PRE_TRAINED_CHECKPOINT})\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "modernbert_large_mismatched_cm = sklearn.metrics.confusion_matrix(\n",
    "    tokenized_dataset[\"test_mismatched\"][\"label\"],\n",
    "    predictions_mismatched.predictions.argmax(-1),\n",
    ")\n",
    "plot = sns.heatmap(modernbert_large_mismatched_cm, annot=True, fmt=\"d\")\n",
    "plot.set_xlabel(\"True label\")\n",
    "plot.set_ylabel(\"Predicted label\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"### Prediction Confusion Matrix (mismatchded - {PRE_TRAINED_CHECKPOINT})\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_matched.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mismatched.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"### Best Model performance:\"))\n",
    "results[\"our ModernBERT_LARGE\"] = [\n",
    "    training_summary_modernbert_base.metrics[\"train_runtime\"],\n",
    "    best_model_evaluation[\"eval_mnli_matched_loss\"],\n",
    "    best_model_evaluation[\"eval_mnli_matched_accuracy\"],\n",
    "    best_model_evaluation[\"eval_mnli_mismatched_loss\"],\n",
    "    best_model_evaluation[\"eval_mnli_mismatched_accuracy\"],\n",
    "    best_model_evaluation[\"eval_accuracy_avg\"],\n",
    "    predictions_matched.metrics[\"test_accuracy\"],\n",
    "    predictions_mismatched.metrics[\"test_accuracy\"],\n",
    "]\n",
    "results = results[\n",
    "    [\n",
    "        \"our BERT_BASE\",\n",
    "        \"original BERT_BASE\",\n",
    "        \"our ModernBERT_BASE\",\n",
    "        \"our BERT_LARGE\",\n",
    "        \"original BERT_LARGE\",\n",
    "        \"our ModernBERT_LARGE\",\n",
    "    ]\n",
    "]\n",
    "print('\"BERT_BASE\" and \"BERT_LARGE\" performance on GLUE testing data as reported in original paper.')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "titles = [\"BERT-base\", \"BERT-large\", \"ModernBERT-base\", \"ModernBERT-large\"]\n",
    "training_histories = [training_history_bert_base, training_history_bert_large, training_history_modernbert_base, training_history_modernbert_large]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2,ncols=len(training_histories), sharey=True, sharex=True)\n",
    "\n",
    "def draw_loss_eval_plot(title, history, ax, mismatched=False):\n",
    "    eval_loss_key = \"eval_mnli_mismatched_loss\" if mismatched else \"eval_mnli_matched_loss\"\n",
    "    eval_acc_key = \"eval_mnli_mismatched_accuracy\" if mismatched else \"eval_mnli_matched_accuracy\"\n",
    "\n",
    "    data = history[[\"loss\", eval_loss_key, \"epoch\", eval_acc_key]]\n",
    "    data.columns = [\"Train. Loss\", \"Eval. Loss\", \"Training Epoch\", \"Acc.\"]\n",
    "    data = data[:-1]\n",
    "    data = pd.melt(data, ['Training Epoch']).dropna()\n",
    "\n",
    "    plot = sns.lineplot(data=data, x=\"Training Epoch\", y=\"value\", hue=\"variable\", style=\"variable\", markers=True, ax=ax)\n",
    "    plot.set_ylabel(\"\")\n",
    "    plot.set(xticks=list(set(history.epoch)))\n",
    "    plot.legend(title=\"\", loc='upper left')\n",
    "    plot.set_title(title)\n",
    "\n",
    "for title, history, ax in zip(titles, training_histories, axes[0]):\n",
    "    draw_loss_eval_plot(title, history, ax)\n",
    "for title, history, ax in zip(titles, training_histories, axes[1]):\n",
    "    draw_loss_eval_plot(title, history, ax, mismatched=True)\n",
    "\n",
    "for ax in axes[0][1:]:\n",
    "    ax.get_legend().remove()\n",
    "for ax in axes[1]:\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "axes[0][0].set_ylabel(\"MATCHED Evaluation / Test Set\", weight='bold', fontsize=14)\n",
    "axes[1][0].set_ylabel(\"MISMATCHED Evaluation / Test Set\", weight='bold', fontsize=14)\n",
    "\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(10)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\"BERT-base\", \"BERT-large\", \"ModernBERT-base\", \"ModernBERT-large\"]\n",
    "our_results_matched = (\n",
    "    results.loc[\"matched_test_accuracy\"]\n",
    "    .drop(\"original BERT_BASE\")\n",
    "    .drop(\"original BERT_LARGE\")\n",
    ")\n",
    "our_results_mismatched = (\n",
    "    results.loc[\"mismatched_test_accuracy\"]\n",
    "    .drop(\"original BERT_BASE\")\n",
    "    .drop(\"original BERT_LARGE\")\n",
    ")\n",
    "titles_matched = [\n",
    "    title + \" - \" + f\"Matched Acc.: {matched_acc:.2f}\"\n",
    "    for title, matched_acc in zip(titles, our_results_matched)\n",
    "]\n",
    "titles_mismatched = [\n",
    "    title + \" - \" + f\"Mismatched Acc.: {mismatched_acc:.2f}\"\n",
    "    for title, mismatched_acc in zip(titles, our_results_mismatched)\n",
    "]\n",
    "matched_cms = [\n",
    "    bert_base_matched_cm,\n",
    "    bert_large_matched_cm,\n",
    "    modernbert_base_matched_cm,\n",
    "    modernbert_large_matched_cm,\n",
    "]\n",
    "mismatched_cms = [\n",
    "    bert_base_mismatched_cm,\n",
    "    bert_large_mismatched_cm,\n",
    "    modernbert_base_mismatched_cm,\n",
    "    modernbert_large_mismatched_cm,\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(4.2, 4.2))\n",
    "\n",
    "\n",
    "def draw_confusion_matrix_plot(title, cm, ax):\n",
    "    include_cbar = title in (titles_matched[-1], titles_mismatched[-1])\n",
    "    plot = sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", square=True, cmap=\"viridis\", cbar=include_cbar, ax=ax\n",
    "    )\n",
    "    plot.set_xlabel(\"True label\")\n",
    "    plot.set_ylabel(\"Predicted label\")\n",
    "    plot.set_title(title)\n",
    "\n",
    "\n",
    "for title, history, ax in zip(titles_matched, matched_cms, axes[0]):\n",
    "    draw_confusion_matrix_plot(title, history, ax)\n",
    "for title, history, ax in zip(titles_mismatched, matched_cms, axes[1]):\n",
    "    draw_confusion_matrix_plot(title, history, ax)\n",
    "\n",
    "\n",
    "fig.set_figwidth(18)\n",
    "fig.set_figheight(8)\n",
    "fig.tight_layout(h_pad=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup = results[\"our BERT_BASE\"][\"train_runtime_s\"] / results[\"our ModernBERT_BASE\"][\"train_runtime_s\"] \n",
    "speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
