{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de32eb56-2c57-4ba0-ae83-7e5a2bd58b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, DatasetDict, load_from_disk\n",
    "\n",
    "load_from_file = False\n",
    "\n",
    "if load_from_disk:\n",
    "    datasets = load_from_disk(\"../swag.hf\")\n",
    "else:\n",
    "    datasets = load_dataset(\"swag\", \"regular\")\n",
    "    # the labels for the test split are not public, therefore we create our own split\n",
    "    # 60% train, 20% validation, 20% test\n",
    "    merged_datasets = concatenate_datasets([datasets[\"train\"], datasets[\"validation\"]])\n",
    "    \n",
    "    train_testvalid = merged_datasets.train_test_split(test_size=0.4)\n",
    "    # Split the 10% test + valid in half test, half valid\n",
    "    test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "    # gather everyone if you want to have a single DatasetDict\n",
    "    datasets = DatasetDict({\n",
    "        'train': train_testvalid['train'],\n",
    "        'test': test_valid['test'],\n",
    "        'validation': test_valid['train']})\n",
    "\n",
    "    datasets.save_to_disk(\"../swag.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd014d1c-efd8-4a8b-b831-36d846aaa596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e9db1a-c242-4919-b5a9-f3040c75ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ModernBertModel\n",
    "\n",
    "class ModernBERTForMultipleChoice(nn.Module):\n",
    "    def __init__(self, model_name=\"answerdotai/ModernBERT-large\"):\n",
    "        super(ModernBERTForMultipleChoice, self).__init__()\n",
    "        self.modernBERT = ModernBertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(1024, 1)\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        bsz, num_choices, seq_len = input_ids.size()\n",
    "        \n",
    "        input_ids = input_ids.view(-1, seq_len)\n",
    "        attention_mask = attention_mask.view(-1, seq_len)\n",
    "        # Pass through BERT\n",
    "        outputs = self.modernBERT(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Extract [CLS] token representation: (batch_size * 4, hidden_size)\n",
    "        hidden_state = outputs.last_hidden_state  # (batch_size*num_choices, seq_len, hidden_size)\n",
    "        pooled_output = hidden_state[:, 0, :]  \n",
    "\n",
    "        logits = self.classifier(self.dropout(pooled_output))  # (batch_size*num_choices, 1)\n",
    "        reshaped_logits = logits.view(bsz, num_choices)  # (batch_size, num_choices)\n",
    "        \n",
    "        # Compute loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(reshaped_logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": reshaped_logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a069992-7ab1-48d8-979c-9958abd2b2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f529423b414c98b209c1da0cb7e805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c206189422154f3ea8dd2148c29146d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18711 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b14bbda1394c4989efa7047fc7c20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adopted from: https://huggingface.co/docs/transformers/tasks/multiple_choice\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n",
    "    question_headers = examples[\"sent2\"]\n",
    "    answer_choices = [examples[f\"ending{i}\"] for i in range(4)]\n",
    "\n",
    "    second_sentences = [\n",
    "        [f\"{header} {choice}\" for choice in choices]\n",
    "        for header, choices in zip(question_headers, zip(*answer_choices))\n",
    "    ]\n",
    "\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        first_sentences,\n",
    "        second_sentences,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\", \n",
    "        max_length=128,        \n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"].view(-1, 4, tokenized[\"input_ids\"].shape[-1]),\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"].view(-1, 4, tokenized[\"attention_mask\"].shape[-1]),\n",
    "        \"labels\": examples[\"label\"],\n",
    "    }\n",
    "\n",
    "encoded_dataset = datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75e3732-a6dc-4cb1-a223-794057c140d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_predictions):\n",
    "    predictions, label_ids = eval_predictions\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c00e3-6a74-4125-8e2e-2f0b34559d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5001' max='10527' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5001/10527 2:06:04 < 2:19:21, 0.66 it/s, Epoch 1.42/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.649400</td>\n",
       "      <td>0.570847</td>\n",
       "      <td>0.782416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.556800</td>\n",
       "      <td>0.578723</td>\n",
       "      <td>0.788081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.560700</td>\n",
       "      <td>0.518744</td>\n",
       "      <td>0.802940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.621283</td>\n",
       "      <td>0.803848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='1170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/1170 03:24 < 04:33, 2.45 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "model = ModernBERTForMultipleChoice(\"answerdotai/ModernBERT-large\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"modernBert-large-finetuned-swag\",\n",
    "    eval_strategy= \"steps\",\n",
    "    eval_steps=1000,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04a3a8a5-9105-4404-913b-31ee41115f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1972671747207642,\n",
       " 'eval_accuracy': 0.8199358582496643,\n",
       " 'eval_runtime': 479.6592,\n",
       " 'eval_samples_per_second': 39.007,\n",
       " 'eval_steps_per_second': 2.439,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluated = trainer.evaluate()\n",
    "evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "222d3576-0dbd-4cfe-a57d-0fbdcb518763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7273</td>\n",
       "      <td>11.111187</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>500</td>\n",
       "      <td>0.570847</td>\n",
       "      <td>0.782416</td>\n",
       "      <td>481.6994</td>\n",
       "      <td>38.842</td>\n",
       "      <td>2.429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2357</td>\n",
       "      <td>15.915008</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.621283</td>\n",
       "      <td>0.803848</td>\n",
       "      <td>478.0415</td>\n",
       "      <td>39.139</td>\n",
       "      <td>2.447</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0387</td>\n",
       "      <td>13.706006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>7500</td>\n",
       "      <td>1.165250</td>\n",
       "      <td>0.817370</td>\n",
       "      <td>477.7302</td>\n",
       "      <td>39.164</td>\n",
       "      <td>2.449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10527</td>\n",
       "      <td>1.197267</td>\n",
       "      <td>0.819936</td>\n",
       "      <td>477.6933</td>\n",
       "      <td>39.167</td>\n",
       "      <td>2.449</td>\n",
       "      <td>16674.5278</td>\n",
       "      <td>10.099</td>\n",
       "      <td>0.631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.284195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  grad_norm  learning_rate   step  eval_loss  eval_accuracy  \\\n",
       "epoch                                                                      \n",
       "0      0.7273  11.111187       0.000019    500   0.570847       0.782416   \n",
       "1      0.2357  15.915008       0.000012   4000   0.621283       0.803848   \n",
       "2      0.0387  13.706006       0.000006   7500   1.165250       0.817370   \n",
       "3         NaN        NaN            NaN  10527   1.197267       0.819936   \n",
       "\n",
       "       eval_runtime  eval_samples_per_second  eval_steps_per_second  \\\n",
       "epoch                                                                 \n",
       "0          481.6994                   38.842                  2.429   \n",
       "1          478.0415                   39.139                  2.447   \n",
       "2          477.7302                   39.164                  2.449   \n",
       "3          477.6933                   39.167                  2.449   \n",
       "\n",
       "       train_runtime  train_samples_per_second  train_steps_per_second  \\\n",
       "epoch                                                                    \n",
       "0                NaN                       NaN                     NaN   \n",
       "1                NaN                       NaN                     NaN   \n",
       "2                NaN                       NaN                     NaN   \n",
       "3         16674.5278                    10.099                   0.631   \n",
       "\n",
       "       total_flos  train_loss  \n",
       "epoch                          \n",
       "0             NaN         NaN  \n",
       "1             NaN         NaN  \n",
       "2             NaN         NaN  \n",
       "3             0.0    0.284195  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "training_history_modernBert = pd.DataFrame(trainer.state.log_history)\n",
    "training_history_modernBert.epoch = training_history_modernBert.epoch.astype(int)\n",
    "training_history_modernBert.groupby(\"epoch\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "150e6326-d75f-44e0-839f-200417958bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.1635264158248901,\n",
       " 'test_accuracy': 0.8218160271644592,\n",
       " 'test_runtime': 478.3647,\n",
       " 'test_samples_per_second': 39.115,\n",
       " 'test_steps_per_second': 2.446}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = trainer.predict(encoded_dataset[\"test\"])\n",
    "test_pred.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31facd45-dee8-4e07-a33b-123140826216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare the data\n",
    "data = training_history_modernBert[[\"loss\", \"eval_loss\", \"step\", \"eval_accuracy\"]]\n",
    "data.columns = [\"Train. Loss\", \"Eval. Loss\", \"Training Steps\", \"Accuracy\"]\n",
    "data = data[:-1]  # drop last row\n",
    "data = pd.melt(data, ['Training Steps']).dropna()\n",
    "\n",
    "# Plot using Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each variable separately without using groupby\n",
    "train_loss = data[data['variable'] == \"Train. Loss\"]\n",
    "eval_loss = data[data['variable'] == \"Eval. Loss\"]\n",
    "acc = data[data['variable'] == \"Accuracy\"]\n",
    "\n",
    "plt.plot(train_loss[\"Training Steps\"], train_loss[\"value\"], marker='o', label=\"Train. Loss\")\n",
    "plt.plot(eval_loss[\"Training Steps\"], eval_loss[\"value\"], marker='o', label=\"Eval. Loss\")\n",
    "plt.plot(acc[\"Training Steps\"], acc[\"value\"], marker='o', label=\"Accuracy\")\n",
    "\n",
    "# Labels and Title\n",
    "plt.ylabel('Accuarcy/Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.title('ModernBert-Large: Training Accuarcy vs Evaluation Accuarcy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('ModernBert_large_swag_finetuned.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c97dc-b089-4a5a-937a-f8ea0aa807ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"confusion_matrix\")\n",
    "\n",
    "predictions = np.argmax(test_pred.predictions, axis=-1)\n",
    "metric.add_batch(predictions=predictions, references=datasets[\"test\"][\"label\"])\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = metric.compute()['confusion_matrix']\n",
    "\n",
    "# Define class labels manually (adjust as needed)\n",
    "labels = [\"1\", \"2\", \"3\", \"4\"]\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('ModernBert-Large: Confusion Matrix')\n",
    "plt.savefig('swag_ModernBert_large_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e022231-d40e-4a1b-8937-9a35f9a52f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (esim)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
